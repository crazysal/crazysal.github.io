webpackJsonp([1],Array(44).concat([function(e,t,n){"use strict";var a=n(333),s=n.n(a),i=s.a.create({xsrfCookieName:"csrftoken",xsrfHeaderName:"X-CSRFToken"});t.a=i},,,,,,,,,,,,,,,,,,,,,,,,,,,,,function(e,t,n){"use strict";var a=n(44);t.a={login:function(e,t){return a.a.post("/auth/login/",{username:e,password:t})},logout:function(){return a.a.post("/auth/logout/",{})},createAccount:function(e,t,n,s){return a.a.post("/registration/",{username:e,password1:t,password2:n,email:s})},changeAccountPassword:function(e,t){return a.a.post("/auth/password/change/",{password1:e,password2:t})},sendAccountPasswordResetEmail:function(e){return a.a.post("/auth/password/reset/",{email:e})},resetAccountPassword:function(e,t,n,s){return a.a.post("/auth/password/reset/confirm/",{uid:e,token:t,new_password1:n,new_password2:s})},getAccountDetails:function(){return a.a.get("/auth/user/")},updateAccountDetails:function(e){return a.a.patch("/auth/user/",e)},verifyAccountEmail:function(e){return a.a.post("/registration/verify-email/",{key:e})}}},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s}),n.d(t,"c",function(){return i}),n.d(t,"d",function(){return o}),n.d(t,"e",function(){return r}),n.d(t,"f",function(){return l}),n.d(t,"g",function(){return u}),n.d(t,"h",function(){return p}),n.d(t,"i",function(){return d}),n.d(t,"j",function(){return c}),n.d(t,"k",function(){return m}),n.d(t,"l",function(){return h}),n.d(t,"m",function(){return f}),n.d(t,"n",function(){return _}),n.d(t,"o",function(){return g}),n.d(t,"p",function(){return y}),n.d(t,"q",function(){return b}),n.d(t,"r",function(){return v}),n.d(t,"s",function(){return w}),n.d(t,"t",function(){return x}),n.d(t,"v",function(){return T}),n.d(t,"u",function(){return I});var a="ACTIVATION_BEGIN",s="ACTIVATION_CLEAR",i="ACTIVATION_FAILURE",o="ACTIVATION_SUCCESS",r="LOGIN_BEGIN",l="LOGIN_FAILURE",u="LOGIN_SUCCESS",p="LOGOUT",d="PASSWORD_EMAIL_BEGIN",c="PASSWORD_EMAIL_CLEAR",m="PASSWORD_EMAIL_FAILURE",h="PASSWORD_EMAIL_SUCCESS",f="PASSWORD_RESET_BEGIN",_="PASSWORD_RESET_CLEAR",g="PASSWORD_RESET_FAILURE",y="PASSWORD_RESET_SUCCESS",b="REGISTRATION_BEGIN",v="REGISTRATION_CLEAR",w="REGISTRATION_FAILURE",x="REGISTRATION_SUCCESS",T="SET_TOKEN",I="REMOVE_TOKEN"},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,function(e,t,n){"use strict";t.a={name:"app"}},function(e,t,n){"use strict";var a=n(47),s=n(9),i=n(297),o=n(353),r=n(354),l=n(358),u=n(359),p=n(360);a.a.use(s.a),t.a=new s.a.Store({modules:{cytoscape:i.a,modals:o.a,auth:r.a,password:l.a,signup:u.a,graph:p.a}})},,,,,,,,,,,,,,,,,,,,,,,,function(e,t,n){"use strict";var a=n(363),s=n(364);t.a={name:"full",components:{AppHeader:s.d,Sidebar:s.e,AppAside:s.a,AppFooter:s.c,Breadcrumb:s.b},data:function(){return{nav:a.a.items}},computed:{name:function(){return this.$route.name},list:function(){return this.$route.matched}}}},function(e,t,n){"use strict";t.a={name:"c-aside"}},function(e,t,n){"use strict";t.a={props:{list:{type:Array,required:!0,default:function(){return[]}}},methods:{isLast:function(e){return e===this.list.length-1},showName:function(e){return e.meta&&e.meta.label&&(e=e.meta&&e.meta.label),e.name&&(e=e.name),e}}}},function(e,t,n){"use strict";t.a={props:{variant:{type:String}},computed:{classList:function(){return["callout",this.calloutVariant]},calloutVariant:function(){return this.variant?"callout-"+this.variant:""}}}},function(e,t,n){"use strict";t.a={name:"c-footer"}},function(e,t,n){"use strict";var a=n(374);t.a={name:"c-header",components:{HeaderDropdown:a.a},methods:{sidebarToggle:function(e){e.preventDefault(),document.body.classList.toggle("sidebar-hidden")},sidebarMinimize:function(e){e.preventDefault(),document.body.classList.toggle("sidebar-minimized")},mobileSidebarToggle:function(e){e.preventDefault(),document.body.classList.toggle("sidebar-mobile-show")},asideToggle:function(e){e.preventDefault(),document.body.classList.toggle("aside-menu-hidden")}}}},function(e,t,n){"use strict";t.a={name:"header-dropdown",data:function(){return{itemsCount:42}}}},function(e,t,n){"use strict";var a=n(379),s=n(381),i=n(383),o=n(385),r=n(387),l=n(393),u=n(149),p=n(396),d=n(152),c=n(399);t.a={name:"sidebar",props:{navItems:{type:Array,required:!0,default:function(){return[]}}},components:{SidebarFooter:a.a,SidebarForm:s.a,SidebarHeader:i.a,SidebarMinimizer:o.a,SidebarNavDivider:r.a,SidebarNavDropdown:l.a,SidebarNavLink:u.a,SidebarNavTitle:p.a,SidebarNavItem:d.a,SidebarNavLabel:c.a},methods:{handleClick:function(e){e.preventDefault(),e.target.parentElement.classList.toggle("open")}}}},function(e,t,n){"use strict";t.a={name:"sidebar-footer"}},function(e,t,n){"use strict";t.a={name:"sidebar-form"}},function(e,t,n){"use strict";t.a={name:"sidebar-header"}},function(e,t,n){"use strict";t.a={name:"sidebar-minimizer",methods:{sidebarMinimize:function(){document.body.classList.toggle("sidebar-minimized")},brandMinimize:function(){document.body.classList.toggle("brand-minimized")}}}},function(e,t,n){"use strict";var a=n(36),s=n.n(a);t.a={name:"sidebar-nav-divider",props:{classes:{type:String,default:""}},computed:{classList:function(){return["divider"].concat(s()(this.itemClasses))},itemClasses:function(){return this.classes?this.classes.split(" "):""}}}},function(e,t,n){"use strict";t.a={props:{name:{type:String,default:""},url:{type:String,default:""},icon:{type:String,default:""}},methods:{handleClick:function(e){e.preventDefault(),e.target.parentElement.classList.toggle("open")}}}},function(e,t,n){"use strict";var a=n(150),s=n(395),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";var a=n(36),s=n.n(a);t.a={name:"sidebar-nav-link",props:{name:{type:String,default:""},url:{type:String,default:""},icon:{type:String,default:""},badge:{type:Object,default:function(){}},variant:{type:String,default:""},classes:{type:String,default:""}},computed:{classList:function(){return["nav-link",this.linkVariant].concat(s()(this.itemClasses))},linkVariant:function(){return this.variant?"nav-link-"+this.variant:""},itemClasses:function(){return this.classes?this.classes.split(" "):[]},isExternalLink:function(){return"http"===this.url.substring(0,4)}}}},function(e,t,n){"use strict";var a=n(36),s=n.n(a);t.a={props:{name:{type:String,default:""},classes:{type:String,default:""},wrapper:{type:Object,default:function(){}}},computed:{classList:function(){return["nav-title"].concat(s()(this.itemClasses))},itemClasses:function(){return this.classes?this.classes.split(" "):""}}}},function(e,t,n){"use strict";var a=n(153),s=n(398),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";var a=n(36),s=n.n(a);t.a={name:"sidebar-nav-item",props:{classes:{type:String,default:""}},computed:{classList:function(){return["nav-item"].concat(s()(this.itemClasses))},itemClasses:function(){return this.classes?this.classes.split(" "):""}},methods:{hideMobile:function(){document.body.classList.contains("sidebar-mobile-show")&&document.body.classList.toggle("sidebar-mobile-show")}}}},function(e,t,n){"use strict";var a=n(36),s=n.n(a),i=n(152),o=n(149);t.a={name:"sidebar-nav-label",components:{SidebarNavItem:i.a,SidebarNavLink:o.a},props:{name:{type:String,default:""},url:{type:String,default:"#"},icon:{type:String,default:"fa fa-circle"},classes:{type:String,default:""},label:{type:Object,required:!0,default:function(){}}},computed:{classList:function(){return{navItem:["hidden-cn"].concat(s()(this.getClasses(this.classes))).join(" "),navLink:"nav-label",icon:[this.icon?this.icon:"fa fa-circle",this.label.variant?"text-"+this.label.variant:"",this.label.class?this.label.class:""].join(" ")}}},methods:{getClasses:function(e){return e?e.split(" "):[]}}}},function(e,t,n){"use strict";t.a={model:{prop:"checked",event:"change"},props:{value:{default:!0},uncheckedValue:{default:!1},checked:{default:!1},type:{type:String,default:"default"},variant:{type:String,default:""},pill:{type:Boolean,default:!1},on:{type:String,default:null},off:{type:String,default:null},size:{type:String,default:null}},computed:{classList:function(){return["switch",this.switchType,this.switchVariant,this.switchPill,this.switchSize]},switchType:function(){return this.type?"switch-"+this.type:"switch-default"},switchVariant:function(){return this.variant?"switch-"+this.variant:"switch-secondary"},switchPill:function(){return this.pill?"switch-pill":null},switchSize:function(){return this.size?"switch-"+this.size:""},isChecked:function(){return this.checked===this.value},isOn:function(){return!!this.on||null}},methods:{handleChange:function(e){var t=e.target.checked;this.$emit("change",t?this.value:this.uncheckedValue)}}}},function(e,t){console.log("from inside landing.vue")},function(e,t,n){"use strict";var a=n(11),s=n.n(a),i=n(413),o=n(463),r=n(9);t.a={name:"dashboard",components:{CytoscapeGraph:i.a,Modals:o.a},computed:s()({},Object(r.c)({currentGraphType:"graph/currentGraphType",currentGraphId:"graph/currentGraphId",cy:"cy"})),methods:s()({},Object(r.d)({showLoad:"showLoadGraph",showSave:"showSaveGraph",showUpdate:"showUpdateGraph",showWizard:"showWizard",showHelp:"showHelp"}),Object(r.b)({updateRootGraphs:"graph/updateRootGraphs",updateUserGraphs:"graph/updateUserGraphs",runCurrentGraph:"graph/runCurrentGraph"}),{handleRun:function(){this.runCurrentGraph()},handleHelp:function(){this.showHelp()},handleLoad:function(){this.updateRootGraphs(),this.updateUserGraphs(),this.showLoad()},handleSave:function(){"root"===this.currentGraphType?this.showSave():this.showUpdate()},handleResetLayout:function(e){this.cy.layout({name:e,animate:!0,animationDuration:500}).run()},handleShowWizard:function(){this.showWizard()}})}},,function(e,t,n){"use strict";var a=n(158),s=n.n(a),i=n(11),o=n.n(i),r=n(415),l=n.n(r),u=n(419),p=n.n(u),d=n(460),c=n.n(d),m=n(461),h=n.n(m),f=n(46),_=n.n(f),g=n(9);t.a={name:"CytoscapeGraph",computed:o()({},Object(g.c)({cytoscapeConfig:"cytoscapeConfig",edgehandlesConfig:"edgehandlesConfig",contextMenusConfig:"contextMenusConfig",cy:"cy",funcMeta:"funcMeta",initGraph:"initGraph"})),methods:o()({},Object(g.b)({setCytoscapeInitConfig:"setCytoscapeInitConfig",setEdgehandlesConfig:"setEdgehandlesConfig",setContextMenusConfig:"setContextMenusConfig",setCy:"setCy",setLibHierarchy:"setLibHierarchy",setFuncMeta:"setFuncMeta",setLibMeta:"setLibMeta",setLibUINames:"setLibUINames",setInitGraph:"setInitGraph"})),mounted:function(){var e=this,t=_()("#cy");this.setLibHierarchy().then(function(){console.log("updated lib hierarchy")}),this.setLibMeta(),this.setLibUINames(),this.setFuncMeta().then(function(){e.setCytoscapeInitConfig(t).then(function(){l.a.use(p.a),l.a.use(h.a),c()(l.a,_.a),e.setCy(l()(s()(e.cytoscapeConfig))),e.setEdgehandlesConfig(e).then(function(){e.cy.edgehandles(e.edgehandlesConfig)}),e.setContextMenusConfig(e).then(function(){e.cy.contextMenus(e.contextMenusConfig)})}).then(function(){e.setInitGraph(e).then(function(){e.cy.json(JSON.parse(e.initGraph.content))})})})}}},,,,,,,function(e,t,n){"use strict";var a=n(464),s=n(468),i=n(471),o=n(473),r=n(476),l=n(480),u=n(483),p=n(491);t.a={name:"modals",components:{AddNode:a.a,EditNode:s.a,EditEdge:i.a,LoadGraph:o.a,Help:p.a,SaveGraph:r.a,UpdateGraph:l.a,Wizard:u.a},data:function(){return{}}}},function(e,t,n){"use strict";var a=n(11),s=n.n(a),i=n(9),o=n(46),r=n.n(o),l=n(466);t.a={name:"addNode",data:function(){return{info:"Asdf"}},computed:s()({},Object(i.c)({visible:"addNodeVisible",clickPos:"cyClickPos",libHierarchy:"libHierarchy",cy:"cy"}),{myModal:{get:function(){return this.visible},set:function(e){!0===e?this.show():this.hide()}}}),methods:s()({},Object(i.d)({show:"showAddNode",hide:"hideAddNode",shownext:"showEditNode",setSelectedNodeId:"setSelectedNodeId",setSelectedNodeElem:"setSelectedNodeElem"}),{handleOk:function(){var e=r()("input[name=radiosInline]:checked");this.addNode(JSON.parse(e.val()),e.attr("key1"),e.attr("key2"));for(var t=document.getElementsByName("radiosInline"),n=0;n<t.length;n++)t[n].checked=!1;this.hide(),this.shownext()},addNode:function(e,t,n){var a=t+" : "+n,s={};"UploadData"===n&&(s={group:"nodes",data:{id:l.a.guid(),name:a,info:e},style:{content:"data(name)","text-opacity":.5,"text-valign":"center","text-halign":"right","background-color":"#000000"},position:{x:this.clickPos.x,y:this.clickPos.y}}),"decomposition"===n&&(s={group:"nodes",data:{id:l.a.guid(),name:a,info:e},style:{content:"data(name)","text-opacity":.5,"text-valign":"center","text-halign":"right","background-color":"#aa0000"},position:{x:this.clickPos.x,y:this.clickPos.y}}),"preprocessing"===n&&(s={group:"nodes",data:{id:l.a.guid(),name:a,info:e},style:{content:"data(name)","text-opacity":.5,"text-valign":"center","text-halign":"right","background-color":"#0000aa"},position:{x:this.clickPos.x,y:this.clickPos.y}}),"linear_model"===n&&(s={group:"nodes",data:{id:l.a.guid(),name:a,info:e},style:{content:"data(name)","text-opacity":.5,"text-valign":"center","text-halign":"right","background-color":"#aaaa00"},position:{x:this.clickPos.x,y:this.clickPos.y}}),"model_selection"===n&&(s={group:"nodes",data:{id:l.a.guid(),name:a,info:e},style:{content:"data(name)","text-opacity":.5,"text-valign":"center","text-halign":"right","background-color":"#00aa00"},position:{x:this.clickPos.x,y:this.clickPos.y}}),"svm"===n&&(s={group:"nodes",data:{id:l.a.guid(),name:a,info:e},style:{content:"data(name)","text-opacity":.5,"text-valign":"center","text-halign":"right","background-color":"#aa00aa"},position:{x:this.clickPos.x,y:this.clickPos.y}}),this.setSelectedNodeId(s.data.id),this.setSelectedNodeElem(s),this.cy.add(s)}}),mounted:function(){}}},function(e,t,n){"use strict";var a=n(11),s=n.n(a),i=n(9),o=n(70),r=n.n(o);t.a={name:"editNode",data:function(){return{host:"",func:"",funcm:"",funcminputs:[],funcmoutputs:[],meths:[],wparams:[],isHidden:!0,isHidden2:!0,selec2ted:!0,btnclass1:"btn btn-outline-primary",btnclass2:"btn btn-outline-primary",fparams:[]}},computed:s()({},Object(i.c)({visible:"editNodeVisible",selectedNode:"selectedNode",funcMeta:"funcMeta",cy:"cy"}),{myModal:{get:function(){return this.visible},set:function(e){!0===e?this.show():this.hide()}}}),methods:s()({},Object(i.d)({show:"showEditNode",hide:"hideEditNode",resetSelectedNode:"resetSelectedNode"}),{handleFuncChange:function(e,t,n){this.host=e,this.func=t,this.meths=n,this.isHidden=!0,this.isHidden2=!0,this.wparams=r.a.cloneDeep(this.funcMeta[t].WParameters),this.fparams=r.a.cloneDeep(this.funcMeta[t].FParameters),void 0===this.meths&&(this.meths=["data","obj"]),console.log("funchange",e,t,n),console.log("funchange",this.host,this.func,this.meths),console.log("funchange",this.funcMeta[t])},handleMethChange:function(e){console.log("In methchange",e,this.func),console.log(this.funcMeta[this.func].Methods[e]),"obj"===e?(this.funcm="obj",this.funcmoutputs=[{name:"obj"}],this.funcminputs=[{name:"obj"}]):(this.funcm=e,this.funcmoutputs=r.a.cloneDeep(this.funcMeta[this.func].Methods[e].outputs),this.funcminputs=r.a.cloneDeep(this.funcMeta[this.func].Methods[e].inputs)),console.log(this.funcminputs,this.funcmoutputs)},handleOk:function(){var e=this.selectedNode;e.elem.data.params={wparams:this.wparams,fparams:this.fparams,meths:this.meths,funcm:this.funcm,inp:this.funcminputs,op:this.funcmoutputs},e.elem.data.func=this.func,e.elem.data.host=this.host,this.cy.add(e.elem),this.hide(),this.resetSelectedNode()},handleCancel:function(){this.hide(),this.resetSelectedNode()}}),watch:{selectedNode:function(e,t){this.selectedNode.hasOwnProperty("elem")?this.selectedNode.elem.data.hasOwnProperty("params")&&(this.func=this.selectedNode.elem.data.func,this.funcm=this.selectedNode.elem.data.params.funcm,console.log("selected node data",this.selectedNode.elem.data),setTimeout(function(){document.getElementById(this.func).checked=!0,document.getElementById(this.funcm).checked=!0}.bind(this),500),this.wparams=this.selectedNode.elem.data.params.wparams,this.fparams=this.selectedNode.elem.data.params.fparams,this.meths=this.selectedNode.elem.data.params.meths):(this.func="",this.funcm="",this.wparams=[],this.fparams=[],this.meths=[])}},mounted:function(){}}},function(e,t,n){"use strict";var a=n(11),s=n.n(a),i=n(9);t.a={name:"editEdge",data:function(){return{inputs:[],outputs:[]}},computed:s()({},Object(i.c)({visible:"editEdgeVisible",selectedEdge:"selectedEdge",funcMeta:"funcMeta",cy:"cy"}),{myModal:{get:function(){return this.visible},set:function(e){!0===e?this.show():this.hide()}}}),methods:s()({},Object(i.d)({show:"showEditEdge",hide:"hideEditEdge"}),{handleOk:function(){var e=this.selectedEdge;e.data.inputs=this.inputs,e.data.outputs=this.outputs,this.cy.add(e),this.hide()}}),watch:{selectedEdge:function(e,t){e.data.hasOwnProperty("inputs")?(this.inputs=e.data.inputs,this.outputs=e.data.outputs):(this.inputs=[],this.outputs=[])}},mounted:function(){console.log("selectedEdge mounted"),console.log(this.selectedEdge)}}},function(e,t,n){"use strict";var a=n(11),s=n.n(a),i=n(9),o=n(46),r=n.n(o);t.a={name:"loadGraph",data:function(){return{graphId:""}},computed:s()({},Object(i.c)({visible:"loadGraphVisible",rootgraphs:"graph/rootgraphs",usergraphs:"graph/usergraphs",cy:"cy"}),{myModal:{get:function(){return this.visible},set:function(e){!0===e?this.show():this.hide()}}}),methods:s()({},Object(i.d)({show:"showLoadGraph",hide:"hideLoadGraph"}),Object(i.b)({updateRootGraphs:"graph/updateRootGraphs",updateUserGraphs:"graph/updateUserGraphs",setCurrentGraphType:"graph/setCurrentGraphType",setCurrentGraphId:"graph/setCurrentGraphId",setCurrentGraphTitle:"graph/setCurrentGraphTitle"}),{handleOk:function(){var e=r()(".sections.active").first().attr("id"),t=r()("#"+e+"graphs option:selected").val();void 0!==t&&this.loadGraph(e,t),this.hide()},openView:function(e){r()(".graphviews").addClass("hiddenView"),r()("#"+e).removeClass("hiddenView")},FindGraphById:function(e){if(String(e.graph_id)===String(this.graphId))return e},loadGraph:function(e,t){console.log(t),this.graphId=t;var n="";"root"===e?n=this.rootgraphs.find(this.FindGraphById,t):"user"===e&&(n=this.usergraphs.find(this.FindGraphById,t));var a=JSON.parse(n.content);for(var s in a.elements.nodes){var i=a.elements.nodes[s].data.name.split(":")[1];console.log("sskey2",i)," UploadData"===i&&(a.elements.nodes[s].style={"background-color":"#000"})," decomposition"===i&&(a.elements.nodes[s].style={"background-color":"#a00"})," preprocessing"===i&&(a.elements.nodes[s].style={"background-color":"#00a"})," linear_model"===i&&(a.elements.nodes[s].style={"background-color":"#aa0"})," model_selection"===i&&(a.elements.nodes[s].style={"background-color":"#0a0"})," svm"===i&&(a.elements.nodes[s].style={"background-color":"#a0a"})}this.cy.json(a),console.log("frm load",a),this.setCurrentGraphId(t),this.setCurrentGraphType(e),this.setCurrentGraphTitle(n.title)}}),mounted:function(){r()(".sections").click(function(){r()(".active").removeClass("active"),r()(this).addClass("active")}),this.updateRootGraphs(),this.updateUserGraphs()}}},function(e,t,n){"use strict";var a=n(76),s=n.n(a),i=n(11),o=n.n(i),r=n(9);t.a={name:"saveGraph",data:function(){return{newGraphTitle:"Untitled"}},computed:o()({},Object(r.c)({visible:"saveGraphVisible",clickPos:"cyClickPos",libHierarchy:"libHierarchy",cy:"cy",currentGraphId:"graph/currentGraphId",currentGraphType:"graph/currentGraphType",currentGraphTitle:"graph/currentGraphTitle"}),{myModal:{get:function(){return this.visible},set:function(e){!0===e?this.show():this.hide()}}}),methods:o()({},Object(r.d)({show:"showSaveGraph",hide:"hideSaveGraph"}),Object(r.b)({saveCurrentUserGraph:"graph/saveCurrentUserGraph"}),{handleOk:function(){var e=s()(this.cy.json());this.saveCurrentUserGraph({title:this.newGraphTitle,content:e}),this.hide()},printCyJson:function(){console.log(this.currentGraphId),console.log(this.currentGraphType)}}),mounted:function(){}}},function(e,t,n){"use strict";var a=n(76),s=n.n(a),i=n(11),o=n.n(i),r=n(9);t.a={name:"updateGraph",data:function(){return{info:"Asdf"}},computed:o()({},Object(r.c)({visible:"updateGraphVisible",clickPos:"cyClickPos",libHierarchy:"libHierarchy",cy:"cy",currentGraphId:"graph/currentGraphId",currentGraphType:"graph/currentGraphType"}),{myModal:{get:function(){return this.visible},set:function(e){!0===e?this.show():this.hide()}}}),methods:o()({},Object(r.d)({show:"showUpdateGraph",hide:"hideUpdateGraph",showSave:"showSaveGraph"}),Object(r.b)({updateCurrentUserGraph:"graph/updateCurrentUserGraph"}),{handleCancel:function(){this.hide(),this.showSave()},handleOk:function(){if("user"===this.currentGraphType){var e=s()(this.cy.json());this.updateCurrentUserGraph({id:this.currentGraphId,content:e})}this.hide()},printCyJson:function(){console.log(this.currentGraphId),console.log(this.currentGraphType)}}),mounted:function(){}}},function(e,t,n){"use strict";var a=n(76),s=n.n(a),i=n(11),o=n.n(i),r=n(9),l=n(485),u=(n.n(l),n(486)),p=(n.n(u),n(487));t.a={name:"wizard",components:{FormWizard:l.FormWizard,TabContent:l.TabContent,FileReader:p.a},data:function(){return{text:"",info:"Startup Wizard",icon:{Enter:"fa fa-database",Represent:"fa fa-cubes",Prepare:"fa fa-magic",Model:"fa fa-dashboard",Search:"fa fa-search",Visualize:"fa fa-bar-chart",Store:"fa fa-file",Finish:"fa fa-check"},selectedFunction:{},activeIndex:0,moduleCount:[],selected:""}},computed:o()({},Object(r.c)({visible:"wizardVisible",clickPos:"cyClickPos",libHierarchy:"libHierarchy",wizHierarchy:"wizHierarchy",libMeta:"libMeta",libUINames:"libUINames",cy:"cy",currentGraphId:"graph/currentGraphId",currentGraphType:"graph/currentGraphType"}),{myModal:{get:function(){return this.visible},set:function(e){!0===e?this.show():this.hide()}}}),methods:o()({},Object(r.d)({showWizard:"showWizard",hideWizard:"hideWizard"}),{show:function(){this.showWizard()},hide:function(){this.$refs.formWizard.reset(),this.hideWizard()},handleCancel:function(){this.hide()},handleOk:function(){this.hide()},clearDesc:function(){return this.selectedFunction="",!0},setActiveIndex:function(e,t){this.activeIndex=t}}),beforeCreate:function(){console.log("beforeCreate start"),console.log(s()(this.libHierarchy)),console.log("beforeCreate end")},mounted:function(){console.log("mounted start"),console.log(s()(this.libHierarchy)),console.log("mounted end")}}},function(e,t,n){"use strict";t.a={name:"fileReader",methods:{loadTextFromFile:function(e){var t=this,n=e.target.files[0],a=new FileReader;a.onload=function(e){return t.$emit("load",e.target.result)},a.readAsText(n)}}}},function(e,t,n){"use strict";var a=n(11),s=n.n(a),i=n(9),o=n(46),r=n.n(o);t.a={name:"help",data:function(){return{graphId:""}},computed:s()({},Object(i.c)({visible:"helpVisible",rootgraphs:"graph/rootgraphs",usergraphs:"graph/usergraphs",cy:"cy"}),{myModal:{get:function(){return this.visible},set:function(e){!0===e?this.show():this.hide()}}}),methods:s()({},Object(i.d)({show:"showHelp",hide:"hideHelp"}),Object(i.b)({updateRootGraphs:"graph/updateRootGraphs",updateUserGraphs:"graph/updateUserGraphs",setCurrentGraphType:"graph/setCurrentGraphType",setCurrentGraphId:"graph/setCurrentGraphId",setCurrentGraphTitle:"graph/setCurrentGraphTitle"}),{handleOk:function(){var e=r()(".sections.active").first().attr("id"),t=r()("#"+e+"graphs option:selected").val();void 0!==t&&this.help(e,t),this.hide()},openView:function(e){r()(".graphviews").addClass("hiddenView"),r()("#"+e).removeClass("hiddenView")}}),mounted:function(){r()(".sections").click(function(){r()(".active").removeClass("active"),r()(this).addClass("active")}),this.updateRootGraphs(),this.updateUserGraphs()}}},function(e,t,n){"use strict";var a=n(497);t.a={name:"tables",components:{cTable:a.a}}},function(e,t,n){"use strict";var a=n(11),s=n.n(a),i=n(9);t.a={name:"c-table",props:{caption:{type:String,default:"Table"},hover:{type:Boolean,default:!1},striped:{type:Boolean,default:!1},bordered:{type:Boolean,default:!1},small:{type:Boolean,default:!1},fixed:{type:Boolean,default:!1}},data:function(){return{items:[{username:"Samppa Nori",registered:"2012/01/01",role:"Member",status:"Active"},{username:"Estavan Lykos",registered:"2012/02/01",role:"Staff",status:"Banned"},{username:"Chetan Mohamed",registered:"2012/02/01",role:"Admin",status:"Inactive"},{username:"Derick Maximinus",registered:"2012/03/01",role:"Member",status:"Pending"},{username:"Friderik Dávid",registered:"2012/01/21",role:"Staff",status:"Active"},{username:"Yiorgos Avraamu",registered:"2012/01/01",role:"Member",status:"Active"},{username:"Avram Tarasios",registered:"2012/02/01",role:"Staff",status:"Banned"},{username:"Quintin Ed",registered:"2012/02/01",role:"Admin",status:"Inactive"},{username:"Enéas Kwadwo",registered:"2012/03/01",role:"Member",status:"Pending"},{username:"Agapetus Tadeáš",registered:"2012/01/21",role:"Staff",status:"Active"},{username:"Carwyn Fachtna",registered:"2012/01/01",role:"Member",status:"Active"},{username:"Nehemiah Tatius",registered:"2012/02/01",role:"Staff",status:"Banned"},{username:"Ebbe Gemariah",registered:"2012/02/01",role:"Admin",status:"Inactive"},{username:"Eustorgios Amulius",registered:"2012/03/01",role:"Member",status:"Pending"},{username:"Leopold Gáspár",registered:"2012/01/21",role:"Staff",status:"Active"},{username:"Pompeius René",registered:"2012/01/01",role:"Member",status:"Active"},{username:"Paĉjo Jadon",registered:"2012/02/01",role:"Staff",status:"Banned"},{username:"Micheal Mercurius",registered:"2012/02/01",role:"Admin",status:"Inactive"},{username:"Ganesha Dubhghall",registered:"2012/03/01",role:"Member",status:"Pending"},{username:"Hiroto Šimun",registered:"2012/01/21",role:"Staff",status:"Active"},{username:"Vishnu Serghei",registered:"2012/01/01",role:"Member",status:"Active"},{username:"Zbyněk Phoibos",registered:"2012/02/01",role:"Staff",status:"Banned"},{username:"Einar Randall",registered:"2012/02/01",role:"Admin",status:"Inactive"},{username:"Félix Troels",registered:"2012/03/21",role:"Staff",status:"Active"},{username:"Aulus Agmundr",registered:"2012/01/01",role:"Member",status:"Pending"}],fields:[{key:"username"},{key:"registered"},{key:"role"},{key:"status"}],graphruns_fields:[{key:"id"},{key:"graph"},{key:"start_time"},{key:"status"},{key:"actions"}],currentPage:1,perPage:10,totalRows:0}},computed:s()({},Object(i.c)({graphruns:"graph/graphruns"})),methods:s()({},Object(i.b)({setGraphRuns:"graph/setGraphRuns"}),{working:function(e){alert(e)},getBadge:function(e){return"Started"===e?"secondary":"Success"===e?"success":"NA"===e?"warning":"Error"===e?"danger":"primary"},getRowCount:function(e){return e.length}}),mounted:function(){this.setGraphRuns()}}},function(e,t,n){"use strict";t.a={name:"Page404"}},function(e,t,n){"use strict";t.a={name:"Page500"}},function(e,t,n){"use strict";var a=n(11),s=n.n(a),i=n(9);t.a={name:"Login",data:function(){return{user:"",pass:""}},computed:s()({},Object(i.c)({loginFailed:"auth/loginFailed"})),methods:s()({},Object(i.b)({authLogin:"auth/login"}),{login:function(e,t){var n=this;this.authLogin({username:e,password:t}).then(function(){return n.$router.push("/")})}})}},function(e,t,n){"use strict";t.a={name:"Register"}},function(e,t,n){"use strict";Object.defineProperty(t,"__esModule",{value:!0});var a=n(47),s=n(184),i=n(291),o=n(295),r=n(111);a.a.use(s.a),new a.a({el:"#app",router:o.a,store:r.a,template:"<App/>",components:{App:i.a}})},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,function(e,t){},,,,,,,,,,,,,,,,,,function(e,t){},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,function(e,t){},,,,,,,,,,,,,,function(e,t,n){"use strict";function a(e){n(292),n(293)}var s=n(110),i=n(294),o=n(2),r=a,l=Object(o.a)(s.a,i.a,i.b,!1,r,null,null);t.a=l.exports},function(e,t){},function(e,t){},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement;return(e._self._c||t)("router-view")},s=[]},function(e,t,n){"use strict";(function(e){var a=n(47),s=n(296),i=n(111),o=n(362),r=n(405),l=n(407),u=n(496),p=n(500),d=n(502),c=n(504),m=n(506),h=n(508);a.a.use(s.a);var f=function(e,t,n){i.a.dispatch("auth/logout").then(function(){return n("/pages/login")})},_=new s.a({mode:"hash",base:e,linkActiveClass:"open active",scrollBehavior:function(){return{y:0}},routes:[{path:"/",redirect:"/Landing",name:"Home",component:o.a,children:[{path:"landing",name:"Landing",component:r.default,meta:{requiresAuth:!0}},{path:"dashboard",name:"Dashboard",component:l.a,meta:{requiresAuth:!0}},{path:"results",name:"Results",component:u.a,meta:{requiresAuth:!0}},{path:"analysis",name:"Analysis",component:p.a,meta:{requiresAuth:!0}}]},{path:"/pages",redirect:"/pages/404",name:"Pages",component:{render:function(e){return e("router-view")}},children:[{path:"404",name:"Page404",component:d.a},{path:"500",name:"Page500",component:c.a},{path:"login",name:"Login",component:m.a,meta:{requiresUnauth:!0}},{path:"register",name:"Register",component:h.a}]},{path:"/logout",beforeEnter:f}]});_.beforeEach(function(e,t,n){e.matched.some(function(e){return e.meta.requiresAuth})?i.a.dispatch("auth/initialize").then(function(){i.a.getters["auth/isAuthenticated"]?n():n("/pages/login")}):e.matched.some(function(e){return e.meta.requiresUnauth})?i.a.dispatch("auth/initialize").then(function(){i.a.getters["auth/isAuthenticated"]?n("/pages/dashboard"):n()}):n()}),t.a=_}).call(t,"/")},,function(e,t,n){"use strict";var a=n(298),s=n.n(a),i=n(325),o=n(44),r=n(352),l=n(70),u=n.n(l),p={cytoscapeConfig:{},edgehandlesConfig:{},contextMenusConfig:{},funcMeta:{},libMeta:{},libUINames:{},cy:{},cyClickPos:{x:20,y:20},libHierarchy:{},wizHierarchy:{},selectedNode:{},selectedEdge:{},initGraph:{}},d={cytoscapeConfig:function(e){return e.cytoscapeConfig},edgehandlesConfig:function(e){return e.edgehandlesConfig},contextMenusConfig:function(e){return e.contextMenusConfig},libHierarchy:function(e){return e.libHierarchy},wizHierarchy:function(e){return e.wizHierarchy},funcMeta:function(e){return e.funcMeta},libMeta:function(e){return e.libMeta},libUINames:function(e){return e.libUINames},elements:function(e){return e.cy.elements},cy:function(e){return e.cy},cyClickPos:function(e){return e.cyClickPos},selectedNode:function(e){return e.selectedNode},selectedEdge:function(e){return e.selectedEdge},initGraph:function(e){return e.initGraph}},c={setCytoscapeInitConfig:function(e,t){var n=e.commit;return new s.a(function(e,a){i.a.getCytoscapeInitConfig(function(a){a.container=t,n("setCytoscapeConfig",a),e()})})},setEdgehandlesConfig:function(e,t){var n=e.commit;return new s.a(function(e,a){i.a.getEdgehandlesConfig(function(a){a.complete=r.a.getEdgeDropFunction(t),n("setEdgehandlesConfig",a),e()})})},setLibHierarchy:function(e){var t=e.commit;return new s.a(function(e,n){i.a.getLibHierarchy(function(n){t("setLibHierarchy",n),t("setWizHierarchy",n),e()})})},setFuncMeta:function(e){var t=e.commit;return new s.a(function(e,n){i.a.getFuncMeta(function(n){t("setFuncMeta",n),e()})})},setLibMeta:function(e){var t=e.commit;return new s.a(function(e,n){i.a.getLibMeta(function(n){t("setLibMeta",n),e()})})},setLibUINames:function(e){var t=e.commit;return new s.a(function(e,n){i.a.getlibUINames(function(n){t("setLibUINames",n),e()})})},setContextMenusConfig:function(e,t){var n=e.commit;return new s.a(function(e,a){i.a.getContextMenusConfig(function(a){a.menuItems[0].onClickFunction=r.a.getEditNodeOnClickFunction(t),a.menuItems[1].onClickFunction=r.a.getEditEdgeOnClickFunction(t),a.menuItems[2].onClickFunction=r.a.getRemoveNodeOnClickFunction(t),a.menuItems[3].onClickFunction=r.a.getRemoveEdgeOnClickFunction(t),a.menuItems[4].onClickFunction=r.a.getAddNodeOnClickFunction(t),n("setContextMenusConfig",a),e()})})},setCy:function(e,t){var n=e.commit;return new s.a(function(e,a){n("setCy",t),e()})},setInitGraph:function(e){var t=e.commit;return new s.a(function(e,n){o.a.get("/api/graphs/").then(function(n){t("setInitGraph",n.data),e()},function(e){console.error(e),n(e)})})}},m={setCytoscapeConfig:function(e,t){e.cytoscapeConfig=t},setEdgehandlesConfig:function(e,t){e.edgehandlesConfig=t},setLibHierarchy:function(e,t){e.libHierarchy=t},setWizHierarchy:function(e,t){var n=u.a.cloneDeep(t);n.Finish="finish",e.wizHierarchy=n},setFuncMeta:function(e,t){e.funcMeta=t},setLibMeta:function(e,t){e.libMeta=t},setLibUINames:function(e,t){e.libUINames=t},setContextMenusConfig:function(e,t){e.contextMenusConfig=t},setCy:function(e,t){e.cy=t},setCyClickPos:function(e,t){console.log(t),e.cyClickPos.x=t.x,e.cyClickPos.y=t.y},setSelectedNodeId:function(e,t){e.selectedNode={id:t}},setSelectedNodeElem:function(e,t){e.selectedNode.elem=t},resetSelectedNode:function(e){e.selectedNode={}},setSelectedEdge:function(e,t){e.selectedEdge=t},setInitGraph:function(e,t){e.initGraph=t}};t.a={state:p,getters:d,actions:c,mutations:m}},,,,,,,,,,,,,,,,,,,,,,,,,,,,function(e,t,n){"use strict";var a=n(326),s={container:null,boxSelectionEnabled:!1,autounselectify:!0,minZoom:.2,maxZoom:2,layout:{name:"dagre"},style:[{selector:"node",style:{content:"data(name)","text-opacity":.5,"text-valign":"center","text-halign":"right","background-color":"#11479e"}},{selector:"edge",style:{"curve-style":"bezier",width:4,"target-arrow-shape":"triangle","line-color":"#9dbaea","target-arrow-color":"#9dbaea"}},{selector:".eh-handle",style:{"background-color":"red",width:12,height:12,shape:"ellipse","overlay-opacity":0,"border-width":12,"border-opacity":0}},{selector:".eh-hover",style:{"background-color":"red"}},{selector:".eh-source",style:{"border-width":2,"border-color":"green"}},{selector:".eh-target",style:{"border-width":2,"border-color":"blue"}},{selector:".eh-preview, .eh-ghost-edge",style:{"background-color":"red","line-color":"red","target-arrow-color":"red","source-arrow-color":"red"}}],elements:null},i={toggleOffOnLeave:!0,handleNodes:"node",handleSize:10,edgeType:function(e,t){return"flat"}},o={menuItems:[{id:"edit-node",content:"Edit Node",tooltipText:"Edit Node",selector:"node",onClickFunction:function(e){}},{id:"edit-edge",content:"Edit Edge",tooltipText:"Edit Edge",selector:"edge",onClickFunction:function(e){}},{id:"remove-node",content:"Remove Node",tooltipText:"Remove Node",selector:"node",onClickFunction:function(e){}},{id:"remove-edge",content:"Remove Edge",tooltipText:"Remove Edge",selector:"edge",onClickFunction:function(e){}},{id:"add-node",content:"Add Node",tooltipText:"Add Node",coreAsWell:!0,onClickFunction:function(e){}}],menuItemClasses:["custom-menu-item"],contextMenuClasses:["custom-context-menu"]},r={table:"Input data in Table format from csv or excel file",Convert:"Convert the input file from one format to another",datasets:"Use one of the many standard datasets available",xyz:"Input data in the XYZ format","python script":"Use Python Script to get the input to the system","molecular descriptors":"Select the representation format of molecules for the chemical datasets","inorganic input":"Inorganic input details","distance matrix":"distance matrix details","inorganic descriptors":"inorganic descriptors details","data manipulation":"Use this split or concatenate the dataset","feature transformation":"Run KernalPCA or PCA on the data","data cleaning":"Select from the different data cleaning methods",scaling:"Feature Scaling","feature representation":"Select on how to represent the features - binarized, one-hot encoded or as polynomial features",split:"Select a method on how to split the data",regression:"Select from the different regression methods available",evaluate:"Use to to evaluate",validate:"Select from the different validation methods",grid:"Use grid search","genetic algorithm":"Use GA_DEAP",plot:"Use this to plot the data as histogram or scatterplot",artist:"Lorem ipsum",figure:"Use this to save the generated plot",file:"Use this to save the data into a file",NA:"Skip creating a node"},l={table:"Table",Convert:"Convert",datasets:"Datasets",xyz:"XYZ","python script":"Python Script","molecular descriptors":"Molecular Descriptors","inorganic input":"Inorganic Input","distance matrix":"Distance Matrix","inorganic descriptors":"Inorganic Descriptors","data manipulation":"Data Manipulation","feature transformation":"Feature Transformation","data cleaning":"Data Cleaning",scaling:"Feature Scaling","feature representation":"Feature Representation",split:"Split",regression:"Regression",evaluate:"Evaluate",validate:"Validate",grid:"Grid","genetic algorithm":"Genetic Algorithm",plot:"Plot",artist:"Artist",figure:"Figure",file:"File"};t.a={getCytoscapeInitConfig:function(e){var t=s;setTimeout(function(){return e(t)},5)},getEdgehandlesConfig:function(e){setTimeout(function(){return e(i)},5)},getContextMenusConfig:function(e){setTimeout(function(){return e(o)},5)},getLibHierarchy:function(e){setTimeout(function(){return e(a.a.lh)},5)},getFuncMeta:function(e){setTimeout(function(){return e(a.a.fp)},5)},getLibMeta:function(e){setTimeout(function(){return e(r)},5)},getlibUINames:function(e){setTimeout(function(){return e(l)},5)},buyProducts:function(e,t,n){setTimeout(function(){Math.random()>.5||navigator.userAgent.indexOf("PhantomJS")>-1?t():n()},100)}}},function(e,t,n){"use strict";var a=n(327),s=n(328),i=n(329),o=n(330),r=n(331),l=n(332),u={Enter:{},Represent:{},Prepare:{},Model:{},Search:{},Mix:{},Visualize:{},Save:{}},p={};u.Enter.UploadData={pandas:{name:[],functions:[]}},u.Search.model_selection={sklearn:{name:[],functions:[]}},u.Search.svm={sklearn:{name:[],functions:[]}},u.Represent.decomposition={sklearn:{name:[],functions:[]}},u.Prepare.preprocessing={sklearn:{name:[],functions:[]}},u.Model.linear_model={sklearn:{name:[],functions:[]}};for(var d=0;d<o.a.nodes.length;d++){var c=o.a.nodes[d];u.Search.svm.sklearn.name.push(c.name),p[c.name]={FParameters:[],Methods:{}};for(var m=["obj"],h=0;h<c.inputs.length;h++){var f=c.inputs[h];p[c.name].FParameters.push({param_type:f.param_type,display_name:f.name,name:f.name,value:f.default_value,desc:f.docstring,is_optional:f.is_optional})}for(var _=0;_<c.node_functions.length;_++){var g=[],y=[],b=c.node_functions[_];if("_"!==b.name.split("")[0]){p[c.name].Methods[b.name]={},m.push(b.name);for(var v=0;v<b.inputs.length;v++){var w=b.inputs[v];y.push({name:w.name,docstring:w.docstring,param_type:w.param_type,is_optional:w.is_optional})}for(var x=0;x<b.outputs.length;x++){var T=b.outputs[x];g.push({name:T.name,docstring:T.docstring,param_type:T.param_type,is_optional:T.is_optional})}p[c.name].Methods[b.name].inputs=y,p[c.name].Methods[b.name].outputs=g}}u.Search.svm.sklearn.functions.push(m)}for(var I=0;I<a.a.nodes.length;I++){var S=a.a.nodes[I];u.Search.model_selection.sklearn.name.push(S.name),m=["obj"],p[S.name]={FParameters:[],Methods:{}};for(var N=0;N<S.inputs.length;N++){var k=S.inputs[N];p[S.name].FParameters.push({param_type:k.param_type,display_name:k.name,name:k.name,value:k.default_value,desc:k.docstring,is_optional:k.is_optional})}for(var F=0;F<S.node_functions.length;F++){var X=S.node_functions[F],C=X.name.split("")[0];if(g=[],y=[],"_"!==C){p[S.name].Methods[X.name]={},m.push(X.name);for(var R=0;R<X.inputs.length;R++){var j=X.inputs[R];y.push({name:j.name,docstring:j.docstring,param_type:j.param_type,is_optional:j.is_optional})}for(var A=0;A<X.outputs.length;A++){var P=X.outputs[A];g.push({name:P.name,docstring:P.docstring,param_type:P.param_type,is_optional:P.is_optional})}p[S.name].Methods[X.name].inputs=y,p[S.name].Methods[X.name].outputs=g}}u.Search.model_selection.sklearn.functions.push(m)}for(var z=0;z<l.a.node_functions.length;z++){var L=l.a.node_functions[z];u.Enter.UploadData.pandas.name.push(L.name),p[L.name]={FParameters:[],Methods:{}};for(var G=0;G<L.inputs.length;G++){var E=L.inputs[G];p[L.name].FParameters.push({name:E.name,value:E.default_value,is_optional:E.is_optional})}var O=L.outputs[0],M=L.outputs[1];p[L.name].Methods[O.name]={},p[L.name].Methods[O.name].outputs=[{name:O.name},{name:M.name}]}for(var D=0;D<s.a.nodes.length;D++){var V=s.a.nodes[D];u.Represent.decomposition.sklearn.name.push(V.name),m=["obj"],p[V.name]={FParameters:[],Methods:{}};for(var W=0;W<V.inputs.length;W++){var q=V.inputs[W];p[V.name].FParameters.push({param_type:q.param_type,display_name:q.name,name:q.name,value:q.default_value,desc:q.docstring,is_optional:q.is_optional})}for(var U=0;U<V.node_functions.length;U++){var K=V.node_functions[U],H=K.name.split("")[0];if(g=[],y=[],"_"!==H){p[V.name].Methods[K.name]={},m.push(K.name);for(var B=0;B<K.inputs.length;B++){var $=K.inputs[B];y.push({name:$.name,docstring:$.docstring,param_type:$.param_type,is_optional:$.is_optional})}for(var Y=0;Y<K.outputs.length;Y++){var Z=K.outputs[Y];g.push({name:Z.name,docstring:Z.docstring,param_type:Z.param_type,is_optional:Z.is_optional})}p[V.name].Methods[K.name].inputs=y,p[V.name].Methods[K.name].outputs=g}}u.Represent.decomposition.sklearn.functions.push(m)}for(var J=0;J<i.a.nodes.length;J++){var Q=i.a.nodes[J];u.Prepare.preprocessing.sklearn.name.push(Q.name),m=["obj"],p[Q.name]={FParameters:[],Methods:{}};for(var ee=0;ee<Q.inputs.length;ee++){var te=Q.inputs[ee];p[Q.name].FParameters.push({name:te.name,value:te.default_value,is_optional:te.is_optional})}for(var ne=0;ne<Q.node_functions.length;ne++){var ae=Q.node_functions[ne],se=ae.name.split("")[0];if(g=[],y=[],"_"!==se){m.push(ae.name),p[Q.name].Methods[ae.name]={};for(var ie=0;ie<ae.inputs.length;ie++){var oe=ae.inputs[ie];y.push({name:oe.name,is_optional:oe.is_optional})}for(var re=0;re<ae.outputs.length;re++){var le=ae.outputs[re];g.push({name:le.name,is_optional:le.is_optional})}p[Q.name].Methods[ae.name].inputs=y,p[Q.name].Methods[ae.name].outputs=g}}u.Prepare.preprocessing.sklearn.functions.push(m)}for(var ue=0;ue<r.a.nodes.length;ue++){var pe=r.a.nodes[ue];u.Model.linear_model.sklearn.name.push(pe.name),m=["obj"],p[pe.name]={FParameters:[],Methods:{}};for(var de=0;de<pe.inputs.length;de++){var ce=pe.inputs[de];p[pe.name].FParameters.push({param_type:ce.param_type,display_name:ce.name,name:ce.name,value:ce.default_value,desc:ce.docstring,is_optional:ce.is_optional})}for(var me=0;me<pe.node_functions.length;me++){var he=pe.node_functions[me],fe=he.name.split("")[0];if(y=[],g=[],"_"!==fe){p[pe.name].Methods[he.name]={},m.push(he.name);for(var _e=0;_e<he.inputs.length;_e++){var ge=he.inputs[_e];y.push({name:ge.name,docstring:ge.docstring,param_type:ge.param_type,is_optional:ge.is_optional})}for(var ye=0;ye<he.outputs.length;ye++){var be=he.outputs[ye];g.push({name:be.name,docstring:be.docstring,param_type:be.param_type,is_optional:be.is_optional})}p[pe.name].Methods[he.name].inputs=y,p[pe.name].Methods[he.name].outputs=g}}u.Model.linear_model.sklearn.functions.push(m)}console.log("idhar",p,u),t.a={lh:u,fp:p}},function(e,t,n){"use strict";t.a={name:"sklearn.model_selection",docstring:"",inputs:[],outputs:[],node_functions:[{name:"check_cv",docstring:"Input checker utility for building a cross-validator\n\n    Parameters\n    ----------\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if classifier is True and ``y`` is either\n        binary or multiclass, :class:`StratifiedKFold` is used. In all other\n        cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value will change from 3-fold to 5-fold in v0.22.\n\n    y : array-like, optional\n        The target variable for supervised learning problems.\n\n    classifier : boolean, optional, default False\n        Whether the task is a classification task, in which case\n        stratified KFold will be used.\n\n    Returns\n    -------\n    checked_cv : a cross-validator instance.\n        The return value is a cross-validator which generates the train/test\n        splits via the ``split`` method.\n    ",inputs:[{name:"cv",docstring:"Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, if classifier is True and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.20     ``cv`` default value will change from 3-fold to 5-fold in v0.22.",param_type:["int","iter"],expected_shape:null,is_optional:!0,default_value:null},{name:"y",docstring:"The target variable for supervised learning problems.",param_type:["array"],expected_shape:null,is_optional:!0,default_value:null},{name:"classifier",docstring:"Whether the task is a classification task, in which case stratified KFold will be used.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:null}],outputs:[{name:"checked_cv",docstring:"The return value is a cross-validator which generates the train/test splits via the ``split`` method.",param_type:[null],returned:!0}]},{name:"cross_val_predict",docstring:"Generate cross-validated estimates for each input data point\n\n    It is not appropriate to pass these predictions into an evaluation\n    metric. Use :func:`cross_validate` to measure generalization error.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    estimator : estimator object implementing 'fit' and 'predict'\n        The object to use to fit the data.\n\n    X : array-like\n        The data to fit. Can be, for example a list, or an array at least 2d.\n\n    y : array-like, optional, default: None\n        The target variable to try to predict in the case of\n        supervised learning.\n\n    groups : array-like, with shape (n_samples,), optional\n        Group labels for the samples used while splitting the dataset into\n        train/test set.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value if None will change from 3-fold to 5-fold\n            in v0.22.\n\n    n_jobs : int or None, optional (default=None)\n        The number of CPUs to use to do the computation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : integer, optional\n        The verbosity level.\n\n    fit_params : dict, optional\n        Parameters to pass to the fit method of the estimator.\n\n    pre_dispatch : int, or string, optional\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n            - None, in which case all the jobs are immediately\n              created and spawned. Use this for lightweight and\n              fast-running jobs, to avoid delays due to on-demand\n              spawning of the jobs\n\n            - An int, giving the exact number of total jobs that are\n              spawned\n\n            - A string, giving an expression as a function of n_jobs,\n              as in '2*n_jobs'\n\n    method : string, optional, default: 'predict'\n        Invokes the passed method name of the passed estimator. For\n        method='predict_proba', the columns correspond to the classes\n        in sorted order.\n\n    Returns\n    -------\n    predictions : ndarray\n        This is the result of calling ``method``\n\n    See also\n    --------\n    cross_val_score : calculate score for each CV split\n\n    cross_validate : calculate one or more scores and timings for each CV split\n\n    Notes\n    -----\n    In the case that one or more classes are absent in a training portion, a\n    default score needs to be assigned to all instances for that class if\n    ``method`` produces columns per class, as in {'decision_function',\n    'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is\n    0.  In order to ensure finite output, we approximate negative infinity by\n    the minimum finite float value for the dtype in other cases.\n\n    Examples\n    --------\n    >>> from sklearn import datasets, linear_model\n    >>> from sklearn.model_selection import cross_val_predict\n    >>> diabetes = datasets.load_diabetes()\n    >>> X = diabetes.data[:150]\n    >>> y = diabetes.target[:150]\n    >>> lasso = linear_model.Lasso()\n    >>> y_pred = cross_val_predict(lasso, X, y, cv=3)\n    ",inputs:[{name:"estimator",docstring:"The object to use to fit the data.",param_type:["object","dict"],expected_shape:null,is_optional:!1,default_value:null},{name:"X",docstring:"The data to fit. Can be, for example a list, or an array at least 2d.",param_type:["array"],expected_shape:null,is_optional:!1,default_value:null},{name:"y",docstring:"The target variable to try to predict in the case of supervised learning.",param_type:["array",null],expected_shape:null,is_optional:!0,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null},{name:"cv",docstring:"Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.20     ``cv`` default value if None will change from 3-fold to 5-fold     in v0.22.",param_type:["int","iter"],expected_shape:null,is_optional:!0,default_value:null},{name:"n_jobs",docstring:"The number of CPUs to use to do the computation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null},{name:"verbose",docstring:"The verbosity level.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:null},{name:"fit_params",docstring:"Parameters to pass to the fit method of the estimator.",param_type:["dict"],expected_shape:null,is_optional:!0,default_value:null},{name:"pre_dispatch",docstring:"Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:      - None, in which case all the jobs are immediately       created and spawned. Use this for lightweight and       fast-running jobs, to avoid delays due to on-demand       spawning of the jobs      - An int, giving the exact number of total jobs that are       spawned      - A string, giving an expression as a function of n_jobs,       as in '2*n_jobs'",param_type:["int","str"],expected_shape:null,is_optional:!0,default_value:null},{name:"method",docstring:"Invokes the passed method name of the passed estimator. For method='predict_proba', the columns correspond to the classes in sorted order.",param_type:["str","dict"],expected_shape:null,is_optional:!0,default_value:null}],outputs:[{name:"predictions",docstring:"This is the result of calling ``method``",param_type:["array"],returned:!0}]},{name:"cross_val_score",docstring:"Evaluate a score by cross-validation\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    estimator : estimator object implementing 'fit'\n        The object to use to fit the data.\n\n    X : array-like\n        The data to fit. Can be for example a list, or an array.\n\n    y : array-like, optional, default: None\n        The target variable to try to predict in the case of\n        supervised learning.\n\n    groups : array-like, with shape (n_samples,), optional\n        Group labels for the samples used while splitting the dataset into\n        train/test set.\n\n    scoring : string, callable or None, optional, default: None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)`` which should return only\n        a single value.\n\n        Similar to :func:`cross_validate`\n        but only a single metric is permitted.\n\n        If None, the estimator's default scorer (if available) is used.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value if None will change from 3-fold to 5-fold\n            in v0.22.\n\n    n_jobs : int or None, optional (default=None)\n        The number of CPUs to use to do the computation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : integer, optional\n        The verbosity level.\n\n    fit_params : dict, optional\n        Parameters to pass to the fit method of the estimator.\n\n    pre_dispatch : int, or string, optional\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n            - None, in which case all the jobs are immediately\n              created and spawned. Use this for lightweight and\n              fast-running jobs, to avoid delays due to on-demand\n              spawning of the jobs\n\n            - An int, giving the exact number of total jobs that are\n              spawned\n\n            - A string, giving an expression as a function of n_jobs,\n              as in '2*n_jobs'\n\n    error_score : 'raise' | 'raise-deprecating' or numeric\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised.\n        If set to 'raise-deprecating', a FutureWarning is printed before the\n        error is raised.\n        If a numeric value is given, FitFailedWarning is raised. This parameter\n        does not affect the refit step, which will always raise the error.\n        Default is 'raise-deprecating' but from version 0.22 it will change\n        to np.nan.\n\n    Returns\n    -------\n    scores : array of float, shape=(len(list(cv)),)\n        Array of scores of the estimator for each run of the cross validation.\n\n    Examples\n    --------\n    >>> from sklearn import datasets, linear_model\n    >>> from sklearn.model_selection import cross_val_score\n    >>> diabetes = datasets.load_diabetes()\n    >>> X = diabetes.data[:150]\n    >>> y = diabetes.target[:150]\n    >>> lasso = linear_model.Lasso()\n    >>> print(cross_val_score(lasso, X, y, cv=3))  # doctest: +ELLIPSIS\n    [0.33150734 0.08022311 0.03531764]\n\n    See Also\n    ---------\n    :func:`sklearn.model_selection.cross_validate`:\n        To run cross-validation on multiple metrics and also to return\n        train scores, fit times and score times.\n\n    :func:`sklearn.model_selection.cross_val_predict`:\n        Get predictions from each split of cross-validation for diagnostic\n        purposes.\n\n    :func:`sklearn.metrics.make_scorer`:\n        Make a scorer from a performance metric or loss function.\n\n    ",inputs:[{name:"estimator",docstring:"The object to use to fit the data.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"X",docstring:"The data to fit. Can be for example a list, or an array.",param_type:["array"],expected_shape:null,is_optional:!1,default_value:null},{name:"y",docstring:"The target variable to try to predict in the case of supervised learning.",param_type:["array",null],expected_shape:null,is_optional:!0,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null},{name:"scoring",docstring:"A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)`` which should return only a single value.  Similar to :func:`cross_validate` but only a single metric is permitted.  If None, the estimator's default scorer (if available) is used.",param_type:["str","callable",null],expected_shape:null,is_optional:!0,default_value:null},{name:"cv",docstring:"Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.20     ``cv`` default value if None will change from 3-fold to 5-fold     in v0.22.",param_type:["int","iter"],expected_shape:null,is_optional:!0,default_value:null},{name:"n_jobs",docstring:"The number of CPUs to use to do the computation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null},{name:"verbose",docstring:"The verbosity level.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:null},{name:"fit_params",docstring:"Parameters to pass to the fit method of the estimator.",param_type:["dict"],expected_shape:null,is_optional:!0,default_value:null},{name:"pre_dispatch",docstring:"Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:      - None, in which case all the jobs are immediately       created and spawned. Use this for lightweight and       fast-running jobs, to avoid delays due to on-demand       spawning of the jobs      - An int, giving the exact number of total jobs that are       spawned      - A string, giving an expression as a function of n_jobs,       as in '2*n_jobs'",param_type:["int","str"],expected_shape:null,is_optional:!0,default_value:null},{name:"error_score",docstring:"Value to assign to the score if an error occurs in estimator fitting. If set to 'raise', the error is raised. If set to 'raise-deprecating', a FutureWarning is printed before the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. Default is 'raise-deprecating' but from version 0.22 it will change to np.nan.",param_type:["LIST_VALID_OPTIONS"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[{name:"scores",docstring:"Array of scores of the estimator for each run of the cross validation.",param_type:["array","float","list"],returned:!0}]},{name:"cross_validate",docstring:"Evaluate metric(s) by cross-validation and also record fit/score times.\n\n    Read more in the :ref:`User Guide <multimetric_cross_validation>`.\n\n    Parameters\n    ----------\n    estimator : estimator object implementing 'fit'\n        The object to use to fit the data.\n\n    X : array-like\n        The data to fit. Can be for example a list, or an array.\n\n    y : array-like, optional, default: None\n        The target variable to try to predict in the case of\n        supervised learning.\n\n    groups : array-like, with shape (n_samples,), optional\n        Group labels for the samples used while splitting the dataset into\n        train/test set.\n\n    scoring : string, callable, list/tuple, dict or None, default: None\n        A single string (see :ref:`scoring_parameter`) or a callable\n        (see :ref:`scoring`) to evaluate the predictions on the test set.\n\n        For evaluating multiple metrics, either give a list of (unique) strings\n        or a dict with names as keys and callables as values.\n\n        NOTE that when using custom scorers, each scorer should return a single\n        value. Metric functions returning a list/array of values can be wrapped\n        into multiple scorers that return one value each.\n\n        See :ref:`multimetric_grid_search` for an example.\n\n        If None, the estimator's score method is used.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value if None will change from 3-fold to 5-fold\n            in v0.22.\n\n    n_jobs : int or None, optional (default=None)\n        The number of CPUs to use to do the computation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : integer, optional\n        The verbosity level.\n\n    fit_params : dict, optional\n        Parameters to pass to the fit method of the estimator.\n\n    pre_dispatch : int, or string, optional\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n            - None, in which case all the jobs are immediately\n              created and spawned. Use this for lightweight and\n              fast-running jobs, to avoid delays due to on-demand\n              spawning of the jobs\n\n            - An int, giving the exact number of total jobs that are\n              spawned\n\n            - A string, giving an expression as a function of n_jobs,\n              as in '2*n_jobs'\n\n    return_train_score : boolean, default=False\n        Whether to include train scores.\n        Computing training scores is used to get insights on how different\n        parameter settings impact the overfitting/underfitting trade-off.\n        However computing the scores on the training set can be computationally\n        expensive and is not strictly required to select the parameters that\n        yield the best generalization performance.\n\n    return_estimator : boolean, default False\n        Whether to return the estimators fitted on each split.\n\n    error_score : 'raise' | 'raise-deprecating' or numeric\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised.\n        If set to 'raise-deprecating', a FutureWarning is printed before the\n        error is raised.\n        If a numeric value is given, FitFailedWarning is raised. This parameter\n        does not affect the refit step, which will always raise the error.\n        Default is 'raise-deprecating' but from version 0.22 it will change\n        to np.nan.\n\n    Returns\n    -------\n    scores : dict of float arrays of shape=(n_splits,)\n        Array of scores of the estimator for each run of the cross validation.\n\n        A dict of arrays containing the score/time arrays for each scorer is\n        returned. The possible keys for this ``dict`` are:\n\n            ``test_score``\n                The score array for test scores on each cv split.\n            ``train_score``\n                The score array for train scores on each cv split.\n                This is available only if ``return_train_score`` parameter\n                is ``True``.\n            ``fit_time``\n                The time for fitting the estimator on the train\n                set for each cv split.\n            ``score_time``\n                The time for scoring the estimator on the test set for each\n                cv split. (Note time for scoring on the train set is not\n                included even if ``return_train_score`` is set to ``True``\n            ``estimator``\n                The estimator objects for each cv split.\n                This is available only if ``return_estimator`` parameter\n                is set to ``True``.\n\n    Examples\n    --------\n    >>> from sklearn import datasets, linear_model\n    >>> from sklearn.model_selection import cross_validate\n    >>> from sklearn.metrics.scorer import make_scorer\n    >>> from sklearn.metrics import confusion_matrix\n    >>> from sklearn.svm import LinearSVC\n    >>> diabetes = datasets.load_diabetes()\n    >>> X = diabetes.data[:150]\n    >>> y = diabetes.target[:150]\n    >>> lasso = linear_model.Lasso()\n\n    Single metric evaluation using ``cross_validate``\n\n    >>> cv_results = cross_validate(lasso, X, y, cv=3)\n    >>> sorted(cv_results.keys())                         # doctest: +ELLIPSIS\n    ['fit_time', 'score_time', 'test_score']\n    >>> cv_results['test_score']    # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n    array([0.33150734, 0.08022311, 0.03531764])\n\n    Multiple metric evaluation using ``cross_validate``\n    (please refer the ``scoring`` parameter doc for more information)\n\n    >>> scores = cross_validate(lasso, X, y, cv=3,\n    ...                         scoring=('r2', 'neg_mean_squared_error'),\n    ...                         return_train_score=True)\n    >>> print(scores['test_neg_mean_squared_error'])      # doctest: +ELLIPSIS\n    [-3635.5... -3573.3... -6114.7...]\n    >>> print(scores['train_r2'])                         # doctest: +ELLIPSIS\n    [0.28010158 0.39088426 0.22784852]\n\n    See Also\n    ---------\n    :func:`sklearn.model_selection.cross_val_score`:\n        Run cross-validation for single metric evaluation.\n\n    :func:`sklearn.model_selection.cross_val_predict`:\n        Get predictions from each split of cross-validation for diagnostic\n        purposes.\n\n    :func:`sklearn.metrics.make_scorer`:\n        Make a scorer from a performance metric or loss function.\n\n    ",inputs:[{name:"estimator",docstring:"The object to use to fit the data.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"X",docstring:"The data to fit. Can be for example a list, or an array.",param_type:["array"],expected_shape:null,is_optional:!1,default_value:null},{name:"y",docstring:"The target variable to try to predict in the case of supervised learning.",param_type:["array",null],expected_shape:null,is_optional:!0,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null},{name:"scoring",docstring:"A single string (see :ref:`scoring_parameter`) or a callable (see :ref:`scoring`) to evaluate the predictions on the test set.  For evaluating multiple metrics, either give a list of (unique) strings or a dict with names as keys and callables as values.  NOTE that when using custom scorers, each scorer should return a single value. Metric functions returning a list/array of values can be wrapped into multiple scorers that return one value each.  See :ref:`multimetric_grid_search` for an example.  If None, the estimator's score method is used.",param_type:["str","dict","list","tuple","callable",null],expected_shape:null,is_optional:!0,default_value:"None"},{name:"cv",docstring:"Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.20     ``cv`` default value if None will change from 3-fold to 5-fold     in v0.22.",param_type:["int","iter"],expected_shape:null,is_optional:!0,default_value:null},{name:"n_jobs",docstring:"The number of CPUs to use to do the computation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null},{name:"verbose",docstring:"The verbosity level.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:null},{name:"fit_params",docstring:"Parameters to pass to the fit method of the estimator.",param_type:["dict"],expected_shape:null,is_optional:!0,default_value:null},{name:"pre_dispatch",docstring:"Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:      - None, in which case all the jobs are immediately       created and spawned. Use this for lightweight and       fast-running jobs, to avoid delays due to on-demand       spawning of the jobs      - An int, giving the exact number of total jobs that are       spawned      - A string, giving an expression as a function of n_jobs,       as in '2*n_jobs'",param_type:["int","str"],expected_shape:null,is_optional:!0,default_value:null},{name:"return_train_score",docstring:"Whether to include train scores. Computing training scores is used to get insights on how different parameter settings impact the overfitting/underfitting trade-off. However computing the scores on the training set can be computationally expensive and is not strictly required to select the parameters that yield the best generalization performance.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"False"},{name:"return_estimator",docstring:"Whether to return the estimators fitted on each split.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"False"},{name:"error_score",docstring:"Value to assign to the score if an error occurs in estimator fitting. If set to 'raise', the error is raised. If set to 'raise-deprecating', a FutureWarning is printed before the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. Default is 'raise-deprecating' but from version 0.22 it will change to np.nan.",param_type:["LIST_VALID_OPTIONS"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[{name:"scores",docstring:"Array of scores of the estimator for each run of the cross validation.  A dict of arrays containing the score/time arrays for each scorer is returned. The possible keys for this ``dict`` are:      ``test_score``         The score array for test scores on each cv split.     ``train_score``         The score array for train scores on each cv split.         This is available only if ``return_train_score`` parameter         is ``True``.     ``fit_time``         The time for fitting the estimator on the train         set for each cv split.     ``score_time``         The time for scoring the estimator on the test set for each         cv split. (Note time for scoring on the train set is not         included even if ``return_train_score`` is set to ``True``     ``estimator``         The estimator objects for each cv split.         This is available only if ``return_estimator`` parameter         is set to ``True``.",param_type:["array","float","dict"],returned:!0}]},{name:"fit_grid_point",docstring:"Run fit on one set of parameters.\n\n    Parameters\n    ----------\n    X : array-like, sparse matrix or list\n        Input data.\n\n    y : array-like or None\n        Targets for input data.\n\n    estimator : estimator object\n        A object of that type is instantiated for each grid point.\n        This is assumed to implement the scikit-learn estimator interface.\n        Either estimator needs to provide a ``score`` function,\n        or ``scoring`` must be passed.\n\n    parameters : dict\n        Parameters to be set on estimator for this grid point.\n\n    train : ndarray, dtype int or bool\n        Boolean mask or indices for training set.\n\n    test : ndarray, dtype int or bool\n        Boolean mask or indices for test set.\n\n    scorer : callable or None\n        The scorer callable object / function must have its signature as\n        ``scorer(estimator, X, y)``.\n\n        If ``None`` the estimator's score method is used.\n\n    verbose : int\n        Verbosity level.\n\n    **fit_params : kwargs\n        Additional parameter passed to the fit function of the estimator.\n\n    error_score : 'raise' or numeric\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised. If a numeric value is given,\n        FitFailedWarning is raised. This parameter does not affect the refit\n        step, which will always raise the error. Default is 'raise' but from\n        version 0.22 it will change to np.nan.\n\n    Returns\n    -------\n    score : float\n         Score of this parameter setting on given test split.\n\n    parameters : dict\n        The parameters that have been evaluated.\n\n    n_samples_test : int\n        Number of test samples in this split.\n    ",inputs:[{name:"X",docstring:"Input data.",param_type:["array","list"],expected_shape:null,is_optional:!1,default_value:null},{name:"y",docstring:"Targets for input data.",param_type:["array",null],expected_shape:null,is_optional:!1,default_value:null},{name:"estimator",docstring:"A object of that type is instantiated for each grid point. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a ``score`` function, or ``scoring`` must be passed.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"parameters",docstring:"Parameters to be set on estimator for this grid point.",param_type:["dict"],expected_shape:null,is_optional:!1,default_value:null},{name:"train",docstring:"Boolean mask or indices for training set.",param_type:["array","int","bool"],expected_shape:null,is_optional:!1,default_value:null},{name:"test",docstring:"Boolean mask or indices for test set.",param_type:["array","int","bool"],expected_shape:null,is_optional:!1,default_value:null},{name:"scorer",docstring:"The scorer callable object / function must have its signature as ``scorer(estimator, X, y)``.  If ``None`` the estimator's score method is used.",param_type:["callable",null],expected_shape:null,is_optional:!1,default_value:null},{name:"verbose",docstring:"Verbosity level.",param_type:["int"],expected_shape:null,is_optional:!1,default_value:null},{name:"**fit_params",docstring:"Additional parameter passed to the fit function of the estimator.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null},{name:"error_score",docstring:"Value to assign to the score if an error occurs in estimator fitting. If set to 'raise', the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. Default is 'raise' but from version 0.22 it will change to np.nan.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null}],outputs:[{name:"score",docstring:"Score of this parameter setting on given test split.",param_type:["float"],returned:!0},{name:"parameters",docstring:"The parameters that have been evaluated.",param_type:["dict"],returned:!0},{name:"n_samples_test",docstring:"Number of test samples in this split.",param_type:["int"],returned:!0}]},{name:"learning_curve",docstring:"Learning curve.\n\n    Determines cross-validated training and test scores for different training\n    set sizes.\n\n    A cross-validation generator splits the whole dataset k times in training\n    and test data. Subsets of the training set with varying sizes will be used\n    to train the estimator and a score for each training subset size and the\n    test set will be computed. Afterwards, the scores will be averaged over\n    all k runs for each training subset size.\n\n    Read more in the :ref:`User Guide <learning_curve>`.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    groups : array-like, with shape (n_samples,), optional\n        Group labels for the samples used while splitting the dataset into\n        train/test set.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value if None will change from 3-fold to 5-fold\n            in v0.22.\n\n    scoring : string, callable or None, optional, default: None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    exploit_incremental_learning : boolean, optional, default: False\n        If the estimator supports incremental learning, this will be\n        used to speed up fitting for different training set sizes.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    pre_dispatch : integer or string, optional\n        Number of predispatched jobs for parallel execution (default is\n        all). The option can reduce the allocated memory. The string can\n        be an expression like '2*n_jobs'.\n\n    verbose : integer, optional\n        Controls the verbosity: the higher, the more messages.\n\n    shuffle : boolean, optional\n        Whether to shuffle training data before taking prefixes of it\n        based on``train_sizes``.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``shuffle`` is True.\n\n    error_score : 'raise' | 'raise-deprecating' or numeric\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised.\n        If set to 'raise-deprecating', a FutureWarning is printed before the\n        error is raised.\n        If a numeric value is given, FitFailedWarning is raised. This parameter\n        does not affect the refit step, which will always raise the error.\n        Default is 'raise-deprecating' but from version 0.22 it will change\n        to np.nan.\n\n    Returns\n    -------\n    train_sizes_abs : array, shape (n_unique_ticks,), dtype int\n        Numbers of training examples that has been used to generate the\n        learning curve. Note that the number of ticks might be less\n        than n_ticks because duplicate entries will be removed.\n\n    train_scores : array, shape (n_ticks, n_cv_folds)\n        Scores on training sets.\n\n    test_scores : array, shape (n_ticks, n_cv_folds)\n        Scores on test set.\n\n    Notes\n    -----\n    See :ref:`examples/model_selection/plot_learning_curve.py\n    <sphx_glr_auto_examples_model_selection_plot_learning_curve.py>`\n    ",inputs:[{name:"estimator",docstring:"An object of that type which is cloned for each validation.",param_type:["object","dict"],expected_shape:null,is_optional:!1,default_value:null},{name:"X",docstring:"Training vector, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"Target relative to X for classification or regression; None for unsupervised learning.",param_type:["array"],expected_shape:"(n_samples) or (n_samples, n_features), optional",is_optional:!0,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null},{name:"train_sizes",docstring:"Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set (that is determined by the selected validation method), i.e. it has to be within (0, 1]. Otherwise it is interpreted as absolute sizes of the training sets. Note that for classification the number of samples usually have to be big enough to contain at least one sample from each class. (default: np.linspace(0.1, 1.0, 5))",param_type:["array","int","float"],expected_shape:"(n_ticks,), dtype float or int",is_optional:!1,default_value:null},{name:"cv",docstring:"Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.20     ``cv`` default value if None will change from 3-fold to 5-fold     in v0.22.",param_type:["int","iter"],expected_shape:null,is_optional:!0,default_value:null},{name:"scoring",docstring:"A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``.",param_type:["str","callable",null],expected_shape:null,is_optional:!0,default_value:null},{name:"exploit_incremental_learning",docstring:"If the estimator supports incremental learning, this will be used to speed up fitting for different training set sizes.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:null},{name:"n_jobs",docstring:"Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null},{name:"pre_dispatch",docstring:"Number of predispatched jobs for parallel execution (default is all). The option can reduce the allocated memory. The string can be an expression like '2*n_jobs'.",param_type:["int","str"],expected_shape:null,is_optional:!0,default_value:null},{name:"verbose",docstring:"Controls the verbosity: the higher, the more messages.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:null},{name:"shuffle",docstring:"Whether to shuffle training data before taking prefixes of it based on``train_sizes``.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:null},{name:"random_state",docstring:"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. Used when ``shuffle`` is True.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null},{name:"error_score",docstring:"Value to assign to the score if an error occurs in estimator fitting. If set to 'raise', the error is raised. If set to 'raise-deprecating', a FutureWarning is printed before the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. Default is 'raise-deprecating' but from version 0.22 it will change to np.nan.",param_type:["LIST_VALID_OPTIONS"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[{name:"train_sizes_abs",docstring:"Numbers of training examples that has been used to generate the learning curve. Note that the number of ticks might be less than n_ticks because duplicate entries will be removed.",param_type:["array","int"],returned:!0},{name:"train_scores",docstring:"Scores on training sets.",param_type:["array"],returned:!0},{name:"test_scores",docstring:"Scores on test set.",param_type:["array"],returned:!0}]},{name:"permutation_test_score",docstring:"Evaluate the significance of a cross-validated score with permutations\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    estimator : estimator object implementing 'fit'\n        The object to use to fit the data.\n\n    X : array-like of shape at least 2D\n        The data to fit.\n\n    y : array-like\n        The target variable to try to predict in the case of\n        supervised learning.\n\n    groups : array-like, with shape (n_samples,), optional\n        Labels to constrain permutation within groups, i.e. ``y`` values\n        are permuted among samples with the same group identifier.\n        When not specified, ``y`` values are permuted among all samples.\n\n        When a grouped cross-validator is used, the group labels are\n        also passed on to the ``split`` method of the cross-validator. The\n        cross-validator uses them for grouping the samples  while splitting\n        the dataset into train/test set.\n\n    scoring : string, callable or None, optional, default: None\n        A single string (see :ref:`scoring_parameter`) or a callable\n        (see :ref:`scoring`) to evaluate the predictions on the test set.\n\n        If None the estimator's score method is used.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value if None will change from 3-fold to 5-fold\n            in v0.22.\n\n    n_permutations : integer, optional\n        Number of times to permute ``y``.\n\n    n_jobs : int or None, optional (default=None)\n        The number of CPUs to use to do the computation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance or None, optional (default=0)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    verbose : integer, optional\n        The verbosity level.\n\n    Returns\n    -------\n    score : float\n        The true score without permuting targets.\n\n    permutation_scores : array, shape (n_permutations,)\n        The scores obtained for each permutations.\n\n    pvalue : float\n        The p-value, which approximates the probability that the score would\n        be obtained by chance. This is calculated as:\n\n        `(C + 1) / (n_permutations + 1)`\n\n        Where C is the number of permutations whose score >= the true score.\n\n        The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.\n\n    Notes\n    -----\n    This function implements Test 1 in:\n\n        Ojala and Garriga. Permutation Tests for Studying Classifier\n        Performance.  The Journal of Machine Learning Research (2010)\n        vol. 11\n\n    ",inputs:[{name:"estimator",docstring:"The object to use to fit the data.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"X",docstring:"The data to fit.",param_type:["array"],expected_shape:"at least 2D",is_optional:!1,default_value:null},{name:"y",docstring:"The target variable to try to predict in the case of supervised learning.",param_type:["array"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Labels to constrain permutation within groups, i.e. ``y`` values are permuted among samples with the same group identifier. When not specified, ``y`` values are permuted among all samples.  When a grouped cross-validator is used, the group labels are also passed on to the ``split`` method of the cross-validator. The cross-validator uses them for grouping the samples  while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null},{name:"scoring",docstring:"A single string (see :ref:`scoring_parameter`) or a callable (see :ref:`scoring`) to evaluate the predictions on the test set.  If None the estimator's score method is used.",param_type:["str","callable",null],expected_shape:null,is_optional:!0,default_value:null},{name:"cv",docstring:"Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.20     ``cv`` default value if None will change from 3-fold to 5-fold     in v0.22.",param_type:["int","iter"],expected_shape:null,is_optional:!0,default_value:null},{name:"n_permutations",docstring:"Number of times to permute ``y``.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:null},{name:"n_jobs",docstring:"The number of CPUs to use to do the computation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null},{name:"random_state",docstring:"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null},{name:"verbose",docstring:"The verbosity level.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:null}],outputs:[{name:"score",docstring:"The true score without permuting targets.",param_type:["float"],returned:!0},{name:"permutation_scores",docstring:"The scores obtained for each permutations.",param_type:["array"],returned:!0},{name:"pvalue",docstring:"The p-value, which approximates the probability that the score would be obtained by chance. This is calculated as:  `(C + 1) / (n_permutations + 1)`  Where C is the number of permutations whose score >= the true score.  The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.",param_type:["float"],returned:!0}]},{name:"train_test_split",docstring:"Split arrays or matrices into random train and test subsets\n\n    Quick utility that wraps input validation and\n    ``next(ShuffleSplit().split(X, y))`` and application to input data\n    into a single call for splitting (and optionally subsampling) data in a\n    oneliner.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    *arrays : sequence of indexables with same length / shape[0]\n        Allowed inputs are lists, numpy arrays, scipy-sparse\n        matrices or pandas dataframes.\n\n    test_size : float, int or None, optional (default=None)\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to include in the test split. If int, represents the\n        absolute number of test samples. If None, the value is set to the\n        complement of the train size. If ``train_size`` is also None, it will\n        be set to 0.25.\n\n    train_size : float, int, or None, (default=None)\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the dataset to include in the train split. If\n        int, represents the absolute number of train samples. If None,\n        the value is automatically set to the complement of the test size.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    shuffle : boolean, optional (default=True)\n        Whether or not to shuffle the data before splitting. If shuffle=False\n        then stratify must be None.\n\n    stratify : array-like or None (default=None)\n        If not None, data is split in a stratified fashion, using this as\n        the class labels.\n\n    Returns\n    -------\n    splitting : list, length=2 * len(arrays)\n        List containing train-test split of inputs.\n\n        .. versionadded:: 0.16\n            If the input is sparse, the output will be a\n            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n            input type.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import train_test_split\n    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n    >>> X\n    array([[0, 1],\n           [2, 3],\n           [4, 5],\n           [6, 7],\n           [8, 9]])\n    >>> list(y)\n    [0, 1, 2, 3, 4]\n\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, test_size=0.33, random_state=42)\n    ...\n    >>> X_train\n    array([[4, 5],\n           [0, 1],\n           [6, 7]])\n    >>> y_train\n    [2, 0, 3]\n    >>> X_test\n    array([[2, 3],\n           [8, 9]])\n    >>> y_test\n    [1, 4]\n\n    >>> train_test_split(y, shuffle=False)\n    [[0, 1, 2], [3, 4]]\n\n    ",inputs:[{name:"*arrays",docstring:"Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null},{name:"test_size",docstring:"If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If ``train_size`` is also None, it will be set to 0.25.",param_type:["int","float",null],expected_shape:null,is_optional:!0,default_value:null},{name:"train_size",docstring:"If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.",param_type:["int","float",null],expected_shape:null,is_optional:!0,default_value:"None"},{name:"random_state",docstring:"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null},{name:"shuffle",docstring:"Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:null},{name:"stratify",docstring:"If not None, data is split in a stratified fashion, using this as the class labels.",param_type:["array",null],expected_shape:null,is_optional:!0,default_value:"None"}],outputs:[{name:"splitting",docstring:"List containing train-test split of inputs.  .. versionadded:: 0.16     If the input is sparse, the output will be a     ``scipy.sparse.csr_matrix``. Else, output type is the same as the     input type.",param_type:["array","list"],returned:!0}]},{name:"validation_curve",docstring:"Validation curve.\n\n    Determine training and test scores for varying parameter values.\n\n    Compute scores for an estimator with different values of a specified\n    parameter. This is similar to grid search with one parameter. However, this\n    will also compute training scores and is merely a utility for plotting the\n    results.\n\n    Read more in the :ref:`User Guide <learning_curve>`.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    param_name : string\n        Name of the parameter that will be varied.\n\n    param_range : array-like, shape (n_values,)\n        The values of the parameter that will be evaluated.\n\n    groups : array-like, with shape (n_samples,), optional\n        Group labels for the samples used while splitting the dataset into\n        train/test set.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value if None will change from 3-fold to 5-fold\n            in v0.22.\n\n    scoring : string, callable or None, optional, default: None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    pre_dispatch : integer or string, optional\n        Number of predispatched jobs for parallel execution (default is\n        all). The option can reduce the allocated memory. The string can\n        be an expression like '2*n_jobs'.\n\n    verbose : integer, optional\n        Controls the verbosity: the higher, the more messages.\n\n    error_score : 'raise' | 'raise-deprecating' or numeric\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised.\n        If set to 'raise-deprecating', a FutureWarning is printed before the\n        error is raised.\n        If a numeric value is given, FitFailedWarning is raised. This parameter\n        does not affect the refit step, which will always raise the error.\n        Default is 'raise-deprecating' but from version 0.22 it will change\n        to np.nan.\n\n    Returns\n    -------\n    train_scores : array, shape (n_ticks, n_cv_folds)\n        Scores on training sets.\n\n    test_scores : array, shape (n_ticks, n_cv_folds)\n        Scores on test set.\n\n    Notes\n    -----\n    See :ref:`sphx_glr_auto_examples_model_selection_plot_validation_curve.py`\n\n    ",inputs:[{name:"estimator",docstring:"An object of that type which is cloned for each validation.",param_type:["object","dict"],expected_shape:null,is_optional:!1,default_value:null},{name:"X",docstring:"Training vector, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"Target relative to X for classification or regression; None for unsupervised learning.",param_type:["array"],expected_shape:"(n_samples) or (n_samples, n_features), optional",is_optional:!0,default_value:null},{name:"param_name",docstring:"Name of the parameter that will be varied.",param_type:["str"],expected_shape:null,is_optional:!1,default_value:null},{name:"param_range",docstring:"The values of the parameter that will be evaluated.",param_type:["array"],expected_shape:"(n_values,)",is_optional:!1,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null},{name:"cv",docstring:"Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.20     ``cv`` default value if None will change from 3-fold to 5-fold     in v0.22.",param_type:["int","iter"],expected_shape:null,is_optional:!0,default_value:null},{name:"scoring",docstring:"A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``.",param_type:["str","callable",null],expected_shape:null,is_optional:!0,default_value:null},{name:"n_jobs",docstring:"Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null},{name:"pre_dispatch",docstring:"Number of predispatched jobs for parallel execution (default is all). The option can reduce the allocated memory. The string can be an expression like '2*n_jobs'.",param_type:["int","str"],expected_shape:null,is_optional:!0,default_value:null},{name:"verbose",docstring:"Controls the verbosity: the higher, the more messages.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:null},{name:"error_score",docstring:"Value to assign to the score if an error occurs in estimator fitting. If set to 'raise', the error is raised. If set to 'raise-deprecating', a FutureWarning is printed before the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. Default is 'raise-deprecating' but from version 0.22 it will change to np.nan.",param_type:["LIST_VALID_OPTIONS"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[{name:"train_scores",docstring:"Scores on training sets.",param_type:["array"],returned:!0},{name:"test_scores",docstring:"Scores on test set.",param_type:["array"],returned:!0}]}],nodes:[{name:"BaseCrossValidator",docstring:"Base class for all cross-validators\n\n    Implementations must define `_iter_test_masks` or `_iter_test_indices`.\n    ",inputs:[],outputs:[],node_functions:[{name:"__init__",docstring:"Initialize self.  See help(type(self)) for accurate signature.",inputs:[],outputs:[]},{name:"_iter_test_indices",docstring:"Generates integer indices corresponding to test sets.",inputs:[],outputs:[]},{name:"_iter_test_masks",docstring:"Generates boolean masks corresponding to test sets.\n\n        By default, delegates to _iter_test_indices(X, y, groups)\n        ",inputs:[],outputs:[]},{name:"get_n_splits",docstring:"Returns the number of splitting iterations in the cross-validator",inputs:[],outputs:[]},{name:"split",docstring:"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, of length n_samples\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        ",inputs:[{name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"The target variable for supervised learning problems.",param_type:["array"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null}],outputs:[{name:"train",docstring:"The training set indices for that split.",param_type:["array"],returned:!1},{name:"test",docstring:"The testing set indices for that split.",param_type:["array"],returned:!1}]}],nodes:[]},{name:"GridSearchCV",docstring:"Exhaustive search over specified parameter values for an estimator.\n\n    Important members are fit, predict.\n\n    GridSearchCV implements a \"fit\" and a \"score\" method.\n    It also implements \"predict\", \"predict_proba\", \"decision_function\",\n    \"transform\" and \"inverse_transform\" if they are implemented in the\n    estimator used.\n\n    The parameters of the estimator used to apply these methods are optimized\n    by cross-validated grid-search over a parameter grid.\n\n    Read more in the :ref:`User Guide <grid_search>`.\n\n    Parameters\n    ----------\n    estimator : estimator object.\n        This is assumed to implement the scikit-learn estimator interface.\n        Either estimator needs to provide a ``score`` function,\n        or ``scoring`` must be passed.\n\n    param_grid : dict or list of dictionaries\n        Dictionary with parameters names (string) as keys and lists of\n        parameter settings to try as values, or a list of such\n        dictionaries, in which case the grids spanned by each dictionary\n        in the list are explored. This enables searching over any sequence\n        of parameter settings.\n\n    scoring : string, callable, list/tuple, dict or None, default: None\n        A single string (see :ref:`scoring_parameter`) or a callable\n        (see :ref:`scoring`) to evaluate the predictions on the test set.\n\n        For evaluating multiple metrics, either give a list of (unique) strings\n        or a dict with names as keys and callables as values.\n\n        NOTE that when using custom scorers, each scorer should return a single\n        value. Metric functions returning a list/array of values can be wrapped\n        into multiple scorers that return one value each.\n\n        See :ref:`multimetric_grid_search` for an example.\n\n        If None, the estimator's score method is used.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    pre_dispatch : int, or string, optional\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n            - None, in which case all the jobs are immediately\n              created and spawned. Use this for lightweight and\n              fast-running jobs, to avoid delays due to on-demand\n              spawning of the jobs\n\n            - An int, giving the exact number of total jobs that are\n              spawned\n\n            - A string, giving an expression as a function of n_jobs,\n              as in '2*n_jobs'\n\n    iid : boolean, default='warn'\n        If True, return the average score across folds, weighted by the number\n        of samples in each test set. In this case, the data is assumed to be\n        identically distributed across the folds, and the loss minimized is\n        the total loss per sample, and not the mean loss across the folds. If\n        False, return the average score across folds. Default is True, but\n        will change to False in version 0.22, to correspond to the standard\n        definition of cross-validation.\n\n        .. versionchanged:: 0.20\n            Parameter ``iid`` will change from True to False by default in\n            version 0.22, and will be removed in 0.24.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value if None will change from 3-fold to 5-fold\n            in v0.22.\n\n    refit : boolean, string, or callable, default=True\n        Refit an estimator using the best found parameters on the whole\n        dataset.\n\n        For multiple metric evaluation, this needs to be a string denoting the\n        scorer that would be used to find the best parameters for refitting\n        the estimator at the end.\n\n        Where there are considerations other than maximum score in\n        choosing a best estimator, ``refit`` can be set to a function which\n        returns the selected ``best_index_`` given ``cv_results_``.\n\n        The refitted estimator is made available at the ``best_estimator_``\n        attribute and permits using ``predict`` directly on this\n        ``GridSearchCV`` instance.\n\n        Also for multiple metric evaluation, the attributes ``best_index_``,\n        ``best_score_`` and ``best_params_`` will only be available if\n        ``refit`` is set and all of them will be determined w.r.t this specific\n        scorer. ``best_score_`` is not returned if refit is callable.\n\n        See ``scoring`` parameter to know more about multiple metric\n        evaluation.\n\n        .. versionchanged:: 0.20\n            Support for callable added.\n\n    verbose : integer\n        Controls the verbosity: the higher, the more messages.\n\n    error_score : 'raise' or numeric\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised. If a numeric value is given,\n        FitFailedWarning is raised. This parameter does not affect the refit\n        step, which will always raise the error. Default is 'raise' but from\n        version 0.22 it will change to np.nan.\n\n    return_train_score : boolean, default=False\n        If ``False``, the ``cv_results_`` attribute will not include training\n        scores.\n        Computing training scores is used to get insights on how different\n        parameter settings impact the overfitting/underfitting trade-off.\n        However computing the scores on the training set can be computationally\n        expensive and is not strictly required to select the parameters that\n        yield the best generalization performance.\n\n\n    Examples\n    --------\n    >>> from sklearn import svm, datasets\n    >>> from sklearn.model_selection import GridSearchCV\n    >>> iris = datasets.load_iris()\n    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n    >>> svc = svm.SVC(gamma=\"scale\")\n    >>> clf = GridSearchCV(svc, parameters, cv=5)\n    >>> clf.fit(iris.data, iris.target)\n    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n    GridSearchCV(cv=5, error_score=...,\n           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,\n                         decision_function_shape='ovr', degree=..., gamma=...,\n                         kernel='rbf', max_iter=-1, probability=False,\n                         random_state=None, shrinking=True, tol=...,\n                         verbose=False),\n           iid=..., n_jobs=None,\n           param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,\n           scoring=..., verbose=...)\n    >>> sorted(clf.cv_results_.keys())\n    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n    ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n     'param_C', 'param_kernel', 'params',...\n     'rank_test_score', 'split0_test_score',...\n     'split2_test_score', ...\n     'std_fit_time', 'std_score_time', 'std_test_score']\n\n    Attributes\n    ----------\n    cv_results_ : dict of numpy (masked) ndarrays\n        A dict with keys as column headers and values as columns, that can be\n        imported into a pandas ``DataFrame``.\n\n        For instance the below given table\n\n        +------------+-----------+------------+-----------------+---+---------+\n        |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n        +============+===========+============+=================+===+=========+\n        |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n        +------------+-----------+------------+-----------------+---+---------+\n\n        will be represented by a ``cv_results_`` dict of::\n\n            {\n            'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n                                         mask = [False False False False]...)\n            'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n                                        mask = [ True  True False False]...),\n            'param_degree': masked_array(data = [2.0 3.0 -- --],\n                                         mask = [False False  True  True]...),\n            'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n            'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n            'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n            'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n            'rank_test_score'    : [2, 4, 3, 1],\n            'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n            'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n            'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n            'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n            'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n            'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n            'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n            'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n            'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n            }\n\n        NOTE\n\n        The key ``'params'`` is used to store a list of parameter\n        settings dicts for all the parameter candidates.\n\n        The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n        ``std_score_time`` are all in seconds.\n\n        For multi-metric evaluation, the scores for all the scorers are\n        available in the ``cv_results_`` dict at the keys ending with that\n        scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n        above. ('split0_test_precision', 'mean_train_precision' etc.)\n\n    best_estimator_ : estimator or dict\n        Estimator that was chosen by the search, i.e. estimator\n        which gave highest score (or smallest loss if specified)\n        on the left out data. Not available if ``refit=False``.\n\n        See ``refit`` parameter for more information on allowed values.\n\n    best_score_ : float\n        Mean cross-validated score of the best_estimator\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n    best_params_ : dict\n        Parameter setting that gave the best results on the hold out data.\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n    best_index_ : int\n        The index (of the ``cv_results_`` arrays) which corresponds to the best\n        candidate parameter setting.\n\n        The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n        the parameter setting for the best model, that gives the highest\n        mean score (``search.best_score_``).\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n    scorer_ : function or a dict\n        Scorer function used on the held out data to choose the best\n        parameters for the model.\n\n        For multi-metric evaluation, this attribute holds the validated\n        ``scoring`` dict which maps the scorer key to the scorer callable.\n\n    n_splits_ : int\n        The number of cross-validation splits (folds/iterations).\n\n    refit_time_ : float\n        Seconds used for refitting the best model on the whole dataset.\n\n        This is present only if ``refit`` is not False.\n\n    Notes\n    -----\n    The parameters selected are those that maximize the score of the left out\n    data, unless an explicit score is passed in which case it is used instead.\n\n    If `n_jobs` was set to a value higher than one, the data is copied for each\n    point in the grid (and not `n_jobs` times). This is done for efficiency\n    reasons if individual jobs take very little time, but may raise errors if\n    the dataset is large and not enough memory is available.  A workaround in\n    this case is to set `pre_dispatch`. Then, the memory is copied only\n    `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n    n_jobs`.\n\n    See Also\n    ---------\n    :class:`ParameterGrid`:\n        generates all the combinations of a hyperparameter grid.\n\n    :func:`sklearn.model_selection.train_test_split`:\n        utility function to split the data into a development set usable\n        for fitting a GridSearchCV instance and an evaluation set for\n        its final evaluation.\n\n    :func:`sklearn.metrics.make_scorer`:\n        Make a scorer from a performance metric or loss function.\n\n    ",inputs:[{name:"estimator",docstring:"This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a ``score`` function, or ``scoring`` must be passed.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"param_grid",docstring:"Dictionary with parameters names (string) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings.",param_type:["dict","list"],expected_shape:null,is_optional:!1,default_value:null},{name:"scoring",docstring:"A single string (see :ref:`scoring_parameter`) or a callable (see :ref:`scoring`) to evaluate the predictions on the test set.  For evaluating multiple metrics, either give a list of (unique) strings or a dict with names as keys and callables as values.  NOTE that when using custom scorers, each scorer should return a single value. Metric functions returning a list/array of values can be wrapped into multiple scorers that return one value each.  See :ref:`multimetric_grid_search` for an example.  If None, the estimator's score method is used.",param_type:["str","dict","list","tuple","callable",null],expected_shape:null,is_optional:!0,default_value:"None"},{name:"n_jobs",docstring:"Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null},{name:"pre_dispatch",docstring:"Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:      - None, in which case all the jobs are immediately       created and spawned. Use this for lightweight and       fast-running jobs, to avoid delays due to on-demand       spawning of the jobs      - An int, giving the exact number of total jobs that are       spawned      - A string, giving an expression as a function of n_jobs,       as in '2*n_jobs'",param_type:["int","str"],expected_shape:null,is_optional:!0,default_value:null},{name:"iid",docstring:"If True, return the average score across folds, weighted by the number of samples in each test set. In this case, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds. If False, return the average score across folds. Default is True, but will change to False in version 0.22, to correspond to the standard definition of cross-validation.  .. versionchanged:: 0.20     Parameter ``iid`` will change from True to False by default in     version 0.22, and will be removed in 0.24.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"warn"},{name:"cv",docstring:"Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.20     ``cv`` default value if None will change from 3-fold to 5-fold     in v0.22.",param_type:["int","iter"],expected_shape:null,is_optional:!0,default_value:null},{name:"refit",docstring:"Refit an estimator using the best found parameters on the whole dataset.  For multiple metric evaluation, this needs to be a string denoting the scorer that would be used to find the best parameters for refitting the estimator at the end.  Where there are considerations other than maximum score in choosing a best estimator, ``refit`` can be set to a function which returns the selected ``best_index_`` given ``cv_results_``.  The refitted estimator is made available at the ``best_estimator_`` attribute and permits using ``predict`` directly on this ``GridSearchCV`` instance.  Also for multiple metric evaluation, the attributes ``best_index_``, ``best_score_`` and ``best_params_`` will only be available if ``refit`` is set and all of them will be determined w.r.t this specific scorer. ``best_score_`` is not returned if refit is callable.  See ``scoring`` parameter to know more about multiple metric evaluation.  .. versionchanged:: 0.20     Support for callable added.",param_type:["bool","str","callable"],expected_shape:null,is_optional:!0,default_value:"True"},{name:"verbose",docstring:"Controls the verbosity: the higher, the more messages.",param_type:["int"],expected_shape:null,is_optional:!1,default_value:null},{name:"error_score",docstring:"Value to assign to the score if an error occurs in estimator fitting. If set to 'raise', the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. Default is 'raise' but from version 0.22 it will change to np.nan.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null},{name:"return_train_score",docstring:"If ``False``, the ``cv_results_`` attribute will not include training scores. Computing training scores is used to get insights on how different parameter settings impact the overfitting/underfitting trade-off. However computing the scores on the training set can be computationally expensive and is not strictly required to select the parameters that yield the best generalization performance.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"False"}],outputs:[{name:"cv_results_",docstring:"A dict with keys as column headers and values as columns, that can be imported into a pandas ``DataFrame``.  For instance the below given table  +------------+-----------+------------+-----------------+---+---------+ |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...| +============+===========+============+=================+===+=========+ |  'poly'    |     --    |      2     |       0.80      |...|    2    | +------------+-----------+------------+-----------------+---+---------+ |  'poly'    |     --    |      3     |       0.70      |...|    4    | +------------+-----------+------------+-----------------+---+---------+ |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    | +------------+-----------+------------+-----------------+---+---------+ |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    | +------------+-----------+------------+-----------------+---+---------+  will be represented by a ``cv_results_`` dict of::      {     'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],                                  mask = [False False False False]...)     'param_gamma': masked_array(data = [-- -- 0.1 0.2],                                 mask = [ True  True False False]...),     'param_degree': masked_array(data = [2.0 3.0 -- --],                                  mask = [False False  True  True]...),     'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],     'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],     'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],     'std_test_score'     : [0.01, 0.10, 0.05, 0.08],     'rank_test_score'    : [2, 4, 3, 1],     'split0_train_score' : [0.80, 0.92, 0.70, 0.93],     'split1_train_score' : [0.82, 0.55, 0.70, 0.87],     'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],     'std_train_score'    : [0.01, 0.19, 0.00, 0.03],     'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],     'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],     'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],     'std_score_time'     : [0.00, 0.00, 0.00, 0.01],     'params'             : [{'kernel': 'poly', 'degree': 2}, ...],     }  NOTE  The key ``'params'`` is used to store a list of parameter settings dicts for all the parameter candidates.  The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and ``std_score_time`` are all in seconds.  For multi-metric evaluation, the scores for all the scorers are available in the ``cv_results_`` dict at the keys ending with that scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown above. ('split0_test_precision', 'mean_train_precision' etc.)",param_type:["array","dict"],returned:!1},{name:"best_estimator_",docstring:"Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if ``refit=False``.  See ``refit`` parameter for more information on allowed values.",param_type:["dict"],returned:!1},{name:"best_score_",docstring:"Mean cross-validated score of the best_estimator  For multi-metric evaluation, this is present only if ``refit`` is specified.",param_type:["float"],returned:!1},{name:"best_params_",docstring:"Parameter setting that gave the best results on the hold out data.  For multi-metric evaluation, this is present only if ``refit`` is specified.",param_type:["dict"],returned:!1},{name:"best_index_",docstring:"The index (of the ``cv_results_`` arrays) which corresponds to the best candidate parameter setting.  The dict at ``search.cv_results_['params'][search.best_index_]`` gives the parameter setting for the best model, that gives the highest mean score (``search.best_score_``).  For multi-metric evaluation, this is present only if ``refit`` is specified.",param_type:["int"],returned:!1},{name:"scorer_",docstring:"Scorer function used on the held out data to choose the best parameters for the model.  For multi-metric evaluation, this attribute holds the validated ``scoring`` dict which maps the scorer key to the scorer callable.",param_type:["dict"],returned:!1},{name:"n_splits_",docstring:"The number of cross-validation splits (folds/iterations).",param_type:["int"],returned:!1},{name:"refit_time_",docstring:"Seconds used for refitting the best model on the whole dataset.  This is present only if ``refit`` is not False.",param_type:["float"],returned:!1}],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_check_is_fitted",docstring:"None",inputs:[],outputs:[]},{name:"_format_results",docstring:"None",inputs:[],outputs:[]},{name:"_get_param_names",docstring:"Get parameter names for the estimator",inputs:[],outputs:[]},{name:"_get_tags",docstring:"None",inputs:[],outputs:[]},{name:"_run_search",docstring:"Search all candidates in param_grid",inputs:[],outputs:[]},{name:"decision_function",docstring:"Call decision_function on the estimator with the best found parameters.\n\n        Only available if ``refit=True`` and the underlying estimator supports\n        ``decision_function``.\n\n        Parameters\n        ----------\n        X : indexable, length n_samples\n            Must fulfill the input assumptions of the\n            underlying estimator.\n\n        ",inputs:[{name:"X",docstring:"Must fulfill the input assumptions of the underlying estimator.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null}],outputs:[]},{name:"fit",docstring:"Run fit with all sets of parameters.\n\n        Parameters\n        ----------\n\n        X : array-like, shape = [n_samples, n_features]\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n            Target relative to X for classification or regression;\n            None for unsupervised learning.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        **fit_params : dict of string -> object\n            Parameters passed to the ``fit`` method of the estimator\n        ",inputs:[{name:"X",docstring:"Training vector, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null},{name:"y",docstring:"Target relative to X for classification or regression; None for unsupervised learning.",param_type:["array"],expected_shape:"[n_samples] or [n_samples, n_output], optional",is_optional:!0,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null},{name:"**fit_params",docstring:"Parameters passed to the ``fit`` method of the estimator",param_type:["object","str","dict"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[]},{name:"get_params",docstring:"Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : boolean, optional\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : mapping of string to any\n            Parameter names mapped to their values.\n        ",inputs:[{name:"deep",docstring:"If True, will return the parameters for this estimator and contained subobjects that are estimators.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:null}],outputs:[{name:"params",docstring:"Parameter names mapped to their values.",param_type:["str"],returned:!0}]},{name:"inverse_transform",docstring:"Call inverse_transform on the estimator with the best found params.\n\n        Only available if the underlying estimator implements\n        ``inverse_transform`` and ``refit=True``.\n\n        Parameters\n        ----------\n        Xt : indexable, length n_samples\n            Must fulfill the input assumptions of the\n            underlying estimator.\n\n        ",inputs:[{name:"Xt",docstring:"Must fulfill the input assumptions of the underlying estimator.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null}],outputs:[]},{name:"predict",docstring:"Call predict on the estimator with the best found parameters.\n\n        Only available if ``refit=True`` and the underlying estimator supports\n        ``predict``.\n\n        Parameters\n        ----------\n        X : indexable, length n_samples\n            Must fulfill the input assumptions of the\n            underlying estimator.\n\n        ",inputs:[{name:"X",docstring:"Must fulfill the input assumptions of the underlying estimator.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null}],outputs:[]},{name:"predict_log_proba",docstring:"Call predict_log_proba on the estimator with the best found parameters.\n\n        Only available if ``refit=True`` and the underlying estimator supports\n        ``predict_log_proba``.\n\n        Parameters\n        ----------\n        X : indexable, length n_samples\n            Must fulfill the input assumptions of the\n            underlying estimator.\n\n        ",inputs:[{name:"X",docstring:"Must fulfill the input assumptions of the underlying estimator.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null}],outputs:[]},{name:"predict_proba",docstring:"Call predict_proba on the estimator with the best found parameters.\n\n        Only available if ``refit=True`` and the underlying estimator supports\n        ``predict_proba``.\n\n        Parameters\n        ----------\n        X : indexable, length n_samples\n            Must fulfill the input assumptions of the\n            underlying estimator.\n\n        ",inputs:[{name:"X",docstring:"Must fulfill the input assumptions of the underlying estimator.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null}],outputs:[]},{name:"score",docstring:"Returns the score on the given data, if the estimator has been refit.\n\n        This uses the score defined by ``scoring`` where provided, and the\n        ``best_estimator_.score`` method otherwise.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Input data, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n            Target relative to X for classification or regression;\n            None for unsupervised learning.\n\n        Returns\n        -------\n        score : float\n        ",inputs:[{name:"X",docstring:"Input data, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null},{name:"y",docstring:"Target relative to X for classification or regression; None for unsupervised learning.",param_type:["array"],expected_shape:"[n_samples] or [n_samples, n_output], optional",is_optional:!0,default_value:null}],outputs:[{name:"score",docstring:"",param_type:["float"],returned:!0}]},{name:"set_params",docstring:"Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as pipelines). The latter have parameters of the form\n        ``<component>__<parameter>`` so that it's possible to update each\n        component of a nested object.\n\n        Returns\n        -------\n        self\n        ",inputs:[],outputs:[{name:"",docstring:"",param_type:[null],returned:!0}]},{name:"transform",docstring:"Call transform on the estimator with the best found parameters.\n\n        Only available if the underlying estimator supports ``transform`` and\n        ``refit=True``.\n\n        Parameters\n        ----------\n        X : indexable, length n_samples\n            Must fulfill the input assumptions of the\n            underlying estimator.\n\n        ",inputs:[{name:"X",docstring:"Must fulfill the input assumptions of the underlying estimator.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null}],outputs:[]}],nodes:[]},{name:"GroupKFold",docstring:'K-fold iterator variant with non-overlapping groups.\n\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n\n    The folds are approximately balanced in the sense that the number of\n    distinct groups is approximately the same in each fold.\n\n    Parameters\n    ----------\n    n_splits : int, default=3\n        Number of folds. Must be at least 2.\n\n        .. versionchanged:: 0.20\n            ``n_splits`` default value will change from 3 to 5 in v0.22.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import GroupKFold\n    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    >>> y = np.array([1, 2, 3, 4])\n    >>> groups = np.array([0, 0, 2, 2])\n    >>> group_kfold = GroupKFold(n_splits=2)\n    >>> group_kfold.get_n_splits(X, y, groups)\n    2\n    >>> print(group_kfold)\n    GroupKFold(n_splits=2)\n    >>> for train_index, test_index in group_kfold.split(X, y, groups):\n    ...     print("TRAIN:", train_index, "TEST:", test_index)\n    ...     X_train, X_test = X[train_index], X[test_index]\n    ...     y_train, y_test = y[train_index], y[test_index]\n    ...     print(X_train, X_test, y_train, y_test)\n    ...\n    TRAIN: [0 1] TEST: [2 3]\n    [[1 2]\n     [3 4]] [[5 6]\n     [7 8]] [1 2] [3 4]\n    TRAIN: [2 3] TEST: [0 1]\n    [[5 6]\n     [7 8]] [[1 2]\n     [3 4]] [3 4] [1 2]\n\n    See also\n    --------\n    LeaveOneGroupOut\n        For splitting the data according to explicit domain-specific\n        stratification of the dataset.\n    ',inputs:[{name:"n_splits",docstring:"Number of folds. Must be at least 2.  .. versionchanged:: 0.20     ``n_splits`` default value will change from 3 to 5 in v0.22.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"3"}],outputs:[],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_iter_test_indices",docstring:"None",inputs:[],outputs:[]},{name:"_iter_test_masks",docstring:"Generates boolean masks corresponding to test sets.\n\n        By default, delegates to _iter_test_indices(X, y, groups)\n        ",inputs:[],outputs:[]},{name:"get_n_splits",docstring:"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        ",inputs:[{name:"X",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"y",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[{name:"n_splits",docstring:"Returns the number of splitting iterations in the cross-validator.",param_type:["int"],returned:!0}]},{name:"split",docstring:"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, shape (n_samples,), optional\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        ",inputs:[{name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"The target variable for supervised learning problems.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null}],outputs:[{name:"train",docstring:"The training set indices for that split.",param_type:["array"],returned:!1},{name:"test",docstring:"The testing set indices for that split.",param_type:["array"],returned:!1}]}],nodes:[]},{name:"GroupShuffleSplit",docstring:"Shuffle-Group(s)-Out cross-validation iterator\n\n    Provides randomized train/test indices to split data according to a\n    third-party provided group. This group information can be used to encode\n    arbitrary domain specific stratifications of the samples as integers.\n\n    For instance the groups could be the year of collection of the samples\n    and thus allow for cross-validation against time-based splits.\n\n    The difference between LeavePGroupsOut and GroupShuffleSplit is that\n    the former generates splits using all subsets of size ``p`` unique groups,\n    whereas GroupShuffleSplit generates a user-determined number of random\n    test splits, each with a user-determined fraction of unique groups.\n\n    For example, a less computationally intensive alternative to\n    ``LeavePGroupsOut(p=10)`` would be\n    ``GroupShuffleSplit(test_size=10, n_splits=100)``.\n\n    Note: The parameters ``test_size`` and ``train_size`` refer to groups, and\n    not to samples, as in ShuffleSplit.\n\n\n    Parameters\n    ----------\n    n_splits : int (default 5)\n        Number of re-shuffling & splitting iterations.\n\n    test_size : float, int, None, optional (default=None)\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to include in the test split. If int, represents the\n        absolute number of test groups. If None, the value is set to the\n        complement of the train size. If ``train_size`` is also None, it will\n        be set to 0.2.\n\n    train_size : float, int, or None, default is None\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the groups to include in the train split. If\n        int, represents the absolute number of train groups. If None,\n        the value is automatically set to the complement of the test size.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    ",inputs:[{name:"n_splits",docstring:"Number of re-shuffling & splitting iterations.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"5"},{name:"test_size",docstring:"If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test groups. If None, the value is set to the complement of the train size. If ``train_size`` is also None, it will be set to 0.2.",param_type:["int","float",null],expected_shape:null,is_optional:!0,default_value:null},{name:"train_size",docstring:"If float, should be between 0.0 and 1.0 and represent the proportion of the groups to include in the train split. If int, represents the absolute number of train groups. If None, the value is automatically set to the complement of the test size.",param_type:["int","float",null],expected_shape:null,is_optional:!0,default_value:"None"},{name:"random_state",docstring:"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null}],outputs:[],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_iter_indices",docstring:"None",inputs:[],outputs:[]},{name:"get_n_splits",docstring:"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        ",inputs:[{name:"X",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"y",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[{name:"n_splits",docstring:"Returns the number of splitting iterations in the cross-validator.",param_type:["int"],returned:!0}]},{name:"split",docstring:"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, shape (n_samples,), optional\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        ",inputs:[{name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"The target variable for supervised learning problems.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null}],outputs:[{name:"train",docstring:"The training set indices for that split.",param_type:["array"],returned:!1},{name:"test",docstring:"The testing set indices for that split.",param_type:["array"],returned:!1}]}],nodes:[]},{name:"KFold",docstring:'K-Folds cross-validator\n\n    Provides train/test indices to split data in train/test sets. Split\n    dataset into k consecutive folds (without shuffling by default).\n\n    Each fold is then used once as a validation while the k - 1 remaining\n    folds form the training set.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default=3\n        Number of folds. Must be at least 2.\n\n        .. versionchanged:: 0.20\n            ``n_splits`` default value will change from 3 to 5 in v0.22.\n\n    shuffle : boolean, optional\n        Whether to shuffle the data before splitting into batches.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``shuffle`` == True.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import KFold\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([1, 2, 3, 4])\n    >>> kf = KFold(n_splits=2)\n    >>> kf.get_n_splits(X)\n    2\n    >>> print(kf)  # doctest: +NORMALIZE_WHITESPACE\n    KFold(n_splits=2, random_state=None, shuffle=False)\n    >>> for train_index, test_index in kf.split(X):\n    ...    print("TRAIN:", train_index, "TEST:", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [2 3] TEST: [0 1]\n    TRAIN: [0 1] TEST: [2 3]\n\n    Notes\n    -----\n    The first ``n_samples % n_splits`` folds have size\n    ``n_samples // n_splits + 1``, other folds have size\n    ``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n\n    Randomized CV splitters may return different results for each call of\n    split. You can make the results identical by setting ``random_state``\n    to an integer.\n\n    See also\n    --------\n    StratifiedKFold\n        Takes group information into account to avoid building folds with\n        imbalanced class distributions (for binary or multiclass\n        classification tasks).\n\n    GroupKFold: K-fold iterator variant with non-overlapping groups.\n\n    RepeatedKFold: Repeats K-Fold n times.\n    ',inputs:[{name:"n_splits",docstring:"Number of folds. Must be at least 2.  .. versionchanged:: 0.20     ``n_splits`` default value will change from 3 to 5 in v0.22.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"3"},{name:"shuffle",docstring:"Whether to shuffle the data before splitting into batches.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:null},{name:"random_state",docstring:"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. Used when ``shuffle`` == True.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null}],outputs:[],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_iter_test_indices",docstring:"None",inputs:[],outputs:[]},{name:"_iter_test_masks",docstring:"Generates boolean masks corresponding to test sets.\n\n        By default, delegates to _iter_test_indices(X, y, groups)\n        ",inputs:[],outputs:[]},{name:"get_n_splits",docstring:"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        ",inputs:[{name:"X",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"y",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[{name:"n_splits",docstring:"Returns the number of splitting iterations in the cross-validator.",param_type:["int"],returned:!0}]},{name:"split",docstring:"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        ",inputs:[{name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"The target variable for supervised learning problems.",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null}],outputs:[{name:"train",docstring:"The training set indices for that split.",param_type:["array"],returned:!1},{name:"test",docstring:"The testing set indices for that split.",param_type:["array"],returned:!1}]}],nodes:[]},{name:"LeaveOneGroupOut",docstring:'Leave One Group Out cross-validator\n\n    Provides train/test indices to split data according to a third-party\n    provided group. This group information can be used to encode arbitrary\n    domain specific stratifications of the samples as integers.\n\n    For instance the groups could be the year of collection of the samples\n    and thus allow for cross-validation against time-based splits.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import LeaveOneGroupOut\n    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    >>> y = np.array([1, 2, 1, 2])\n    >>> groups = np.array([1, 1, 2, 2])\n    >>> logo = LeaveOneGroupOut()\n    >>> logo.get_n_splits(X, y, groups)\n    2\n    >>> logo.get_n_splits(groups=groups)  # \'groups\' is always required\n    2\n    >>> print(logo)\n    LeaveOneGroupOut()\n    >>> for train_index, test_index in logo.split(X, y, groups):\n    ...    print("TRAIN:", train_index, "TEST:", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    ...    print(X_train, X_test, y_train, y_test)\n    TRAIN: [2 3] TEST: [0 1]\n    [[5 6]\n     [7 8]] [[1 2]\n     [3 4]] [1 2] [1 2]\n    TRAIN: [0 1] TEST: [2 3]\n    [[1 2]\n     [3 4]] [[5 6]\n     [7 8]] [1 2] [1 2]\n\n    ',inputs:[],outputs:[],node_functions:[{name:"__init__",docstring:"Initialize self.  See help(type(self)) for accurate signature.",inputs:[],outputs:[]},{name:"_iter_test_indices",docstring:"Generates integer indices corresponding to test sets.",inputs:[],outputs:[]},{name:"_iter_test_masks",docstring:"None",inputs:[],outputs:[]},{name:"get_n_splits",docstring:"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set. This 'groups' parameter must always be specified to\n            calculate the number of splits, though the other parameters can be\n            omitted.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        ",inputs:[{name:"X",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"y",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set. This 'groups' parameter must always be specified to calculate the number of splits, though the other parameters can be omitted.",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null}],outputs:[{name:"n_splits",docstring:"Returns the number of splitting iterations in the cross-validator.",param_type:["int"],returned:!0}]},{name:"split",docstring:"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, of length n_samples, optional\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        ",inputs:[{name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"The target variable for supervised learning problems.",param_type:["array"],expected_shape:null,is_optional:!0,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null}],outputs:[{name:"train",docstring:"The training set indices for that split.",param_type:["array"],returned:!1},{name:"test",docstring:"The testing set indices for that split.",param_type:["array"],returned:!1}]}],nodes:[]},{name:"LeaveOneOut",docstring:'Leave-One-Out cross-validator\n\n    Provides train/test indices to split data in train/test sets. Each\n    sample is used once as a test set (singleton) while the remaining\n    samples form the training set.\n\n    Note: ``LeaveOneOut()`` is equivalent to ``KFold(n_splits=n)`` and\n    ``LeavePOut(p=1)`` where ``n`` is the number of samples.\n\n    Due to the high number of test sets (which is the same as the\n    number of samples) this cross-validation method can be very costly.\n    For large datasets one should favor :class:`KFold`, :class:`ShuffleSplit`\n    or :class:`StratifiedKFold`.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import LeaveOneOut\n    >>> X = np.array([[1, 2], [3, 4]])\n    >>> y = np.array([1, 2])\n    >>> loo = LeaveOneOut()\n    >>> loo.get_n_splits(X)\n    2\n    >>> print(loo)\n    LeaveOneOut()\n    >>> for train_index, test_index in loo.split(X):\n    ...    print("TRAIN:", train_index, "TEST:", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    ...    print(X_train, X_test, y_train, y_test)\n    TRAIN: [1] TEST: [0]\n    [[3 4]] [[1 2]] [2] [1]\n    TRAIN: [0] TEST: [1]\n    [[1 2]] [[3 4]] [1] [2]\n\n    See also\n    --------\n    LeaveOneGroupOut\n        For splitting the data according to explicit, domain-specific\n        stratification of the dataset.\n\n    GroupKFold: K-fold iterator variant with non-overlapping groups.\n    ',inputs:[],outputs:[],node_functions:[{name:"__init__",docstring:"Initialize self.  See help(type(self)) for accurate signature.",inputs:[],outputs:[]},{name:"_iter_test_indices",docstring:"None",inputs:[],outputs:[]},{name:"_iter_test_masks",docstring:"Generates boolean masks corresponding to test sets.\n\n        By default, delegates to _iter_test_indices(X, y, groups)\n        ",inputs:[],outputs:[]},{name:"get_n_splits",docstring:"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        ",inputs:[{name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[{name:"n_splits",docstring:"Returns the number of splitting iterations in the cross-validator.",param_type:["int"],returned:!0}]},{name:"split",docstring:"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, of length n_samples\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        ",inputs:[{name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"The target variable for supervised learning problems.",param_type:["array"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null}],outputs:[{name:"train",docstring:"The training set indices for that split.",param_type:["array"],returned:!1},{name:"test",docstring:"The testing set indices for that split.",param_type:["array"],returned:!1}]}],nodes:[]},{name:"LeavePGroupsOut",docstring:'Leave P Group(s) Out cross-validator\n\n    Provides train/test indices to split data according to a third-party\n    provided group. This group information can be used to encode arbitrary\n    domain specific stratifications of the samples as integers.\n\n    For instance the groups could be the year of collection of the samples\n    and thus allow for cross-validation against time-based splits.\n\n    The difference between LeavePGroupsOut and LeaveOneGroupOut is that\n    the former builds the test sets with all the samples assigned to\n    ``p`` different values of the groups while the latter uses samples\n    all assigned the same groups.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_groups : int\n        Number of groups (``p``) to leave out in the test split.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import LeavePGroupsOut\n    >>> X = np.array([[1, 2], [3, 4], [5, 6]])\n    >>> y = np.array([1, 2, 1])\n    >>> groups = np.array([1, 2, 3])\n    >>> lpgo = LeavePGroupsOut(n_groups=2)\n    >>> lpgo.get_n_splits(X, y, groups)\n    3\n    >>> lpgo.get_n_splits(groups=groups)  # \'groups\' is always required\n    3\n    >>> print(lpgo)\n    LeavePGroupsOut(n_groups=2)\n    >>> for train_index, test_index in lpgo.split(X, y, groups):\n    ...    print("TRAIN:", train_index, "TEST:", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    ...    print(X_train, X_test, y_train, y_test)\n    TRAIN: [2] TEST: [0 1]\n    [[5 6]] [[1 2]\n     [3 4]] [1] [1 2]\n    TRAIN: [1] TEST: [0 2]\n    [[3 4]] [[1 2]\n     [5 6]] [2] [1 1]\n    TRAIN: [0] TEST: [1 2]\n    [[1 2]] [[3 4]\n     [5 6]] [1] [2 1]\n\n    See also\n    --------\n    GroupKFold: K-fold iterator variant with non-overlapping groups.\n    ',inputs:[{name:"n_groups",docstring:"Number of groups (``p``) to leave out in the test split.",param_type:["int"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_iter_test_indices",docstring:"Generates integer indices corresponding to test sets.",inputs:[],outputs:[]},{name:"_iter_test_masks",docstring:"None",inputs:[],outputs:[]},{name:"get_n_splits",docstring:"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set. This 'groups' parameter must always be specified to\n            calculate the number of splits, though the other parameters can be\n            omitted.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        ",inputs:[{name:"X",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"y",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set. This 'groups' parameter must always be specified to calculate the number of splits, though the other parameters can be omitted.",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null}],outputs:[{name:"n_splits",docstring:"Returns the number of splitting iterations in the cross-validator.",param_type:["int"],returned:!0}]},{name:"split",docstring:"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, of length n_samples, optional\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        ",inputs:[{name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"The target variable for supervised learning problems.",param_type:["array"],expected_shape:null,is_optional:!0,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null}],outputs:[{name:"train",docstring:"The training set indices for that split.",param_type:["array"],returned:!1},{name:"test",docstring:"The testing set indices for that split.",param_type:["array"],returned:!1}]}],nodes:[]},{name:"LeavePOut",docstring:'Leave-P-Out cross-validator\n\n    Provides train/test indices to split data in train/test sets. This results\n    in testing on all distinct samples of size p, while the remaining n - p\n    samples form the training set in each iteration.\n\n    Note: ``LeavePOut(p)`` is NOT equivalent to\n    ``KFold(n_splits=n_samples // p)`` which creates non-overlapping test sets.\n\n    Due to the high number of iterations which grows combinatorically with the\n    number of samples this cross-validation method can be very costly. For\n    large datasets one should favor :class:`KFold`, :class:`StratifiedKFold`\n    or :class:`ShuffleSplit`.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    p : int\n        Size of the test sets. Must be strictly greater than the number of\n        samples.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import LeavePOut\n    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    >>> y = np.array([1, 2, 3, 4])\n    >>> lpo = LeavePOut(2)\n    >>> lpo.get_n_splits(X)\n    6\n    >>> print(lpo)\n    LeavePOut(p=2)\n    >>> for train_index, test_index in lpo.split(X):\n    ...    print("TRAIN:", train_index, "TEST:", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [2 3] TEST: [0 1]\n    TRAIN: [1 3] TEST: [0 2]\n    TRAIN: [1 2] TEST: [0 3]\n    TRAIN: [0 3] TEST: [1 2]\n    TRAIN: [0 2] TEST: [1 3]\n    TRAIN: [0 1] TEST: [2 3]\n    ',inputs:[{name:"p",docstring:"Size of the test sets. Must be strictly greater than the number of samples.",param_type:["int"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_iter_test_indices",docstring:"None",inputs:[],outputs:[]},{name:"_iter_test_masks",docstring:"Generates boolean masks corresponding to test sets.\n\n        By default, delegates to _iter_test_indices(X, y, groups)\n        ",inputs:[],outputs:[]},{name:"get_n_splits",docstring:"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n        ",inputs:[{name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[]},{name:"split",docstring:"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, of length n_samples\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        ",inputs:[{name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"The target variable for supervised learning problems.",param_type:["array"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null}],outputs:[{name:"train",docstring:"The training set indices for that split.",param_type:["array"],returned:!1},{name:"test",docstring:"The testing set indices for that split.",param_type:["array"],returned:!1}]}],nodes:[]},{name:"ParameterGrid",docstring:"Grid of parameters with a discrete number of values for each.\n\n    Can be used to iterate over parameter value combinations with the\n    Python built-in function iter.\n\n    Read more in the :ref:`User Guide <grid_search>`.\n\n    Parameters\n    ----------\n    param_grid : dict of string to sequence, or sequence of such\n        The parameter grid to explore, as a dictionary mapping estimator\n        parameters to sequences of allowed values.\n\n        An empty dict signifies default parameters.\n\n        A sequence of dicts signifies a sequence of grids to search, and is\n        useful to avoid exploring parameter combinations that make no sense\n        or have no effect. See the examples below.\n\n    Examples\n    --------\n    >>> from sklearn.model_selection import ParameterGrid\n    >>> param_grid = {'a': [1, 2], 'b': [True, False]}\n    >>> list(ParameterGrid(param_grid)) == (\n    ...    [{'a': 1, 'b': True}, {'a': 1, 'b': False},\n    ...     {'a': 2, 'b': True}, {'a': 2, 'b': False}])\n    True\n\n    >>> grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]\n    >>> list(ParameterGrid(grid)) == [{'kernel': 'linear'},\n    ...                               {'kernel': 'rbf', 'gamma': 1},\n    ...                               {'kernel': 'rbf', 'gamma': 10}]\n    True\n    >>> ParameterGrid(grid)[1] == {'kernel': 'rbf', 'gamma': 1}\n    True\n\n    See also\n    --------\n    :class:`GridSearchCV`:\n        Uses :class:`ParameterGrid` to perform a full parallelized parameter\n        search.\n    ",inputs:[{name:"param_grid",docstring:"The parameter grid to explore, as a dictionary mapping estimator parameters to sequences of allowed values.  An empty dict signifies default parameters.  A sequence of dicts signifies a sequence of grids to search, and is useful to avoid exploring parameter combinations that make no sense or have no effect. See the examples below.",param_type:["str","dict"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]}],nodes:[]},{name:"ParameterSampler",docstring:"Generator on parameters sampled from given distributions.\n\n    Non-deterministic iterable over random candidate combinations for hyper-\n    parameter search. If all parameters are presented as a list,\n    sampling without replacement is performed. If at least one parameter\n    is given as a distribution, sampling with replacement is used.\n    It is highly recommended to use continuous distributions for continuous\n    parameters.\n\n    Note that before SciPy 0.16, the ``scipy.stats.distributions`` do not\n    accept a custom RNG instance and always use the singleton RNG from\n    ``numpy.random``. Hence setting ``random_state`` will not guarantee a\n    deterministic iteration whenever ``scipy.stats`` distributions are used to\n    define the parameter search space. Deterministic behavior is however\n    guaranteed from SciPy 0.16 onwards.\n\n    Read more in the :ref:`User Guide <search>`.\n\n    Parameters\n    ----------\n    param_distributions : dict\n        Dictionary where the keys are parameters and values\n        are distributions from which a parameter is to be sampled.\n        Distributions either have to provide a ``rvs`` function\n        to sample from them, or can be given as a list of values,\n        where a uniform distribution is assumed.\n\n    n_iter : integer\n        Number of parameter settings that are produced.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        Pseudo random number generator state used for random uniform sampling\n        from lists of possible values instead of scipy.stats distributions.\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    params : dict of string to any\n        **Yields** dictionaries mapping each estimator parameter to\n        as sampled value.\n\n    Examples\n    --------\n    >>> from sklearn.model_selection import ParameterSampler\n    >>> from scipy.stats.distributions import expon\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> param_grid = {'a':[1, 2], 'b': expon()}\n    >>> param_list = list(ParameterSampler(param_grid, n_iter=4,\n    ...                                    random_state=rng))\n    >>> rounded_list = [dict((k, round(v, 6)) for (k, v) in d.items())\n    ...                 for d in param_list]\n    >>> rounded_list == [{'b': 0.89856, 'a': 1},\n    ...                  {'b': 0.923223, 'a': 1},\n    ...                  {'b': 1.878964, 'a': 2},\n    ...                  {'b': 1.038159, 'a': 2}]\n    True\n    ",inputs:[{name:"param_distributions",docstring:"Dictionary where the keys are parameters and values are distributions from which a parameter is to be sampled. Distributions either have to provide a ``rvs`` function to sample from them, or can be given as a list of values, where a uniform distribution is assumed.",param_type:["dict"],expected_shape:null,is_optional:!1,default_value:null},{name:"n_iter",docstring:"Number of parameter settings that are produced.",param_type:["int"],expected_shape:null,is_optional:!1,default_value:null},{name:"random_state",docstring:"Pseudo random number generator state used for random uniform sampling from lists of possible values instead of scipy.stats distributions. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null}],outputs:[{name:"params",docstring:"**Yields** dictionaries mapping each estimator parameter to as sampled value.",param_type:["str","dict"],returned:!0}],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]}],nodes:[]},{name:"PredefinedSplit",docstring:'Predefined split cross-validator\n\n    Provides train/test indices to split data into train/test sets using a\n    predefined scheme specified by the user with the ``test_fold`` parameter.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    test_fold : array-like, shape (n_samples,)\n        The entry ``test_fold[i]`` represents the index of the test set that\n        sample ``i`` belongs to. It is possible to exclude sample ``i`` from\n        any test set (i.e. include sample ``i`` in every training set) by\n        setting ``test_fold[i]`` equal to -1.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import PredefinedSplit\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> test_fold = [0, 1, -1, 1]\n    >>> ps = PredefinedSplit(test_fold)\n    >>> ps.get_n_splits()\n    2\n    >>> print(ps)       # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n    PredefinedSplit(test_fold=array([ 0,  1, -1,  1]))\n    >>> for train_index, test_index in ps.split():\n    ...    print("TRAIN:", train_index, "TEST:", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [1 2 3] TEST: [0]\n    TRAIN: [0 2] TEST: [1 3]\n    ',inputs:[{name:"test_fold",docstring:"The entry ``test_fold[i]`` represents the index of the test set that sample ``i`` belongs to. It is possible to exclude sample ``i`` from any test set (i.e. include sample ``i`` in every training set) by setting ``test_fold[i]`` equal to -1.",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null}],outputs:[],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_iter_test_indices",docstring:"Generates integer indices corresponding to test sets.",inputs:[],outputs:[]},{name:"_iter_test_masks",docstring:"Generates boolean masks corresponding to test sets.",inputs:[],outputs:[]},{name:"get_n_splits",docstring:"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        ",inputs:[{name:"X",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"y",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[{name:"n_splits",docstring:"Returns the number of splitting iterations in the cross-validator.",param_type:["int"],returned:!0}]},{name:"split",docstring:"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        ",inputs:[{name:"X",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"y",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[{name:"train",docstring:"The training set indices for that split.",param_type:["array"],returned:!1},{name:"test",docstring:"The testing set indices for that split.",param_type:["array"],returned:!1}]}],nodes:[]},{name:"RandomizedSearchCV",docstring:"Randomized search on hyper parameters.\n\n    RandomizedSearchCV implements a \"fit\" and a \"score\" method.\n    It also implements \"predict\", \"predict_proba\", \"decision_function\",\n    \"transform\" and \"inverse_transform\" if they are implemented in the\n    estimator used.\n\n    The parameters of the estimator used to apply these methods are optimized\n    by cross-validated search over parameter settings.\n\n    In contrast to GridSearchCV, not all parameter values are tried out, but\n    rather a fixed number of parameter settings is sampled from the specified\n    distributions. The number of parameter settings that are tried is\n    given by n_iter.\n\n    If all parameters are presented as a list,\n    sampling without replacement is performed. If at least one parameter\n    is given as a distribution, sampling with replacement is used.\n    It is highly recommended to use continuous distributions for continuous\n    parameters.\n\n    Note that before SciPy 0.16, the ``scipy.stats.distributions`` do not\n    accept a custom RNG instance and always use the singleton RNG from\n    ``numpy.random``. Hence setting ``random_state`` will not guarantee a\n    deterministic iteration whenever ``scipy.stats`` distributions are used to\n    define the parameter search space.\n\n    Read more in the :ref:`User Guide <randomized_parameter_search>`.\n\n    Parameters\n    ----------\n    estimator : estimator object.\n        A object of that type is instantiated for each grid point.\n        This is assumed to implement the scikit-learn estimator interface.\n        Either estimator needs to provide a ``score`` function,\n        or ``scoring`` must be passed.\n\n    param_distributions : dict\n        Dictionary with parameters names (string) as keys and distributions\n        or lists of parameters to try. Distributions must provide a ``rvs``\n        method for sampling (such as those from scipy.stats.distributions).\n        If a list is given, it is sampled uniformly.\n\n    n_iter : int, default=10\n        Number of parameter settings that are sampled. n_iter trades\n        off runtime vs quality of the solution.\n\n    scoring : string, callable, list/tuple, dict or None, default: None\n        A single string (see :ref:`scoring_parameter`) or a callable\n        (see :ref:`scoring`) to evaluate the predictions on the test set.\n\n        For evaluating multiple metrics, either give a list of (unique) strings\n        or a dict with names as keys and callables as values.\n\n        NOTE that when using custom scorers, each scorer should return a single\n        value. Metric functions returning a list/array of values can be wrapped\n        into multiple scorers that return one value each.\n\n        See :ref:`multimetric_grid_search` for an example.\n\n        If None, the estimator's score method is used.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    pre_dispatch : int, or string, optional\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n            - None, in which case all the jobs are immediately\n              created and spawned. Use this for lightweight and\n              fast-running jobs, to avoid delays due to on-demand\n              spawning of the jobs\n\n            - An int, giving the exact number of total jobs that are\n              spawned\n\n            - A string, giving an expression as a function of n_jobs,\n              as in '2*n_jobs'\n\n    iid : boolean, default='warn'\n        If True, return the average score across folds, weighted by the number\n        of samples in each test set. In this case, the data is assumed to be\n        identically distributed across the folds, and the loss minimized is\n        the total loss per sample, and not the mean loss across the folds. If\n        False, return the average score across folds. Default is True, but\n        will change to False in version 0.22, to correspond to the standard\n        definition of cross-validation.\n\n        .. versionchanged:: 0.20\n            Parameter ``iid`` will change from True to False by default in\n            version 0.22, and will be removed in 0.24.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value if None will change from 3-fold to 5-fold\n            in v0.22.\n\n    refit : boolean, string, or callable, default=True\n        Refit an estimator using the best found parameters on the whole\n        dataset.\n\n        For multiple metric evaluation, this needs to be a string denoting the\n        scorer that would be used to find the best parameters for refitting\n        the estimator at the end.\n\n        Where there are considerations other than maximum score in\n        choosing a best estimator, ``refit`` can be set to a function which\n        returns the selected ``best_index_`` given the ``cv_results``.\n\n        The refitted estimator is made available at the ``best_estimator_``\n        attribute and permits using ``predict`` directly on this\n        ``RandomizedSearchCV`` instance.\n\n        Also for multiple metric evaluation, the attributes ``best_index_``,\n        ``best_score_`` and ``best_params_`` will only be available if\n        ``refit`` is set and all of them will be determined w.r.t this specific\n        scorer. When refit is callable, ``best_score_`` is disabled.\n\n        See ``scoring`` parameter to know more about multiple metric\n        evaluation.\n\n        .. versionchanged:: 0.20\n            Support for callable added.\n\n    verbose : integer\n        Controls the verbosity: the higher, the more messages.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        Pseudo random number generator state used for random uniform sampling\n        from lists of possible values instead of scipy.stats distributions.\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    error_score : 'raise' or numeric\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised. If a numeric value is given,\n        FitFailedWarning is raised. This parameter does not affect the refit\n        step, which will always raise the error. Default is 'raise' but from\n        version 0.22 it will change to np.nan.\n\n    return_train_score : boolean, default=False\n        If ``False``, the ``cv_results_`` attribute will not include training\n        scores.\n        Computing training scores is used to get insights on how different\n        parameter settings impact the overfitting/underfitting trade-off.\n        However computing the scores on the training set can be computationally\n        expensive and is not strictly required to select the parameters that\n        yield the best generalization performance.\n\n    Attributes\n    ----------\n    cv_results_ : dict of numpy (masked) ndarrays\n        A dict with keys as column headers and values as columns, that can be\n        imported into a pandas ``DataFrame``.\n\n        For instance the below given table\n\n        +--------------+-------------+-------------------+---+---------------+\n        | param_kernel | param_gamma | split0_test_score |...|rank_test_score|\n        +==============+=============+===================+===+===============+\n        |    'rbf'     |     0.1     |       0.80        |...|       2       |\n        +--------------+-------------+-------------------+---+---------------+\n        |    'rbf'     |     0.2     |       0.90        |...|       1       |\n        +--------------+-------------+-------------------+---+---------------+\n        |    'rbf'     |     0.3     |       0.70        |...|       1       |\n        +--------------+-------------+-------------------+---+---------------+\n\n        will be represented by a ``cv_results_`` dict of::\n\n            {\n            'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],\n                                          mask = False),\n            'param_gamma'  : masked_array(data = [0.1 0.2 0.3], mask = False),\n            'split0_test_score'  : [0.80, 0.90, 0.70],\n            'split1_test_score'  : [0.82, 0.50, 0.70],\n            'mean_test_score'    : [0.81, 0.70, 0.70],\n            'std_test_score'     : [0.01, 0.20, 0.00],\n            'rank_test_score'    : [3, 1, 1],\n            'split0_train_score' : [0.80, 0.92, 0.70],\n            'split1_train_score' : [0.82, 0.55, 0.70],\n            'mean_train_score'   : [0.81, 0.74, 0.70],\n            'std_train_score'    : [0.01, 0.19, 0.00],\n            'mean_fit_time'      : [0.73, 0.63, 0.43],\n            'std_fit_time'       : [0.01, 0.02, 0.01],\n            'mean_score_time'    : [0.01, 0.06, 0.04],\n            'std_score_time'     : [0.00, 0.00, 0.00],\n            'params'             : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...],\n            }\n\n        NOTE\n\n        The key ``'params'`` is used to store a list of parameter\n        settings dicts for all the parameter candidates.\n\n        The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n        ``std_score_time`` are all in seconds.\n\n        For multi-metric evaluation, the scores for all the scorers are\n        available in the ``cv_results_`` dict at the keys ending with that\n        scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n        above. ('split0_test_precision', 'mean_train_precision' etc.)\n\n    best_estimator_ : estimator or dict\n        Estimator that was chosen by the search, i.e. estimator\n        which gave highest score (or smallest loss if specified)\n        on the left out data. Not available if ``refit=False``.\n\n        For multi-metric evaluation, this attribute is present only if\n        ``refit`` is specified.\n\n        See ``refit`` parameter for more information on allowed values.\n\n    best_score_ : float\n        Mean cross-validated score of the best_estimator.\n\n        For multi-metric evaluation, this is not available if ``refit`` is\n        ``False``. See ``refit`` parameter for more information.\n\n    best_params_ : dict\n        Parameter setting that gave the best results on the hold out data.\n\n        For multi-metric evaluation, this is not available if ``refit`` is\n        ``False``. See ``refit`` parameter for more information.\n\n    best_index_ : int\n        The index (of the ``cv_results_`` arrays) which corresponds to the best\n        candidate parameter setting.\n\n        The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n        the parameter setting for the best model, that gives the highest\n        mean score (``search.best_score_``).\n\n        For multi-metric evaluation, this is not available if ``refit`` is\n        ``False``. See ``refit`` parameter for more information.\n\n    scorer_ : function or a dict\n        Scorer function used on the held out data to choose the best\n        parameters for the model.\n\n        For multi-metric evaluation, this attribute holds the validated\n        ``scoring`` dict which maps the scorer key to the scorer callable.\n\n    n_splits_ : int\n        The number of cross-validation splits (folds/iterations).\n\n    refit_time_ : float\n        Seconds used for refitting the best model on the whole dataset.\n\n        This is present only if ``refit`` is not False.\n\n    Notes\n    -----\n    The parameters selected are those that maximize the score of the held-out\n    data, according to the scoring parameter.\n\n    If `n_jobs` was set to a value higher than one, the data is copied for each\n    parameter setting(and not `n_jobs` times). This is done for efficiency\n    reasons if individual jobs take very little time, but may raise errors if\n    the dataset is large and not enough memory is available.  A workaround in\n    this case is to set `pre_dispatch`. Then, the memory is copied only\n    `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n    n_jobs`.\n\n    See Also\n    --------\n    :class:`GridSearchCV`:\n        Does exhaustive search over a grid of parameters.\n\n    :class:`ParameterSampler`:\n        A generator over parameter settings, constructed from\n        param_distributions.\n\n    ",inputs:[{name:"estimator",docstring:"A object of that type is instantiated for each grid point. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a ``score`` function, or ``scoring`` must be passed.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"param_distributions",docstring:"Dictionary with parameters names (string) as keys and distributions or lists of parameters to try. Distributions must provide a ``rvs`` method for sampling (such as those from scipy.stats.distributions). If a list is given, it is sampled uniformly.",param_type:["dict"],expected_shape:null,is_optional:!1,default_value:null},{name:"n_iter",docstring:"Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"10"},{name:"scoring",docstring:"A single string (see :ref:`scoring_parameter`) or a callable (see :ref:`scoring`) to evaluate the predictions on the test set.  For evaluating multiple metrics, either give a list of (unique) strings or a dict with names as keys and callables as values.  NOTE that when using custom scorers, each scorer should return a single value. Metric functions returning a list/array of values can be wrapped into multiple scorers that return one value each.  See :ref:`multimetric_grid_search` for an example.  If None, the estimator's score method is used.",param_type:["str","dict","list","tuple","callable",null],expected_shape:null,is_optional:!0,default_value:"None"},{name:"n_jobs",docstring:"Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null},{name:"pre_dispatch",docstring:"Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:      - None, in which case all the jobs are immediately       created and spawned. Use this for lightweight and       fast-running jobs, to avoid delays due to on-demand       spawning of the jobs      - An int, giving the exact number of total jobs that are       spawned      - A string, giving an expression as a function of n_jobs,       as in '2*n_jobs'",param_type:["int","str"],expected_shape:null,is_optional:!0,default_value:null},{name:"iid",docstring:"If True, return the average score across folds, weighted by the number of samples in each test set. In this case, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds. If False, return the average score across folds. Default is True, but will change to False in version 0.22, to correspond to the standard definition of cross-validation.  .. versionchanged:: 0.20     Parameter ``iid`` will change from True to False by default in     version 0.22, and will be removed in 0.24.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"warn"},{name:"cv",docstring:"Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  .. versionchanged:: 0.20     ``cv`` default value if None will change from 3-fold to 5-fold     in v0.22.",param_type:["int","iter"],expected_shape:null,is_optional:!0,default_value:null},{name:"refit",docstring:"Refit an estimator using the best found parameters on the whole dataset.  For multiple metric evaluation, this needs to be a string denoting the scorer that would be used to find the best parameters for refitting the estimator at the end.  Where there are considerations other than maximum score in choosing a best estimator, ``refit`` can be set to a function which returns the selected ``best_index_`` given the ``cv_results``.  The refitted estimator is made available at the ``best_estimator_`` attribute and permits using ``predict`` directly on this ``RandomizedSearchCV`` instance.  Also for multiple metric evaluation, the attributes ``best_index_``, ``best_score_`` and ``best_params_`` will only be available if ``refit`` is set and all of them will be determined w.r.t this specific scorer. When refit is callable, ``best_score_`` is disabled.  See ``scoring`` parameter to know more about multiple metric evaluation.  .. versionchanged:: 0.20     Support for callable added.",param_type:["bool","str","callable"],expected_shape:null,is_optional:!0,default_value:"True"},{name:"verbose",docstring:"Controls the verbosity: the higher, the more messages.",param_type:["int"],expected_shape:null,is_optional:!1,default_value:null},{name:"random_state",docstring:"Pseudo random number generator state used for random uniform sampling from lists of possible values instead of scipy.stats distributions. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null},{name:"error_score",docstring:"Value to assign to the score if an error occurs in estimator fitting. If set to 'raise', the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. Default is 'raise' but from version 0.22 it will change to np.nan.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null},{name:"return_train_score",docstring:"If ``False``, the ``cv_results_`` attribute will not include training scores. Computing training scores is used to get insights on how different parameter settings impact the overfitting/underfitting trade-off. However computing the scores on the training set can be computationally expensive and is not strictly required to select the parameters that yield the best generalization performance.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"False"}],outputs:[{name:"cv_results_",docstring:"A dict with keys as column headers and values as columns, that can be imported into a pandas ``DataFrame``.  For instance the below given table  +--------------+-------------+-------------------+---+---------------+ | param_kernel | param_gamma | split0_test_score |...|rank_test_score| +==============+=============+===================+===+===============+ |    'rbf'     |     0.1     |       0.80        |...|       2       | +--------------+-------------+-------------------+---+---------------+ |    'rbf'     |     0.2     |       0.90        |...|       1       | +--------------+-------------+-------------------+---+---------------+ |    'rbf'     |     0.3     |       0.70        |...|       1       | +--------------+-------------+-------------------+---+---------------+  will be represented by a ``cv_results_`` dict of::      {     'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],                                   mask = False),     'param_gamma'  : masked_array(data = [0.1 0.2 0.3], mask = False),     'split0_test_score'  : [0.80, 0.90, 0.70],     'split1_test_score'  : [0.82, 0.50, 0.70],     'mean_test_score'    : [0.81, 0.70, 0.70],     'std_test_score'     : [0.01, 0.20, 0.00],     'rank_test_score'    : [3, 1, 1],     'split0_train_score' : [0.80, 0.92, 0.70],     'split1_train_score' : [0.82, 0.55, 0.70],     'mean_train_score'   : [0.81, 0.74, 0.70],     'std_train_score'    : [0.01, 0.19, 0.00],     'mean_fit_time'      : [0.73, 0.63, 0.43],     'std_fit_time'       : [0.01, 0.02, 0.01],     'mean_score_time'    : [0.01, 0.06, 0.04],     'std_score_time'     : [0.00, 0.00, 0.00],     'params'             : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...],     }  NOTE  The key ``'params'`` is used to store a list of parameter settings dicts for all the parameter candidates.  The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and ``std_score_time`` are all in seconds.  For multi-metric evaluation, the scores for all the scorers are available in the ``cv_results_`` dict at the keys ending with that scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown above. ('split0_test_precision', 'mean_train_precision' etc.)",param_type:["array","dict"],returned:!1},{name:"best_estimator_",docstring:"Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if ``refit=False``.  For multi-metric evaluation, this attribute is present only if ``refit`` is specified.  See ``refit`` parameter for more information on allowed values.",param_type:["dict"],returned:!1},{name:"best_score_",docstring:"Mean cross-validated score of the best_estimator.  For multi-metric evaluation, this is not available if ``refit`` is ``False``. See ``refit`` parameter for more information.",param_type:["float"],returned:!1},{name:"best_params_",docstring:"Parameter setting that gave the best results on the hold out data.  For multi-metric evaluation, this is not available if ``refit`` is ``False``. See ``refit`` parameter for more information.",param_type:["dict"],returned:!1},{name:"best_index_",docstring:"The index (of the ``cv_results_`` arrays) which corresponds to the best candidate parameter setting.  The dict at ``search.cv_results_['params'][search.best_index_]`` gives the parameter setting for the best model, that gives the highest mean score (``search.best_score_``).  For multi-metric evaluation, this is not available if ``refit`` is ``False``. See ``refit`` parameter for more information.",param_type:["int"],returned:!1},{name:"scorer_",docstring:"Scorer function used on the held out data to choose the best parameters for the model.  For multi-metric evaluation, this attribute holds the validated ``scoring`` dict which maps the scorer key to the scorer callable.",param_type:["dict"],returned:!1},{name:"n_splits_",docstring:"The number of cross-validation splits (folds/iterations).",param_type:["int"],returned:!1},{name:"refit_time_",docstring:"Seconds used for refitting the best model on the whole dataset.  This is present only if ``refit`` is not False.",param_type:["float"],returned:!1}],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_check_is_fitted",docstring:"None",inputs:[],outputs:[]},{name:"_format_results",docstring:"None",inputs:[],outputs:[]},{name:"_get_param_names",docstring:"Get parameter names for the estimator",inputs:[],outputs:[]},{name:"_get_tags",docstring:"None",inputs:[],outputs:[]},{name:"_run_search",docstring:"Search n_iter candidates from param_distributions",inputs:[],outputs:[]},{name:"decision_function",docstring:"Call decision_function on the estimator with the best found parameters.\n\n        Only available if ``refit=True`` and the underlying estimator supports\n        ``decision_function``.\n\n        Parameters\n        ----------\n        X : indexable, length n_samples\n            Must fulfill the input assumptions of the\n            underlying estimator.\n\n        ",inputs:[{name:"X",docstring:"Must fulfill the input assumptions of the underlying estimator.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null}],outputs:[]},{name:"fit",docstring:"Run fit with all sets of parameters.\n\n        Parameters\n        ----------\n\n        X : array-like, shape = [n_samples, n_features]\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n            Target relative to X for classification or regression;\n            None for unsupervised learning.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        **fit_params : dict of string -> object\n            Parameters passed to the ``fit`` method of the estimator\n        ",inputs:[{name:"X",docstring:"Training vector, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null},{name:"y",docstring:"Target relative to X for classification or regression; None for unsupervised learning.",param_type:["array"],expected_shape:"[n_samples] or [n_samples, n_output], optional",is_optional:!0,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null},{name:"**fit_params",docstring:"Parameters passed to the ``fit`` method of the estimator",param_type:["object","str","dict"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[]},{name:"get_params",docstring:"Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : boolean, optional\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : mapping of string to any\n            Parameter names mapped to their values.\n        ",inputs:[{name:"deep",docstring:"If True, will return the parameters for this estimator and contained subobjects that are estimators.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:null}],outputs:[{name:"params",docstring:"Parameter names mapped to their values.",param_type:["str"],returned:!0}]},{name:"inverse_transform",docstring:"Call inverse_transform on the estimator with the best found params.\n\n        Only available if the underlying estimator implements\n        ``inverse_transform`` and ``refit=True``.\n\n        Parameters\n        ----------\n        Xt : indexable, length n_samples\n            Must fulfill the input assumptions of the\n            underlying estimator.\n\n        ",inputs:[{name:"Xt",docstring:"Must fulfill the input assumptions of the underlying estimator.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null}],outputs:[]},{name:"predict",docstring:"Call predict on the estimator with the best found parameters.\n\n        Only available if ``refit=True`` and the underlying estimator supports\n        ``predict``.\n\n        Parameters\n        ----------\n        X : indexable, length n_samples\n            Must fulfill the input assumptions of the\n            underlying estimator.\n\n        ",inputs:[{name:"X",docstring:"Must fulfill the input assumptions of the underlying estimator.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null}],outputs:[]},{name:"predict_log_proba",docstring:"Call predict_log_proba on the estimator with the best found parameters.\n\n        Only available if ``refit=True`` and the underlying estimator supports\n        ``predict_log_proba``.\n\n        Parameters\n        ----------\n        X : indexable, length n_samples\n            Must fulfill the input assumptions of the\n            underlying estimator.\n\n        ",inputs:[{name:"X",docstring:"Must fulfill the input assumptions of the underlying estimator.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null}],outputs:[]},{name:"predict_proba",docstring:"Call predict_proba on the estimator with the best found parameters.\n\n        Only available if ``refit=True`` and the underlying estimator supports\n        ``predict_proba``.\n\n        Parameters\n        ----------\n        X : indexable, length n_samples\n            Must fulfill the input assumptions of the\n            underlying estimator.\n\n        ",inputs:[{name:"X",docstring:"Must fulfill the input assumptions of the underlying estimator.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null}],outputs:[]},{name:"score",docstring:"Returns the score on the given data, if the estimator has been refit.\n\n        This uses the score defined by ``scoring`` where provided, and the\n        ``best_estimator_.score`` method otherwise.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Input data, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n            Target relative to X for classification or regression;\n            None for unsupervised learning.\n\n        Returns\n        -------\n        score : float\n        ",inputs:[{name:"X",docstring:"Input data, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null},{name:"y",docstring:"Target relative to X for classification or regression; None for unsupervised learning.",param_type:["array"],expected_shape:"[n_samples] or [n_samples, n_output], optional",is_optional:!0,default_value:null}],outputs:[{name:"score",docstring:"",param_type:["float"],returned:!0}]},{name:"set_params",docstring:"Set the parameters of this estimator.\n\n        The method works on simple estimators as well as on nested objects\n        (such as pipelines). The latter have parameters of the form\n        ``<component>__<parameter>`` so that it's possible to update each\n        component of a nested object.\n\n        Returns\n        -------\n        self\n        ",inputs:[],outputs:[{name:"",docstring:"",param_type:[null],returned:!0}]},{name:"transform",docstring:"Call transform on the estimator with the best found parameters.\n\n        Only available if the underlying estimator supports ``transform`` and\n        ``refit=True``.\n\n        Parameters\n        ----------\n        X : indexable, length n_samples\n            Must fulfill the input assumptions of the\n            underlying estimator.\n\n        ",inputs:[{name:"X",docstring:"Must fulfill the input assumptions of the underlying estimator.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null}],outputs:[]}],nodes:[]},{name:"RepeatedKFold",docstring:'Repeated K-Fold cross validator.\n\n    Repeats K-Fold n times with different randomization in each repetition.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n\n    n_repeats : int, default=10\n        Number of times cross-validator needs to be repeated.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import RepeatedKFold\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)\n    >>> for train_index, test_index in rkf.split(X):\n    ...     print("TRAIN:", train_index, "TEST:", test_index)\n    ...     X_train, X_test = X[train_index], X[test_index]\n    ...     y_train, y_test = y[train_index], y[test_index]\n    ...\n    TRAIN: [0 1] TEST: [2 3]\n    TRAIN: [2 3] TEST: [0 1]\n    TRAIN: [1 2] TEST: [0 3]\n    TRAIN: [0 3] TEST: [1 2]\n\n    Notes\n    -----\n    Randomized CV splitters may return different results for each call of\n    split. You can make the results identical by setting ``random_state``\n    to an integer.\n\n    See also\n    --------\n    RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.\n    ',inputs:[{name:"n_splits",docstring:"Number of folds. Must be at least 2.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"5"},{name:"n_repeats",docstring:"Number of times cross-validator needs to be repeated.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"10"},{name:"random_state",docstring:"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null}],outputs:[],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"get_n_splits",docstring:"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n            ``np.zeros(n_samples)`` may be used as a placeholder.\n\n        y : object\n            Always ignored, exists for compatibility.\n            ``np.zeros(n_samples)`` may be used as a placeholder.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        ",inputs:[{name:"X",docstring:"Always ignored, exists for compatibility. ``np.zeros(n_samples)`` may be used as a placeholder.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"y",docstring:"Always ignored, exists for compatibility. ``np.zeros(n_samples)`` may be used as a placeholder.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null}],outputs:[{name:"n_splits",docstring:"Returns the number of splitting iterations in the cross-validator.",param_type:["int"],returned:!0}]},{name:"split",docstring:"Generates indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, of length n_samples\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        ",inputs:[{name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"The target variable for supervised learning problems.",param_type:["array"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null}],outputs:[{name:"train",docstring:"The training set indices for that split.",param_type:["array"],returned:!1},{name:"test",docstring:"The testing set indices for that split.",param_type:["array"],returned:!1}]}],nodes:[]},{name:"RepeatedStratifiedKFold",docstring:'Repeated Stratified K-Fold cross validator.\n\n    Repeats Stratified K-Fold n times with different randomization in each\n    repetition.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n\n    n_repeats : int, default=10\n        Number of times cross-validator needs to be repeated.\n\n    random_state : None, int or RandomState, default=None\n        Random state to be used to generate random state for each\n        repetition.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import RepeatedStratifiedKFold\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,\n    ...     random_state=36851234)\n    >>> for train_index, test_index in rskf.split(X, y):\n    ...     print("TRAIN:", train_index, "TEST:", test_index)\n    ...     X_train, X_test = X[train_index], X[test_index]\n    ...     y_train, y_test = y[train_index], y[test_index]\n    ...\n    TRAIN: [1 2] TEST: [0 3]\n    TRAIN: [0 3] TEST: [1 2]\n    TRAIN: [1 3] TEST: [0 2]\n    TRAIN: [0 2] TEST: [1 3]\n\n    Notes\n    -----\n    Randomized CV splitters may return different results for each call of\n    split. You can make the results identical by setting ``random_state``\n    to an integer.\n\n    See also\n    --------\n    RepeatedKFold: Repeats K-Fold n times.\n    ',inputs:[{name:"n_splits",docstring:"Number of folds. Must be at least 2.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"5"},{name:"n_repeats",docstring:"Number of times cross-validator needs to be repeated.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"10"},{name:"random_state",docstring:"Random state to be used to generate random state for each repetition.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:"None"}],outputs:[],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"get_n_splits",docstring:"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n            ``np.zeros(n_samples)`` may be used as a placeholder.\n\n        y : object\n            Always ignored, exists for compatibility.\n            ``np.zeros(n_samples)`` may be used as a placeholder.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        ",inputs:[{name:"X",docstring:"Always ignored, exists for compatibility. ``np.zeros(n_samples)`` may be used as a placeholder.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"y",docstring:"Always ignored, exists for compatibility. ``np.zeros(n_samples)`` may be used as a placeholder.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null}],outputs:[{name:"n_splits",docstring:"Returns the number of splitting iterations in the cross-validator.",param_type:["int"],returned:!0}]},{name:"split",docstring:"Generates indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, of length n_samples\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        ",inputs:[{name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"The target variable for supervised learning problems.",param_type:["array"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null}],outputs:[{name:"train",docstring:"The training set indices for that split.",param_type:["array"],returned:!1},{name:"test",docstring:"The testing set indices for that split.",param_type:["array"],returned:!1}]}],nodes:[]},{name:"ShuffleSplit",docstring:'Random permutation cross-validator\n\n    Yields indices to split data into training and test sets.\n\n    Note: contrary to other cross-validation strategies, random splits\n    do not guarantee that all folds will be different, although this is\n    still very likely for sizeable datasets.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default 10\n        Number of re-shuffling & splitting iterations.\n\n    test_size : float, int, None, default=None\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to include in the test split. If int, represents the\n        absolute number of test samples. If None, the value is set to the\n        complement of the train size. If ``train_size`` is also None, it will\n        be set to 0.1.\n\n    train_size : float, int, or None, default=None\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the dataset to include in the train split. If\n        int, represents the absolute number of train samples. If None,\n        the value is automatically set to the complement of the test size.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import ShuffleSplit\n    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6]])\n    >>> y = np.array([1, 2, 1, 2, 1, 2])\n    >>> rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n    >>> rs.get_n_splits(X)\n    5\n    >>> print(rs)\n    ShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)\n    >>> for train_index, test_index in rs.split(X):\n    ...    print("TRAIN:", train_index, "TEST:", test_index)\n    ...  # doctest: +ELLIPSIS\n    TRAIN: [1 3 0 4] TEST: [5 2]\n    TRAIN: [4 0 2 5] TEST: [1 3]\n    TRAIN: [1 2 4 0] TEST: [3 5]\n    TRAIN: [3 4 1 0] TEST: [5 2]\n    TRAIN: [3 5 1 0] TEST: [2 4]\n    >>> rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25,\n    ...                   random_state=0)\n    >>> for train_index, test_index in rs.split(X):\n    ...    print("TRAIN:", train_index, "TEST:", test_index)\n    ...  # doctest: +ELLIPSIS\n    TRAIN: [1 3 0] TEST: [5 2]\n    TRAIN: [4 0 2] TEST: [1 3]\n    TRAIN: [1 2 4] TEST: [3 5]\n    TRAIN: [3 4 1] TEST: [5 2]\n    TRAIN: [3 5 1] TEST: [2 4]\n    ',inputs:[{name:"n_splits",docstring:"Number of re-shuffling & splitting iterations.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"10"},{name:"test_size",docstring:"If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If ``train_size`` is also None, it will be set to 0.1.",param_type:["int","float",null],expected_shape:null,is_optional:!0,default_value:"None"},{name:"train_size",docstring:"If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.",param_type:["int","float",null],expected_shape:null,is_optional:!0,default_value:"None"},{name:"random_state",docstring:"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null}],outputs:[],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_iter_indices",docstring:"None",inputs:[],outputs:[]},{name:"get_n_splits",docstring:"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        ",inputs:[{name:"X",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"y",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[{name:"n_splits",docstring:"Returns the number of splitting iterations in the cross-validator.",param_type:["int"],returned:!0}]},{name:"split",docstring:"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        ",inputs:[{name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"The target variable for supervised learning problems.",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null},{name:"groups",docstring:"Group labels for the samples used while splitting the dataset into train/test set.",param_type:["array"],expected_shape:"(n_samples,), optional",is_optional:!0,default_value:null}],outputs:[{name:"train",docstring:"The training set indices for that split.",param_type:["array"],returned:!1},{name:"test",docstring:"The testing set indices for that split.",param_type:["array"],returned:!1}]}],nodes:[]},{name:"StratifiedKFold",docstring:'Stratified K-Folds cross-validator\n\n    Provides train/test indices to split data in train/test sets.\n\n    This cross-validation object is a variation of KFold that returns\n    stratified folds. The folds are made by preserving the percentage of\n    samples for each class.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default=3\n        Number of folds. Must be at least 2.\n\n        .. versionchanged:: 0.20\n            ``n_splits`` default value will change from 3 to 5 in v0.22.\n\n    shuffle : boolean, optional\n        Whether to shuffle each class\'s samples before splitting into batches.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``shuffle`` == True.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import StratifiedKFold\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 1, 1])\n    >>> skf = StratifiedKFold(n_splits=2)\n    >>> skf.get_n_splits(X, y)\n    2\n    >>> print(skf)  # doctest: +NORMALIZE_WHITESPACE\n    StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n    >>> for train_index, test_index in skf.split(X, y):\n    ...    print("TRAIN:", train_index, "TEST:", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [1 3] TEST: [0 2]\n    TRAIN: [0 2] TEST: [1 3]\n\n    Notes\n    -----\n    Train and test sizes may be different in each fold, with a difference of at\n    most ``n_classes``.\n\n    See also\n    --------\n    RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.\n    ',inputs:[{name:"n_splits",docstring:"Number of folds. Must be at least 2.  .. versionchanged:: 0.20     ``n_splits`` default value will change from 3 to 5 in v0.22.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"3"},{name:"shuffle",docstring:"Whether to shuffle each class's samples before splitting into batches.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:null},{name:"random_state",docstring:"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. Used when ``shuffle`` == True.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null}],outputs:[],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_iter_test_indices",docstring:"Generates integer indices corresponding to test sets.",inputs:[],outputs:[]},{name:"_iter_test_masks",docstring:"None",inputs:[],outputs:[]},{name:"_make_test_folds",docstring:"None",inputs:[],outputs:[]},{name:"get_n_splits",docstring:"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        ",inputs:[{name:"X",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"y",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[{name:"n_splits",docstring:"Returns the number of splitting iterations in the cross-validator.",param_type:["int"],returned:!0}]},{name:"split",docstring:"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n            Note that providing ``y`` is sufficient to generate the splits and\n            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n            ``X`` instead of actual training data.\n\n        y : array-like, shape (n_samples,)\n            The target variable for supervised learning problems.\n            Stratification is done based on the y labels.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        ",inputs:[{name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.  Note that providing ``y`` is sufficient to generate the splits and hence ``np.zeros(n_samples)`` may be used as a placeholder for ``X`` instead of actual training data.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"The target variable for supervised learning problems. Stratification is done based on the y labels.",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null},{name:"groups",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[{name:"train",docstring:"The training set indices for that split.",param_type:["array"],returned:!1},{name:"test",docstring:"The testing set indices for that split.",param_type:["array"],returned:!1}]}],nodes:[]},{name:"StratifiedShuffleSplit",docstring:'Stratified ShuffleSplit cross-validator\n\n    Provides train/test indices to split data in train/test sets.\n\n    This cross-validation object is a merge of StratifiedKFold and\n    ShuffleSplit, which returns stratified randomized folds. The folds\n    are made by preserving the percentage of samples for each class.\n\n    Note: like the ShuffleSplit strategy, stratified random splits\n    do not guarantee that all folds will be different, although this is\n    still very likely for sizeable datasets.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default 10\n        Number of re-shuffling & splitting iterations.\n\n    test_size : float, int, None, optional (default=None)\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to include in the test split. If int, represents the\n        absolute number of test samples. If None, the value is set to the\n        complement of the train size. If ``train_size`` is also None, it will\n        be set to 0.1.\n\n    train_size : float, int, or None, default is None\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the dataset to include in the train split. If\n        int, represents the absolute number of train samples. If None,\n        the value is automatically set to the complement of the test size.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import StratifiedShuffleSplit\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([0, 0, 0, 1, 1, 1])\n    >>> sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n    >>> sss.get_n_splits(X, y)\n    5\n    >>> print(sss)       # doctest: +ELLIPSIS\n    StratifiedShuffleSplit(n_splits=5, random_state=0, ...)\n    >>> for train_index, test_index in sss.split(X, y):\n    ...    print("TRAIN:", train_index, "TEST:", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [5 2 3] TEST: [4 1 0]\n    TRAIN: [5 1 4] TEST: [0 2 3]\n    TRAIN: [5 0 2] TEST: [4 3 1]\n    TRAIN: [4 1 0] TEST: [2 3 5]\n    TRAIN: [0 5 1] TEST: [3 4 2]\n    ',inputs:[{name:"n_splits",docstring:"Number of re-shuffling & splitting iterations.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"10"},{name:"test_size",docstring:"If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If ``train_size`` is also None, it will be set to 0.1.",param_type:["int","float",null],expected_shape:null,is_optional:!0,default_value:null},{name:"train_size",docstring:"If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.",param_type:["int","float",null],expected_shape:null,is_optional:!0,default_value:"None"},{name:"random_state",docstring:"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:null}],outputs:[],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_iter_indices",docstring:"None",inputs:[],outputs:[]},{name:"get_n_splits",docstring:"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        ",inputs:[{name:"X",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"y",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[{name:"n_splits",docstring:"Returns the number of splitting iterations in the cross-validator.",param_type:["int"],returned:!0}]},{name:"split",docstring:"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n            Note that providing ``y`` is sufficient to generate the splits and\n            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n            ``X`` instead of actual training data.\n\n        y : array-like, shape (n_samples,)\n            The target variable for supervised learning problems.\n            Stratification is done based on the y labels.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        ",inputs:[{name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.  Note that providing ``y`` is sufficient to generate the splits and hence ``np.zeros(n_samples)`` may be used as a placeholder for ``X`` instead of actual training data.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"The target variable for supervised learning problems. Stratification is done based on the y labels.",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null},{name:"groups",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[{name:"train",docstring:"The training set indices for that split.",param_type:["array"],returned:!1},{name:"test",docstring:"The testing set indices for that split.",param_type:["array"],returned:!1}]}],nodes:[]},{name:"TimeSeriesSplit",docstring:'Time Series cross-validator\n\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals, in train/test sets.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    n_splits : int, default=3\n        Number of splits. Must be at least 2.\n\n        .. versionchanged:: 0.20\n            ``n_splits`` default value will change from 3 to 5 in v0.22.\n\n    max_train_size : int, optional\n        Maximum size for a single training set.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import TimeSeriesSplit\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([1, 2, 3, 4, 5, 6])\n    >>> tscv = TimeSeriesSplit(n_splits=5)\n    >>> print(tscv)  # doctest: +NORMALIZE_WHITESPACE\n    TimeSeriesSplit(max_train_size=None, n_splits=5)\n    >>> for train_index, test_index in tscv.split(X):\n    ...    print("TRAIN:", train_index, "TEST:", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [0] TEST: [1]\n    TRAIN: [0 1] TEST: [2]\n    TRAIN: [0 1 2] TEST: [3]\n    TRAIN: [0 1 2 3] TEST: [4]\n    TRAIN: [0 1 2 3 4] TEST: [5]\n\n    Notes\n    -----\n    The training set has size ``i * n_samples // (n_splits + 1)\n    + n_samples % (n_splits + 1)`` in the ``i``th split,\n    with a test set of size ``n_samples//(n_splits + 1)``,\n    where ``n_samples`` is the number of samples.\n    ',inputs:[{name:"n_splits",docstring:"Number of splits. Must be at least 2.  .. versionchanged:: 0.20     ``n_splits`` default value will change from 3 to 5 in v0.22.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"3"},{name:"max_train_size",docstring:"Maximum size for a single training set.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:null}],outputs:[],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_iter_test_indices",docstring:"Generates integer indices corresponding to test sets.",inputs:[],outputs:[]},{name:"_iter_test_masks",docstring:"Generates boolean masks corresponding to test sets.\n\n        By default, delegates to _iter_test_indices(X, y, groups)\n        ",inputs:[],outputs:[]},{name:"get_n_splits",docstring:"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        ",inputs:[{name:"X",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"y",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null},{name:"groups",docstring:"Always ignored, exists for compatibility.",param_type:["object"],expected_shape:null,is_optional:!1,default_value:null}],outputs:[{name:"n_splits",docstring:"Returns the number of splitting iterations in the cross-validator.",param_type:["int"],returned:!0}]},{name:"split",docstring:"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            Always ignored, exists for compatibility.\n\n        groups : array-like, with shape (n_samples,)\n            Always ignored, exists for compatibility.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        ",inputs:[{name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null},{name:"y",docstring:"Always ignored, exists for compatibility.",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null},{name:"groups",docstring:"Always ignored, exists for compatibility.",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null}],outputs:[{name:"train",docstring:"The training set indices for that split.",param_type:["array"],returned:!1},{name:"test",docstring:"The testing set indices for that split.",param_type:["array"],returned:!1}]}],nodes:[]}],library:"sklearn",module:"model_selection"}},function(e,t,n){"use strict";t.a={inputs:[],module:"decomposition",library:"sklearn",name:"sklearn.decomposition",nodes:[{inputs:[{is_optional:!1,param_type:["int",null],name:"n_components",docstring:"Dimensionality of latent space, the number of components of ``X`` that are obtained after ``transform``. If None, n_components is set to the number of features.",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["float"],name:"tol",docstring:"Stopping tolerance for EM algorithm.",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["bool"],name:"copy",docstring:"Whether to make a copy of X. If ``False``, the input X gets overwritten during fitting.",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["int"],name:"max_iter",docstring:"Maximum number of iterations.",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["LIST_VALID_OPTIONS","array",null],name:"noise_variance_init",docstring:"The initial guess of the noise variance for each feature. If None, it defaults to np.ones(n_features)",options:["None "," array, shape=(n_features,)"],default_value:null,expected_shape:"(n_features,)"},{is_optional:!1,param_type:["LIST_VALID_OPTIONS"],name:"svd_method",docstring:"Which SVD method to use. If 'lapack' use standard SVD from scipy.linalg, if 'randomized' use fast ``randomized_svd`` function. Defaults to 'randomized'. For most applications 'randomized' will be sufficiently precise while providing significant speed gains. Accuracy can also be improved by setting higher values for `iterated_power`. If this is not sufficient, for maximum precision you should choose 'lapack'.",options:["lapack","randomized"],default_value:null,expected_shape:null},{is_optional:!0,param_type:["int"],name:"iterated_power",docstring:"Number of iterations for the power method. 3 by default. Only used if ``svd_method`` equals 'randomized'",options:null,default_value:"3",expected_shape:null}],name:"FactorAnalysis",nodes:[],node_functions:[{inputs:[],outputs:[],name:"__init__",docstring:"None"},{inputs:[],outputs:[],name:"_get_param_names",docstring:"Get parameter names for the estimator"},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"Training data.",options:null,default_value:null,expected_shape:"(n_samples, n_features)"},{is_optional:!1,param_type:[null],name:"y",docstring:"",options:null,default_value:null,expected_shape:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"fit",docstring:"Fit the FactorAnalysis model to X using EM\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n\n        Returns\n        -------\n        self\n        "},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"Training set.",options:null,default_value:null,expected_shape:"[n_samples, n_features]"},{is_optional:!1,param_type:["array"],name:"y",docstring:"Target values.",options:null,default_value:null,expected_shape:"[n_samples]"},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["array"],name:"X_new",docstring:"Transformed array."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"fit_transform",docstring:"Fit to data, then transform it.\n\n        Fits transformer to X and y with optional parameters fit_params\n        and returns a transformed version of X.\n\n        Parameters\n        ----------\n        X : numpy array of shape [n_samples, n_features]\n            Training set.\n\n        y : numpy array of shape [n_samples]\n            Target values.\n\n        Returns\n        -------\n        X_new : numpy array of shape [n_samples, n_features_new]\n            Transformed array.\n\n        "},{inputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["array"],name:"cov",docstring:"Estimated covariance of data."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"get_covariance",docstring:"Compute data covariance with the FactorAnalysis model.\n\n        ``cov = components_.T * components_ + diag(noise_variance)``\n\n        Returns\n        -------\n        cov : array, shape (n_features, n_features)\n            Estimated covariance of data.\n        "},{inputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["array"],name:"precision",docstring:"Estimated precision of data."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"get_precision",docstring:"Compute data precision matrix with the FactorAnalysis model.\n\n        Returns\n        -------\n        precision : array, shape (n_features, n_features)\n            Estimated precision of data.\n        "},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"The data",options:null,default_value:null,expected_shape:"(n_samples, n_features)"},{is_optional:!0,param_type:[null],name:"y",docstring:"",options:null,default_value:null,expected_shape:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["float"],name:"ll",docstring:"Average log-likelihood of the samples under the current model"},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"score",docstring:"Compute the average log-likelihood of the samples\n\n        Parameters\n        ----------\n        X : array, shape (n_samples, n_features)\n            The data\n\n        y : Ignored\n\n        Returns\n        -------\n        ll : float\n            Average log-likelihood of the samples under the current model\n        "},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"The data",options:null,default_value:null,expected_shape:"(n_samples, n_features)"},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["array"],name:"ll",docstring:"Log-likelihood of each sample under the current model"},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"score_samples",docstring:"Compute the log-likelihood of each sample\n\n        Parameters\n        ----------\n        X : array, shape (n_samples, n_features)\n            The data\n\n        Returns\n        -------\n        ll : array, shape (n_samples,)\n            Log-likelihood of each sample under the current model\n        "},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"Training data.",options:null,default_value:null,expected_shape:"(n_samples, n_features)"},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["array"],name:"X_new",docstring:"The latent variables of X."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"transform",docstring:"Apply dimensionality reduction to X using the model.\n\n        Compute the expected mean of the latent variables.\n        See Barber, 21.2.33 (or Bishop, 12.66).\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n            The latent variables of X.\n        "}],docstring:"Factor Analysis (FA)\n\n    A simple linear generative model with Gaussian latent variables.\n\n    The observations are assumed to be caused by a linear transformation of\n    lower dimensional latent factors and added Gaussian noise.\n    Without loss of generality the factors are distributed according to a\n    Gaussian with zero mean and unit covariance. The noise is also zero mean\n    and has an arbitrary diagonal covariance matrix.\n\n    If we would restrict the model further, by assuming that the Gaussian\n    noise is even isotropic (all diagonal entries are the same) we would obtain\n    :class:`PPCA`.\n\n    FactorAnalysis performs a maximum likelihood estimate of the so-called\n    `loading` matrix, the transformation of the latent variables to the\n    observed ones, using expectation-maximization (EM).\n\n    Read more in the :ref:`User Guide <FA>`.\n\n    Parameters\n    ----------\n    n_components : int | None\n        Dimensionality of latent space, the number of components\n        of ``X`` that are obtained after ``transform``.\n        If None, n_components is set to the number of features.\n\n    tol : float\n        Stopping tolerance for EM algorithm.\n\n    copy : bool\n        Whether to make a copy of X. If ``False``, the input X gets overwritten\n        during fitting.\n\n    max_iter : int\n        Maximum number of iterations.\n\n    noise_variance_init : None | array, shape=(n_features,)\n        The initial guess of the noise variance for each feature.\n        If None, it defaults to np.ones(n_features)\n\n    svd_method : {'lapack', 'randomized'}\n        Which SVD method to use. If 'lapack' use standard SVD from\n        scipy.linalg, if 'randomized' use fast ``randomized_svd`` function.\n        Defaults to 'randomized'. For most applications 'randomized' will\n        be sufficiently precise while providing significant speed gains.\n        Accuracy can also be improved by setting higher values for\n        `iterated_power`. If this is not sufficient, for maximum precision\n        you should choose 'lapack'.\n\n    iterated_power : int, optional\n        Number of iterations for the power method. 3 by default. Only used\n        if ``svd_method`` equals 'randomized'\n\n    random_state : int, RandomState instance or None, optional (default=0)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Only used when ``svd_method`` equals 'randomized'.\n\n    Attributes\n    ----------\n    components_ : array, [n_components, n_features]\n        Components with maximum variance.\n\n    loglike_ : list, [n_iterations]\n        The log likelihood at each iteration.\n\n    noise_variance_ : array, shape=(n_features,)\n        The estimated noise variance for each feature.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import FactorAnalysis\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = FactorAnalysis(n_components=7, random_state=0)\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n\n    References\n    ----------\n    .. David Barber, Bayesian Reasoning and Machine Learning,\n        Algorithm 21.1\n\n    .. Christopher M. Bishop: Pattern Recognition and Machine Learning,\n        Chapter 12.2.4\n\n    See also\n    --------\n    PCA: Principal component analysis is also a latent linear variable model\n        which however assumes equal noise variance for each feature.\n        This extra assumption makes probabilistic PCA faster as it can be\n        computed in closed form.\n    FastICA: Independent component analysis, a latent variable model with\n        non-Gaussian latent variables.\n    ",outputs:[{returned:!1,param_type:["array"],name:"components_",docstring:"Components with maximum variance."},{returned:!1,param_type:["list"],name:"loglike_",docstring:"The log likelihood at each iteration."},{returned:!1,param_type:["array"],name:"noise_variance_",docstring:"The estimated noise variance for each feature."},{returned:!1,param_type:["int"],name:"n_iter_",docstring:"Number of iterations run."}]},{inputs:[{is_optional:!1,param_type:["int","float","str",null],name:"n_components",docstring:"Number of components to keep. if n_components is not set all components are kept::      n_components == min(n_samples, n_features)  If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's MLE is used to guess the dimension. Use of ``n_components == 'mle'`` will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.  If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components.  If ``svd_solver == 'arpack'``, the number of components must be strictly less than the minimum of n_features and n_samples.  Hence, the None case results in::      n_components == min(n_samples, n_features) - 1",options:null,default_value:null,expected_shape:null},{is_optional:!0,param_type:["bool"],name:"copy",docstring:"If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead.",options:null,default_value:"True",expected_shape:null},{is_optional:!0,param_type:["bool"],name:"whiten",docstring:"When True (False by default) the `components_` vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances.  Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions.",options:null,default_value:"False",expected_shape:null},{is_optional:!1,param_type:["LIST_VALID_OPTIONS"],name:"svd_solver",docstring:"auto :     the solver is selected by a default policy based on `X.shape` and     `n_components`: if the input data is larger than 500x500 and the     number of components to extract is lower than 80% of the smallest     dimension of the data, then the more efficient 'randomized'     method is enabled. Otherwise the exact full SVD is computed and     optionally truncated afterwards. full :     run exact full SVD calling the standard LAPACK solver via     `scipy.linalg.svd` and select the components by postprocessing arpack :     run SVD truncated to n_components calling ARPACK solver via     `scipy.sparse.linalg.svds`. It requires strictly     0 < n_components < min(X.shape) randomized :     run randomized SVD by the method of Halko et al.  .. versionadded:: 0.18.0",options:["auto","full","arpack","randomized"],default_value:null,expected_shape:null},{is_optional:!0,param_type:["float"],name:"tol",docstring:"Tolerance for singular values computed by svd_solver == 'arpack'.  .. versionadded:: 0.18.0",options:null,default_value:".0",expected_shape:null},{is_optional:!0,param_type:["int"],name:"iterated_power",docstring:"Number of iterations for the power method computed by svd_solver == 'randomized'.  .. versionadded:: 0.18.0",options:null,default_value:"auto",expected_shape:null}],name:"PCA",nodes:[],node_functions:[{inputs:[],outputs:[],name:"__init__",docstring:"None"},{inputs:[],outputs:[],name:"_fit",docstring:"Dispatch to the right submethod depending on the chosen solver."},{inputs:[],outputs:[],name:"_fit_full",docstring:"Fit the model by computing full SVD on X"},{inputs:[],outputs:[],name:"_fit_truncated",docstring:"Fit the model by computing truncated SVD (by ARPACK or randomized)\n        on X\n        "},{inputs:[],outputs:[],name:"_get_param_names",docstring:"Get parameter names for the estimator"},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.",options:null,default_value:null,expected_shape:"(n_samples, n_features)"},{is_optional:!0,param_type:[null],name:"y",docstring:"Ignored",options:null,default_value:null,expected_shape:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"fit",docstring:"Fit the model with X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        "},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"Training data, where n_samples is the number of samples and n_features is the number of features.",options:null,default_value:null,expected_shape:"(n_samples, n_features)"},{is_optional:!0,param_type:[null],name:"y",docstring:"Ignored",options:null,default_value:null,expected_shape:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["array"],name:"X_new",docstring:""},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"fit_transform",docstring:"Fit the model with X and apply the dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : Ignored\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n\n        "},{inputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["array"],name:"cov",docstring:"Estimated covariance of data."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"get_covariance",docstring:"Compute data covariance with the generative model.\n\n        ``cov = components_.T * S**2 * components_ + sigma2 * eye(n_features)``\n        where  S**2 contains the explained variances, and sigma2 contains the\n        noise variances.\n\n        Returns\n        -------\n        cov : array, shape=(n_features, n_features)\n            Estimated covariance of data.\n        "},{inputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["array"],name:"precision",docstring:"Estimated precision of data."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"get_precision",docstring:"Compute data precision matrix with the generative model.\n\n        Equals the inverse of the covariance but computed with\n        the matrix inversion lemma for efficiency.\n\n        Returns\n        -------\n        precision : array, shape=(n_features, n_features)\n            Estimated precision of data.\n        "},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"New data, where n_samples is the number of samples and n_components is the number of components.",options:null,default_value:null,expected_shape:"(n_samples, n_components)"},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:[null],name:"X_original array-like, shape (n_samples, n_features)",docstring:""},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"inverse_transform",docstring:"Transform data back to its original space.\n\n        In other words, return an input X_original whose transform would be X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_components)\n            New data, where n_samples is the number of samples\n            and n_components is the number of components.\n\n        Returns\n        -------\n        X_original array-like, shape (n_samples, n_features)\n\n        Notes\n        -----\n        If whitening is enabled, inverse_transform will compute the\n        exact inverse operation, which includes reversing whitening.\n        "},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"The data.",options:null,default_value:null,expected_shape:"(n_samples, n_features)"},{is_optional:!0,param_type:[null],name:"y",docstring:"Ignored",options:null,default_value:null,expected_shape:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["float"],name:"ll",docstring:"Average log-likelihood of the samples under the current model"},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"score",docstring:'Return the average log-likelihood of all samples.\n\n        See. "Pattern Recognition and Machine Learning"\n        by C. Bishop, 12.2.1 p. 574\n        or http://www.miketipping.com/papers/met-mppca.pdf\n\n        Parameters\n        ----------\n        X : array, shape(n_samples, n_features)\n            The data.\n\n        y : Ignored\n\n        Returns\n        -------\n        ll : float\n            Average log-likelihood of the samples under the current model\n        '},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"The data.",options:null,default_value:null,expected_shape:"(n_samples, n_features)"},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["array"],name:"ll",docstring:"Log-likelihood of each sample under the current model"},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"score_samples",docstring:'Return the log-likelihood of each sample.\n\n        See. "Pattern Recognition and Machine Learning"\n        by C. Bishop, 12.2.1 p. 574\n        or http://www.miketipping.com/papers/met-mppca.pdf\n\n        Parameters\n        ----------\n        X : array, shape(n_samples, n_features)\n            The data.\n\n        Returns\n        -------\n        ll : array, shape (n_samples,)\n            Log-likelihood of each sample under the current model\n        '},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"New data, where n_samples is the number of samples and n_features is the number of features.",options:null,default_value:null,expected_shape:"(n_samples, n_features)"},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["array"],name:"X_new",docstring:""},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"transform",docstring:"Apply dimensionality reduction to X.\n\n        X is projected on the first principal components previously extracted\n        from a training set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            New data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n\n        Examples\n        --------\n\n        >>> import numpy as np\n        >>> from sklearn.decomposition import IncrementalPCA\n        >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n        >>> ipca = IncrementalPCA(n_components=2, batch_size=3)\n        >>> ipca.fit(X)\n        IncrementalPCA(batch_size=3, copy=True, n_components=2, whiten=False)\n        >>> ipca.transform(X) # doctest: +SKIP\n        "}],docstring:"Principal component analysis (PCA)\n\n    Linear dimensionality reduction using Singular Value Decomposition of the\n    data to project it to a lower dimensional space.\n\n    It uses the LAPACK implementation of the full SVD or a randomized truncated\n    SVD by the method of Halko et al. 2009, depending on the shape of the input\n    data and the number of components to extract.\n\n    It can also use the scipy.sparse.linalg ARPACK implementation of the\n    truncated SVD.\n\n    Notice that this class does not support sparse input. See\n    :class:`TruncatedSVD` for an alternative with sparse data.\n\n    Read more in the :ref:`User Guide <PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, float, None or string\n        Number of components to keep.\n        if n_components is not set all components are kept::\n\n            n_components == min(n_samples, n_features)\n\n        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n        MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n        will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n\n        If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n        number of components such that the amount of variance that needs to be\n        explained is greater than the percentage specified by n_components.\n\n        If ``svd_solver == 'arpack'``, the number of components must be\n        strictly less than the minimum of n_features and n_samples.\n\n        Hence, the None case results in::\n\n            n_components == min(n_samples, n_features) - 1\n\n    copy : bool (default True)\n        If False, data passed to fit are overwritten and running\n        fit(X).transform(X) will not yield the expected results,\n        use fit_transform(X) instead.\n\n    whiten : bool, optional (default False)\n        When True (False by default) the `components_` vectors are multiplied\n        by the square root of n_samples and then divided by the singular values\n        to ensure uncorrelated outputs with unit component-wise variances.\n\n        Whitening will remove some information from the transformed signal\n        (the relative variance scales of the components) but can sometime\n        improve the predictive accuracy of the downstream estimators by\n        making their data respect some hard-wired assumptions.\n\n    svd_solver : string {'auto', 'full', 'arpack', 'randomized'}\n        auto :\n            the solver is selected by a default policy based on `X.shape` and\n            `n_components`: if the input data is larger than 500x500 and the\n            number of components to extract is lower than 80% of the smallest\n            dimension of the data, then the more efficient 'randomized'\n            method is enabled. Otherwise the exact full SVD is computed and\n            optionally truncated afterwards.\n        full :\n            run exact full SVD calling the standard LAPACK solver via\n            `scipy.linalg.svd` and select the components by postprocessing\n        arpack :\n            run SVD truncated to n_components calling ARPACK solver via\n            `scipy.sparse.linalg.svds`. It requires strictly\n            0 < n_components < min(X.shape)\n        randomized :\n            run randomized SVD by the method of Halko et al.\n\n        .. versionadded:: 0.18.0\n\n    tol : float >= 0, optional (default .0)\n        Tolerance for singular values computed by svd_solver == 'arpack'.\n\n        .. versionadded:: 0.18.0\n\n    iterated_power : int >= 0, or 'auto', (default 'auto')\n        Number of iterations for the power method computed by\n        svd_solver == 'randomized'.\n\n        .. versionadded:: 0.18.0\n\n    random_state : int, RandomState instance or None, optional (default None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``svd_solver`` == 'arpack' or 'randomized'.\n\n        .. versionadded:: 0.18.0\n\n    Attributes\n    ----------\n    components_ : array, shape (n_components, n_features)\n        Principal axes in feature space, representing the directions of\n        maximum variance in the data. The components are sorted by\n        ``explained_variance_``.\n\n    explained_variance_ : array, shape (n_components,)\n        The amount of variance explained by each of the selected components.\n\n        Equal to n_components largest eigenvalues\n        of the covariance matrix of X.\n\n        .. versionadded:: 0.18\n\n    explained_variance_ratio_ : array, shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n        If ``n_components`` is not set then all components are stored and the\n        sum of the ratios is equal to 1.0.\n\n    singular_values_ : array, shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n    mean_ : array, shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n\n        Equal to `X.mean(axis=0)`.\n\n    n_components_ : int\n        The estimated number of components. When n_components is set\n        to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n        number is estimated from input data. Otherwise it equals the parameter\n        n_components, or the lesser value of n_features and n_samples\n        if n_components is None.\n\n    noise_variance_ : float\n        The estimated noise covariance following the Probabilistic PCA model\n        from Tipping and Bishop 1999. See \"Pattern Recognition and\n        Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n        http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n        compute the estimated data covariance and score samples.\n\n        Equal to the average of (min(n_features, n_samples) - n_components)\n        smallest eigenvalues of the covariance matrix of X.\n\n    References\n    ----------\n    For n_components == 'mle', this class uses the method of `Minka, T. P.\n    \"Automatic choice of dimensionality for PCA\". In NIPS, pp. 598-604`\n\n    Implements the probabilistic PCA model from:\n    `Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\n    component analysis\". Journal of the Royal Statistical Society:\n    Series B (Statistical Methodology), 61(3), 611-622.\n    via the score and score_samples methods.\n    See http://www.miketipping.com/papers/met-mppca.pdf\n\n    For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n\n    For svd_solver == 'randomized', see:\n    `Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n    \"Finding structure with randomness: Probabilistic algorithms for\n    constructing approximate matrix decompositions\".\n    SIAM review, 53(2), 217-288.` and also\n    `Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n    \"A randomized algorithm for the decomposition of matrices\".\n    Applied and Computational Harmonic Analysis, 30(1), 47-68.`\n\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import PCA\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> pca = PCA(n_components=2)\n    >>> pca.fit(X)\n    PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n      svd_solver='auto', tol=0.0, whiten=False)\n    >>> print(pca.explained_variance_ratio_)  # doctest: +ELLIPSIS\n    [0.9924... 0.0075...]\n    >>> print(pca.singular_values_)  # doctest: +ELLIPSIS\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=2, svd_solver='full')\n    >>> pca.fit(X)                 # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n    PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n      svd_solver='full', tol=0.0, whiten=False)\n    >>> print(pca.explained_variance_ratio_)  # doctest: +ELLIPSIS\n    [0.9924... 0.00755...]\n    >>> print(pca.singular_values_)  # doctest: +ELLIPSIS\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=1, svd_solver='arpack')\n    >>> pca.fit(X)\n    PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,\n      svd_solver='arpack', tol=0.0, whiten=False)\n    >>> print(pca.explained_variance_ratio_)  # doctest: +ELLIPSIS\n    [0.99244...]\n    >>> print(pca.singular_values_)  # doctest: +ELLIPSIS\n    [6.30061...]\n\n    See also\n    --------\n    KernelPCA\n    SparsePCA\n    TruncatedSVD\n    IncrementalPCA\n    ",outputs:[{returned:!1,param_type:["array"],name:"components_",docstring:"Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by ``explained_variance_``."},{returned:!1,param_type:["array"],name:"explained_variance_",docstring:"The amount of variance explained by each of the selected components.  Equal to n_components largest eigenvalues of the covariance matrix of X.  .. versionadded:: 0.18"},{returned:!1,param_type:["array"],name:"explained_variance_ratio_",docstring:"Percentage of variance explained by each of the selected components.  If ``n_components`` is not set then all components are stored and the sum of the ratios is equal to 1.0."},{returned:!1,param_type:["array"],name:"singular_values_",docstring:"The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the ``n_components`` variables in the lower-dimensional space."},{returned:!1,param_type:["array"],name:"mean_",docstring:"Per-feature empirical mean, estimated from the training set.  Equal to `X.mean(axis=0)`."},{returned:!1,param_type:["int"],name:"n_components_",docstring:"The estimated number of components. When n_components is set to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this number is estimated from input data. Otherwise it equals the parameter n_components, or the lesser value of n_features and n_samples if n_components is None."},{returned:!1,param_type:["float"],name:"noise_variance_",docstring:'The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See "Pattern Recognition and Machine Learning" by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf. It is required to compute the estimated data covariance and score samples.  Equal to the average of (min(n_features, n_samples) - n_components) smallest eigenvalues of the covariance matrix of X.'}]},{inputs:[{is_optional:!0,param_type:["int"],name:"n_components",docstring:"Desired dimensionality of output data. Must be strictly less than the number of features. The default value is useful for visualisation. For LSA, a value of 100 is recommended.",options:null,default_value:"2",expected_shape:null},{is_optional:!0,param_type:["str"],name:"algorithm",docstring:'SVD solver to use. Either "arpack" for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or "randomized" for the randomized algorithm due to Halko (2009).',options:null,default_value:'"randomized"',expected_shape:null},{is_optional:!0,param_type:["int"],name:"n_iter",docstring:"Number of iterations for randomized SVD solver. Not used by ARPACK. The default is larger than the default in `randomized_svd` to handle sparse matrices that may have large slowly decaying spectrum.",options:null,default_value:"5",expected_shape:null},{is_optional:!0,param_type:["float"],name:"tol",docstring:"Tolerance for ARPACK. 0 means machine precision. Ignored by randomized SVD solver.",options:null,default_value:null,expected_shape:null}],name:"TruncatedSVD",nodes:[],node_functions:[{inputs:[],outputs:[],name:"__init__",docstring:"None"},{inputs:[],outputs:[],name:"_get_param_names",docstring:"Get parameter names for the estimator"},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"Training data.",options:null,default_value:null,expected_shape:"(n_samples, n_features)"},{is_optional:!0,param_type:[null],name:"y",docstring:"Ignored",options:null,default_value:null,expected_shape:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"fit",docstring:"Fit LSI model on training data X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n\n        Returns\n        -------\n        self : object\n            Returns the transformer object.\n        "},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"Training data.",options:null,default_value:null,expected_shape:"(n_samples, n_features)"},{is_optional:!0,param_type:[null],name:"y",docstring:"Ignored",options:null,default_value:null,expected_shape:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["array"],name:"X_new",docstring:"Reduced version of X. This will always be a dense array."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"fit_transform",docstring:"Fit LSI model to X and perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data.\n\n        y : Ignored\n\n        Returns\n        -------\n        X_new : array, shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array.\n        "},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"New data.",options:null,default_value:null,expected_shape:"(n_samples, n_components)"},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["array"],name:"X_original",docstring:"Note that this is always a dense array."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"inverse_transform",docstring:"Transform X back to its original space.\n\n        Returns an array X_original whose transform would be X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_components)\n            New data.\n\n        Returns\n        -------\n        X_original : array, shape (n_samples, n_features)\n            Note that this is always a dense array.\n        "},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"New data.",options:null,default_value:null,expected_shape:"(n_samples, n_features)"},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["array"],name:"X_new",docstring:"Reduced version of X. This will always be a dense array."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"transform",docstring:"Perform dimensionality reduction on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            New data.\n\n        Returns\n        -------\n        X_new : array, shape (n_samples, n_components)\n            Reduced version of X. This will always be a dense array.\n        "}],docstring:'Dimensionality reduction using truncated SVD (aka LSA).\n\n    This transformer performs linear dimensionality reduction by means of\n    truncated singular value decomposition (SVD). Contrary to PCA, this\n    estimator does not center the data before computing the singular value\n    decomposition. This means it can work with scipy.sparse matrices\n    efficiently.\n\n    In particular, truncated SVD works on term count/tf-idf matrices as\n    returned by the vectorizers in sklearn.feature_extraction.text. In that\n    context, it is known as latent semantic analysis (LSA).\n\n    This estimator supports two algorithms: a fast randomized SVD solver, and\n    a "naive" algorithm that uses ARPACK as an eigensolver on (X * X.T) or\n    (X.T * X), whichever is more efficient.\n\n    Read more in the :ref:`User Guide <LSA>`.\n\n    Parameters\n    ----------\n    n_components : int, default = 2\n        Desired dimensionality of output data.\n        Must be strictly less than the number of features.\n        The default value is useful for visualisation. For LSA, a value of\n        100 is recommended.\n\n    algorithm : string, default = "randomized"\n        SVD solver to use. Either "arpack" for the ARPACK wrapper in SciPy\n        (scipy.sparse.linalg.svds), or "randomized" for the randomized\n        algorithm due to Halko (2009).\n\n    n_iter : int, optional (default 5)\n        Number of iterations for randomized SVD solver. Not used by ARPACK.\n        The default is larger than the default in `randomized_svd` to handle\n        sparse matrices that may have large slowly decaying spectrum.\n\n    random_state : int, RandomState instance or None, optional, default = None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    tol : float, optional\n        Tolerance for ARPACK. 0 means machine precision. Ignored by randomized\n        SVD solver.\n\n    Attributes\n    ----------\n    components_ : array, shape (n_components, n_features)\n\n    explained_variance_ : array, shape (n_components,)\n        The variance of the training samples transformed by a projection to\n        each component.\n\n    explained_variance_ratio_ : array, shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n    singular_values_ : array, shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n    Examples\n    --------\n    >>> from sklearn.decomposition import TruncatedSVD\n    >>> from sklearn.random_projection import sparse_random_matrix\n    >>> X = sparse_random_matrix(100, 100, density=0.01, random_state=42)\n    >>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> svd.fit(X)  # doctest: +NORMALIZE_WHITESPACE\n    TruncatedSVD(algorithm=\'randomized\', n_components=5, n_iter=7,\n            random_state=42, tol=0.0)\n    >>> print(svd.explained_variance_ratio_)  # doctest: +ELLIPSIS\n    [0.0606... 0.0584... 0.0497... 0.0434... 0.0372...]\n    >>> print(svd.explained_variance_ratio_.sum())  # doctest: +ELLIPSIS\n    0.249...\n    >>> print(svd.singular_values_)  # doctest: +ELLIPSIS\n    [2.5841... 2.5245... 2.3201... 2.1753... 2.0443...]\n\n    See also\n    --------\n    PCA\n\n    References\n    ----------\n    Finding structure with randomness: Stochastic algorithms for constructing\n    approximate matrix decompositions\n    Halko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf\n\n    Notes\n    -----\n    SVD suffers from a problem called "sign indeterminacy", which means the\n    sign of the ``components_`` and the output from transform depend on the\n    algorithm and random state. To work around this, fit instances of this\n    class to data once, then keep the instance around to do transformations.\n\n    ',outputs:[{returned:!1,param_type:["array"],name:"components_",docstring:""},{returned:!1,param_type:["array"],name:"explained_variance_",docstring:"The variance of the training samples transformed by a projection to each component."},{returned:!1,param_type:["array"],name:"explained_variance_ratio_",docstring:"Percentage of variance explained by each of the selected components."},{returned:!1,param_type:["array"],name:"singular_values_",docstring:"The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the ``n_components`` variables in the lower-dimensional space."}]}],node_functions:[{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"Data matrix.",options:null,default_value:null,expected_shape:"(n_samples, n_features)"},{is_optional:!1,param_type:["int"],name:"n_components",docstring:"Number of dictionary atoms to extract.",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["int"],name:"alpha",docstring:"Sparsity controlling parameter.",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["int"],name:"max_iter",docstring:"Maximum number of iterations to perform.",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["float"],name:"tol",docstring:"Tolerance for the stopping condition.",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["LIST_VALID_OPTIONS"],name:"method",docstring:"lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse.",options:["lars","cd"],default_value:null,expected_shape:null},{is_optional:!0,param_type:["int",null],name:"n_jobs",docstring:"Number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",options:null,default_value:"None",expected_shape:null},{is_optional:!1,param_type:["array"],name:"dict_init",docstring:"Initial value for the dictionary for warm restart scenarios.",options:null,default_value:null,expected_shape:"(n_components, n_features),"},{is_optional:!1,param_type:["array"],name:"code_init",docstring:"Initial value for the sparse code for warm restart scenarios.",options:null,default_value:null,expected_shape:"(n_samples, n_components),"},{is_optional:!0,param_type:["callable",null],name:"callback",docstring:"Callable that gets invoked every five iterations",options:null,default_value:"None",expected_shape:null},{is_optional:!0,param_type:["bool"],name:"verbose",docstring:"To control the verbosity of the procedure.",options:null,default_value:"False",expected_shape:null},{is_optional:!0,param_type:["int",null],name:"random_state",docstring:"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`.",options:null,default_value:"None",expected_shape:null},{is_optional:!1,param_type:["bool"],name:"return_n_iter",docstring:"Whether or not to return the number of iterations.",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["bool"],name:"positive_dict",docstring:"Whether to enforce positivity when finding the dictionary.  .. versionadded:: 0.20",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["bool"],name:"positive_code",docstring:"Whether to enforce positivity when finding the code.  .. versionadded:: 0.20",options:null,default_value:null,expected_shape:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["array"],name:"code",docstring:"The sparse code factor in the matrix factorization."},{returned:!0,param_type:["array"],name:"dictionary",docstring:"The dictionary factor in the matrix factorization."},{returned:!0,param_type:["array"],name:"errors",docstring:"Vector of errors at each iteration."},{returned:!0,param_type:["int"],name:"n_iter",docstring:"Number of iterations run. Returned only if `return_n_iter` is set to True."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"dict_learning",docstring:"Solves a dictionary learning matrix factorization problem.\n\n    Finds the best dictionary and the corresponding sparse code for\n    approximating the data matrix X by solving::\n\n        (U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                     (U,V)\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\n\n    where V is the dictionary and U is the sparse code.\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    X : array of shape (n_samples, n_features)\n        Data matrix.\n\n    n_components : int,\n        Number of dictionary atoms to extract.\n\n    alpha : int,\n        Sparsity controlling parameter.\n\n    max_iter : int,\n        Maximum number of iterations to perform.\n\n    tol : float,\n        Tolerance for the stopping condition.\n\n    method : {'lars', 'cd'}\n        lars: uses the least angle regression method to solve the lasso problem\n        (linear_model.lars_path)\n        cd: uses the coordinate descent method to compute the\n        Lasso solution (linear_model.Lasso). Lars will be faster if\n        the estimated components are sparse.\n\n    n_jobs : int or None, optional (default=None)\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    dict_init : array of shape (n_components, n_features),\n        Initial value for the dictionary for warm restart scenarios.\n\n    code_init : array of shape (n_samples, n_components),\n        Initial value for the sparse code for warm restart scenarios.\n\n    callback : callable or None, optional (default: None)\n        Callable that gets invoked every five iterations\n\n    verbose : bool, optional (default: False)\n        To control the verbosity of the procedure.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    return_n_iter : bool\n        Whether or not to return the number of iterations.\n\n    positive_dict : bool\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    positive_code : bool\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    code : array of shape (n_samples, n_components)\n        The sparse code factor in the matrix factorization.\n\n    dictionary : array of shape (n_components, n_features),\n        The dictionary factor in the matrix factorization.\n\n    errors : array\n        Vector of errors at each iteration.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is\n        set to True.\n\n    See also\n    --------\n    dict_learning_online\n    DictionaryLearning\n    MiniBatchDictionaryLearning\n    SparsePCA\n    MiniBatchSparsePCA\n    "},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"Data matrix.",options:null,default_value:null,expected_shape:"(n_samples, n_features)"},{is_optional:!1,param_type:["int"],name:"n_components",docstring:"Number of dictionary atoms to extract.",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["float"],name:"alpha",docstring:"Sparsity controlling parameter.",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["int"],name:"n_iter",docstring:"Number of iterations to perform.",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["bool"],name:"return_code",docstring:"Whether to also return the code U or just the dictionary V.",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["array"],name:"dict_init",docstring:"Initial value for the dictionary for warm restart scenarios.",options:null,default_value:null,expected_shape:"(n_components, n_features),"},{is_optional:!0,param_type:["callable",null],name:"callback",docstring:"callable that gets invoked every five iterations",options:null,default_value:"None",expected_shape:null},{is_optional:!1,param_type:["int"],name:"batch_size",docstring:"The number of samples to take in each batch.",options:null,default_value:null,expected_shape:null},{is_optional:!0,param_type:["bool"],name:"verbose",docstring:"To control the verbosity of the procedure.",options:null,default_value:"False",expected_shape:null},{is_optional:!1,param_type:["bool"],name:"shuffle",docstring:"Whether to shuffle the data before splitting it in batches.",options:null,default_value:null,expected_shape:null},{is_optional:!0,param_type:["int",null],name:"n_jobs",docstring:"Number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",options:null,default_value:"None",expected_shape:null},{is_optional:!1,param_type:["LIST_VALID_OPTIONS"],name:"method",docstring:"lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse.",options:["lars","cd"],default_value:null,expected_shape:null},{is_optional:!0,param_type:["int"],name:"iter_offset",docstring:"Number of previous iterations completed on the dictionary used for initialization.",options:null,default_value:"0",expected_shape:null},{is_optional:!0,param_type:["int",null],name:"random_state",docstring:"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`.",options:null,default_value:"None",expected_shape:null},{is_optional:!0,param_type:["bool"],name:"return_inner_stats",docstring:"Return the inner statistics A (dictionary covariance) and B (data approximation). Useful to restart the algorithm in an online setting. If return_inner_stats is True, return_code is ignored",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["array","tuple"],name:"inner_stats",docstring:"Inner sufficient statistics that are kept by the algorithm. Passing them at initialization is useful in online settings, to avoid loosing the history of the evolution. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["bool"],name:"return_n_iter",docstring:"Whether or not to return the number of iterations.",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["bool"],name:"positive_dict",docstring:"Whether to enforce positivity when finding the dictionary.  .. versionadded:: 0.20",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["bool"],name:"positive_code",docstring:"Whether to enforce positivity when finding the code.  .. versionadded:: 0.20",options:null,default_value:null,expected_shape:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["array"],name:"code",docstring:"the sparse code (only returned if `return_code=True`)"},{returned:!0,param_type:["array"],name:"dictionary",docstring:"the solutions to the dictionary learning problem"},{returned:!0,param_type:["int"],name:"n_iter",docstring:"Number of iterations run. Returned only if `return_n_iter` is set to `True`."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"dict_learning_online",docstring:"Solves a dictionary learning matrix factorization problem online.\n\n    Finds the best dictionary and the corresponding sparse code for\n    approximating the data matrix X by solving::\n\n        (U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n                     (U,V)\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\n\n    where V is the dictionary and U is the sparse code. This is\n    accomplished by repeatedly iterating over mini-batches by slicing\n    the input data.\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    X : array of shape (n_samples, n_features)\n        Data matrix.\n\n    n_components : int,\n        Number of dictionary atoms to extract.\n\n    alpha : float,\n        Sparsity controlling parameter.\n\n    n_iter : int,\n        Number of iterations to perform.\n\n    return_code : boolean,\n        Whether to also return the code U or just the dictionary V.\n\n    dict_init : array of shape (n_components, n_features),\n        Initial value for the dictionary for warm restart scenarios.\n\n    callback : callable or None, optional (default: None)\n        callable that gets invoked every five iterations\n\n    batch_size : int,\n        The number of samples to take in each batch.\n\n    verbose : bool, optional (default: False)\n        To control the verbosity of the procedure.\n\n    shuffle : boolean,\n        Whether to shuffle the data before splitting it in batches.\n\n    n_jobs : int or None, optional (default=None)\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    method : {'lars', 'cd'}\n        lars: uses the least angle regression method to solve the lasso problem\n        (linear_model.lars_path)\n        cd: uses the coordinate descent method to compute the\n        Lasso solution (linear_model.Lasso). Lars will be faster if\n        the estimated components are sparse.\n\n    iter_offset : int, default 0\n        Number of previous iterations completed on the dictionary used for\n        initialization.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    return_inner_stats : boolean, optional\n        Return the inner statistics A (dictionary covariance) and B\n        (data approximation). Useful to restart the algorithm in an\n        online setting. If return_inner_stats is True, return_code is\n        ignored\n\n    inner_stats : tuple of (A, B) ndarrays\n        Inner sufficient statistics that are kept by the algorithm.\n        Passing them at initialization is useful in online settings, to\n        avoid loosing the history of the evolution.\n        A (n_components, n_components) is the dictionary covariance matrix.\n        B (n_features, n_components) is the data approximation matrix\n\n    return_n_iter : bool\n        Whether or not to return the number of iterations.\n\n    positive_dict : bool\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    positive_code : bool\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    code : array of shape (n_samples, n_components),\n        the sparse code (only returned if `return_code=True`)\n\n    dictionary : array of shape (n_components, n_features),\n        the solutions to the dictionary learning problem\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is\n        set to `True`.\n\n    See also\n    --------\n    dict_learning\n    DictionaryLearning\n    MiniBatchDictionaryLearning\n    SparsePCA\n    MiniBatchSparsePCA\n\n    "},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"Training vector, where n_samples is the number of samples and n_features is the number of features.",options:null,default_value:null,expected_shape:"(n_samples, n_features)"},{is_optional:!0,param_type:["int"],name:"n_components",docstring:"Number of components to extract. If None no dimension reduction is performed.",options:null,default_value:null,expected_shape:null},{is_optional:!0,param_type:["LIST_VALID_OPTIONS"],name:"algorithm",docstring:"Apply a parallel or deflational FASTICA algorithm.",options:["parallel","deflation","optional"],default_value:null,expected_shape:null},{is_optional:!0,param_type:["bool"],name:"whiten",docstring:"If True perform an initial whitening of the data. If False, the data is assumed to have already been preprocessed: it should be centered, normed and white. Otherwise you will get incorrect results. In this case the parameter n_components will be ignored.",options:null,default_value:null,expected_shape:null},{is_optional:!0,param_type:["str"],name:"fun",docstring:"The functional form of the G function used in the approximation to neg-entropy. Could be either 'logcosh', 'exp', or 'cube'. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. The derivative should be averaged along its last dimension. Example:  def my_g(x):     return x ** 3, np.mean(3 * x ** 2, axis=-1)",options:null,default_value:null,expected_shape:null},{is_optional:!0,param_type:["dict"],name:"fun_args",docstring:"Arguments to send to the functional form. If empty or None and if fun='logcosh', fun_args will take value {'alpha' : 1.0}",options:null,default_value:null,expected_shape:null},{is_optional:!0,param_type:["int"],name:"max_iter",docstring:"Maximum number of iterations to perform.",options:null,default_value:null,expected_shape:null},{is_optional:!0,param_type:["float"],name:"tol",docstring:"A positive scalar giving the tolerance at which the un-mixing matrix is considered to have converged.",options:null,default_value:null,expected_shape:null},{is_optional:!0,param_type:["array"],name:"w_init",docstring:"Initial un-mixing array of dimension (n.comp,n.comp). If None (default) then an array of normal r.v.'s is used.",options:null,default_value:null,expected_shape:null},{is_optional:!0,param_type:["int",null],name:"random_state",docstring:"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`.",options:null,default_value:"None",expected_shape:null},{is_optional:!0,param_type:["bool"],name:"return_X_mean",docstring:"If True, X_mean is returned too.",options:null,default_value:null,expected_shape:null},{is_optional:!0,param_type:["bool"],name:"compute_sources",docstring:"If False, sources are not computed, but only the rotation matrix. This can save memory when working with big data. Defaults to True.",options:null,default_value:null,expected_shape:null},{is_optional:!0,param_type:["bool"],name:"return_n_iter",docstring:"Whether or not to return the number of iterations.",options:null,default_value:null,expected_shape:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["LIST_VALID_OPTIONS","array",null],name:"K",docstring:"If whiten is 'True', K is the pre-whitening matrix that projects data onto the first n_components principal components. If whiten is 'False', K is 'None'."},{returned:!0,param_type:["array"],name:"W",docstring:"Estimated un-mixing matrix. The mixing matrix can be obtained by::      w = np.dot(W, K.T)     A = w.T * (w * w.T).I"},{returned:!0,param_type:["LIST_VALID_OPTIONS","array",null],name:"S",docstring:"Estimated source matrix"},{returned:!0,param_type:["array"],name:"X_mean",docstring:"The mean over features. Returned only if return_X_mean is True."},{returned:!0,param_type:["int"],name:"n_iter",docstring:'If the algorithm is "deflation", n_iter is the maximum number of iterations run across all components. Else they are just the number of iterations taken to converge. This is returned only when return_n_iter is set to `True`.'},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"fastica",docstring:"Perform Fast Independent Component Analysis.\n\n    Read more in the :ref:`User Guide <ICA>`.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    n_components : int, optional\n        Number of components to extract. If None no dimension reduction\n        is performed.\n\n    algorithm : {'parallel', 'deflation'}, optional\n        Apply a parallel or deflational FASTICA algorithm.\n\n    whiten : boolean, optional\n        If True perform an initial whitening of the data.\n        If False, the data is assumed to have already been\n        preprocessed: it should be centered, normed and white.\n        Otherwise you will get incorrect results.\n        In this case the parameter n_components will be ignored.\n\n    fun : string or function, optional. Default: 'logcosh'\n        The functional form of the G function used in the\n        approximation to neg-entropy. Could be either 'logcosh', 'exp',\n        or 'cube'.\n        You can also provide your own function. It should return a tuple\n        containing the value of the function, and of its derivative, in the\n        point. The derivative should be averaged along its last dimension.\n        Example:\n\n        def my_g(x):\n            return x ** 3, np.mean(3 * x ** 2, axis=-1)\n\n    fun_args : dictionary, optional\n        Arguments to send to the functional form.\n        If empty or None and if fun='logcosh', fun_args will take value\n        {'alpha' : 1.0}\n\n    max_iter : int, optional\n        Maximum number of iterations to perform.\n\n    tol : float, optional\n        A positive scalar giving the tolerance at which the\n        un-mixing matrix is considered to have converged.\n\n    w_init : (n_components, n_components) array, optional\n        Initial un-mixing array of dimension (n.comp,n.comp).\n        If None (default) then an array of normal r.v.'s is used.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    return_X_mean : bool, optional\n        If True, X_mean is returned too.\n\n    compute_sources : bool, optional\n        If False, sources are not computed, but only the rotation matrix.\n        This can save memory when working with big data. Defaults to True.\n\n    return_n_iter : bool, optional\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    K : array, shape (n_components, n_features) | None.\n        If whiten is 'True', K is the pre-whitening matrix that projects data\n        onto the first n_components principal components. If whiten is 'False',\n        K is 'None'.\n\n    W : array, shape (n_components, n_components)\n        Estimated un-mixing matrix.\n        The mixing matrix can be obtained by::\n\n            w = np.dot(W, K.T)\n            A = w.T * (w * w.T).I\n\n    S : array, shape (n_samples, n_components) | None\n        Estimated source matrix\n\n    X_mean : array, shape (n_features, )\n        The mean over features. Returned only if return_X_mean is True.\n\n    n_iter : int\n        If the algorithm is \"deflation\", n_iter is the\n        maximum number of iterations run across all components. Else\n        they are just the number of iterations taken to converge. This is\n        returned only when return_n_iter is set to `True`.\n\n    Notes\n    -----\n\n    The data matrix X is considered to be a linear combination of\n    non-Gaussian (independent) components i.e. X = AS where columns of S\n    contain the independent components and A is a linear mixing\n    matrix. In short ICA attempts to `un-mix' the data by estimating an\n    un-mixing matrix W where ``S = W K X.``\n\n    This implementation was originally made for data of shape\n    [n_features, n_samples]. Now the input is transposed\n    before the algorithm is applied. This makes it slightly\n    faster for Fortran-ordered input.\n\n    Implemented using FastICA:\n    `A. Hyvarinen and E. Oja, Independent Component Analysis:\n    Algorithms and Applications, Neural Networks, 13(4-5), 2000,\n    pp. 411-430`\n\n    "},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"Constant matrix.",options:null,default_value:null,expected_shape:"(n_samples, n_features)"},{is_optional:!1,param_type:["array"],name:"W",docstring:"If init='custom', it is used as initial guess for the solution.",options:null,default_value:null,expected_shape:"(n_samples, n_components)"},{is_optional:!1,param_type:["array"],name:"H",docstring:"If init='custom', it is used as initial guess for the solution. If update_H=False, it is used as a constant, to solve for W only.",options:null,default_value:null,expected_shape:"(n_components, n_features)"},{is_optional:!1,param_type:["int"],name:"n_components",docstring:"Number of components, if n_components is not set all features are kept.",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["LIST_VALID_OPTIONS",null],name:"init",docstring:"Method used to initialize the procedure. Default: 'random'. Valid options:  - 'random': non-negative random matrices, scaled with:     sqrt(X.mean() / n_components)  - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)     initialization (better for sparseness)  - 'nndsvda': NNDSVD with zeros filled with the average of X     (better when sparsity is not desired)  - 'nndsvdar': NNDSVD with zeros filled with small random values     (generally faster, less accurate alternative to NNDSVDa     for when sparsity is not desired)  - 'custom': use custom matrices W and H",options:[" None "," 'random' "," 'nndsvd' "," 'nndsvda' "," 'nndsvdar' "," 'custom"],default_value:null,expected_shape:null},{is_optional:!0,param_type:["bool"],name:"update_H",docstring:"Set to True, both W and H will be estimated from initial guesses. Set to False, only W will be estimated.",options:null,default_value:"True",expected_shape:null},{is_optional:!1,param_type:["LIST_VALID_OPTIONS"],name:"solver",docstring:"Numerical solver to use: 'cd' is a Coordinate Descent solver that uses Fast Hierarchical     Alternating Least Squares (Fast HALS). 'mu' is a Multiplicative Update solver.  .. versionadded:: 0.17    Coordinate Descent solver.  .. versionadded:: 0.19    Multiplicative Update solver.",options:["cd' "," 'mu"],default_value:null,expected_shape:null},{is_optional:!0,param_type:["float","str"],name:"beta_loss",docstring:"String must be in {'frobenius', 'kullback-leibler', 'itakura-saito'}. Beta divergence to be minimized, measuring the distance between X and the dot product WH. Note that values different from 'frobenius' (or 2) and 'kullback-leibler' (or 1) lead to significantly slower fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input matrix X cannot contain zeros. Used only in 'mu' solver.  .. versionadded:: 0.19",options:null,default_value:"frobenius",expected_shape:null},{is_optional:!0,param_type:["float"],name:"tol",docstring:"Tolerance of the stopping condition.",options:null,default_value:"1e-4",expected_shape:null},{is_optional:!0,param_type:["int"],name:"max_iter",docstring:"Maximum number of iterations before timing out.",options:null,default_value:"200",expected_shape:null},{is_optional:!0,param_type:["float"],name:"alpha",docstring:"Constant that multiplies the regularization terms.",options:null,default_value:"0.",expected_shape:null},{is_optional:!0,param_type:["float"],name:"l1_ratio",docstring:"The regularization mixing parameter, with 0 <= l1_ratio <= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.",options:null,default_value:"0.",expected_shape:null},{is_optional:!1,param_type:["LIST_VALID_OPTIONS",null],name:"regularization",docstring:"Select whether the regularization affects the components (H), the transformation (W), both or none of them.",options:["both' "," 'components' "," 'transformation' "," None"],default_value:null,expected_shape:null},{is_optional:!0,param_type:["int",null],name:"random_state",docstring:"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`.",options:null,default_value:"None",expected_shape:null},{is_optional:!0,param_type:["int"],name:"verbose",docstring:"The verbosity level.",options:null,default_value:"0",expected_shape:null},{is_optional:!0,param_type:["bool"],name:"shuffle",docstring:"If true, randomize the order of coordinates in the CD solver.",options:null,default_value:"False",expected_shape:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["array"],name:"W",docstring:"Solution to the non-negative least squares problem."},{returned:!0,param_type:["array"],name:"H",docstring:"Solution to the non-negative least squares problem."},{returned:!0,param_type:["int"],name:"n_iter",docstring:"Actual number of iterations."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"non_negative_factorization",docstring:"Compute Non-negative Matrix Factorization (NMF)\n\n    Find two non-negative matrices (W, H) whose product approximates the non-\n    negative matrix X. This factorization can be used for example for\n    dimensionality reduction, source separation or topic extraction.\n\n    The objective function is::\n\n        0.5 * ||X - WH||_Fro^2\n        + alpha * l1_ratio * ||vec(W)||_1\n        + alpha * l1_ratio * ||vec(H)||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n        + 0.5 * alpha * (1 - l1_ratio) * ||H||_Fro^2\n\n    Where::\n\n        ||A||_Fro^2 = \\sum_{i,j} A_{ij}^2 (Frobenius norm)\n        ||vec(A)||_1 = \\sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)\n\n    For multiplicative-update ('mu') solver, the Frobenius norm\n    (0.5 * ||X - WH||_Fro^2) can be changed into another beta-divergence loss,\n    by changing the beta_loss parameter.\n\n    The objective function is minimized with an alternating minimization of W\n    and H. If H is given and update_H=False, it solves for W only.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Constant matrix.\n\n    W : array-like, shape (n_samples, n_components)\n        If init='custom', it is used as initial guess for the solution.\n\n    H : array-like, shape (n_components, n_features)\n        If init='custom', it is used as initial guess for the solution.\n        If update_H=False, it is used as a constant, to solve for W only.\n\n    n_components : integer\n        Number of components, if n_components is not set all features\n        are kept.\n\n    init :  None | 'random' | 'nndsvd' | 'nndsvda' | 'nndsvdar' | 'custom'\n        Method used to initialize the procedure.\n        Default: 'random'.\n        Valid options:\n\n        - 'random': non-negative random matrices, scaled with:\n            sqrt(X.mean() / n_components)\n\n        - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)\n            initialization (better for sparseness)\n\n        - 'nndsvda': NNDSVD with zeros filled with the average of X\n            (better when sparsity is not desired)\n\n        - 'nndsvdar': NNDSVD with zeros filled with small random values\n            (generally faster, less accurate alternative to NNDSVDa\n            for when sparsity is not desired)\n\n        - 'custom': use custom matrices W and H\n\n    update_H : boolean, default: True\n        Set to True, both W and H will be estimated from initial guesses.\n        Set to False, only W will be estimated.\n\n    solver : 'cd' | 'mu'\n        Numerical solver to use:\n        'cd' is a Coordinate Descent solver that uses Fast Hierarchical\n            Alternating Least Squares (Fast HALS).\n        'mu' is a Multiplicative Update solver.\n\n        .. versionadded:: 0.17\n           Coordinate Descent solver.\n\n        .. versionadded:: 0.19\n           Multiplicative Update solver.\n\n    beta_loss : float or string, default 'frobenius'\n        String must be in {'frobenius', 'kullback-leibler', 'itakura-saito'}.\n        Beta divergence to be minimized, measuring the distance between X\n        and the dot product WH. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n        matrix X cannot contain zeros. Used only in 'mu' solver.\n\n        .. versionadded:: 0.19\n\n    tol : float, default: 1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : integer, default: 200\n        Maximum number of iterations before timing out.\n\n    alpha : double, default: 0.\n        Constant that multiplies the regularization terms.\n\n    l1_ratio : double, default: 0.\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    regularization : 'both' | 'components' | 'transformation' | None\n        Select whether the regularization affects the components (H), the\n        transformation (W), both or none of them.\n\n    random_state : int, RandomState instance or None, optional, default: None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    verbose : integer, default: 0\n        The verbosity level.\n\n    shuffle : boolean, default: False\n        If true, randomize the order of coordinates in the CD solver.\n\n    Returns\n    -------\n    W : array-like, shape (n_samples, n_components)\n        Solution to the non-negative least squares problem.\n\n    H : array-like, shape (n_components, n_features)\n        Solution to the non-negative least squares problem.\n\n    n_iter : int\n        Actual number of iterations.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import non_negative_factorization\n    >>> W, H, n_iter = non_negative_factorization(X, n_components=2,\n    ... init='random', random_state=0)\n\n    References\n    ----------\n    Cichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for\n    large scale nonnegative matrix and tensor factorizations.\"\n    IEICE transactions on fundamentals of electronics, communications and\n    computer sciences 92.3: 708-721, 2009.\n\n    Fevotte, C., & Idier, J. (2011). Algorithms for nonnegative matrix\n    factorization with the beta-divergence. Neural Computation, 23(9).\n    "},{inputs:[{is_optional:!1,param_type:["array"],name:"M",docstring:"Matrix to decompose",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["int"],name:"n_components",docstring:"Number of singular values and vectors to extract.",options:null,default_value:null,expected_shape:null},{is_optional:!0,param_type:["int"],name:"n_oversamples",docstring:"Additional number of random vectors to sample the range of M so as to ensure proper conditioning. The total number of random vectors used to find the range of M is n_components + n_oversamples. Smaller number can improve speed but can negatively impact the quality of approximation of singular vectors and singular values.",options:null,default_value:"10",expected_shape:null},{is_optional:!0,param_type:["int"],name:"n_iter",docstring:"Number of power iterations. It can be used to deal with very noisy problems. When 'auto', it is set to 4, unless `n_components` is small (< .1 * min(X.shape)) `n_iter` in which case is set to 7. This improves precision with few components.  .. versionchanged:: 0.18",options:null,default_value:"auto",expected_shape:null},{is_optional:!0,param_type:[null],name:"power_iteration_normalizer",docstring:"Whether the power iterations are normalized with step-by-step QR factorization (the slowest but most accurate), 'none' (the fastest but numerically unstable when `n_iter` is large, e.g. typically 5 or larger), or 'LU' factorization (numerically stable but can lose slightly in accuracy). The 'auto' mode applies no normalization if `n_iter` <= 2 and switches to LU otherwise.  .. versionadded:: 0.18",options:null,default_value:null,expected_shape:null},{is_optional:!0,param_type:[null],name:"transpose",docstring:"Whether the algorithm should be applied to M.T instead of M. The result should approximately be the same. The 'auto' mode will trigger the transposition if M.shape[1] > M.shape[0] since this implementation of randomized SVD tend to be a little faster in that case.  .. versionchanged:: 0.18",options:null,default_value:"",expected_shape:null},{is_optional:!0,param_type:["bool"],name:"flip_sign",docstring:"The output of a singular value decomposition is only unique up to a permutation of the signs of the singular vectors. If `flip_sign` is set to `True`, the sign ambiguity is resolved by making the largest loadings for each component in the left singular vectors positive.",options:null,default_value:"",expected_shape:null},{is_optional:!0,param_type:["int",null],name:"random_state",docstring:"The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`.",options:null,default_value:"None",expected_shape:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"randomized_svd",docstring:"Computes a truncated randomized SVD\n\n    Parameters\n    ----------\n    M : ndarray or sparse matrix\n        Matrix to decompose\n\n    n_components : int\n        Number of singular values and vectors to extract.\n\n    n_oversamples : int (default is 10)\n        Additional number of random vectors to sample the range of M so as\n        to ensure proper conditioning. The total number of random vectors\n        used to find the range of M is n_components + n_oversamples. Smaller\n        number can improve speed but can negatively impact the quality of\n        approximation of singular vectors and singular values.\n\n    n_iter : int or 'auto' (default is 'auto')\n        Number of power iterations. It can be used to deal with very noisy\n        problems. When 'auto', it is set to 4, unless `n_components` is small\n        (< .1 * min(X.shape)) `n_iter` in which case is set to 7.\n        This improves precision with few components.\n\n        .. versionchanged:: 0.18\n\n    power_iteration_normalizer : 'auto' (default), 'QR', 'LU', 'none'\n        Whether the power iterations are normalized with step-by-step\n        QR factorization (the slowest but most accurate), 'none'\n        (the fastest but numerically unstable when `n_iter` is large, e.g.\n        typically 5 or larger), or 'LU' factorization (numerically stable\n        but can lose slightly in accuracy). The 'auto' mode applies no\n        normalization if `n_iter` <= 2 and switches to LU otherwise.\n\n        .. versionadded:: 0.18\n\n    transpose : True, False or 'auto' (default)\n        Whether the algorithm should be applied to M.T instead of M. The\n        result should approximately be the same. The 'auto' mode will\n        trigger the transposition if M.shape[1] > M.shape[0] since this\n        implementation of randomized SVD tend to be a little faster in that\n        case.\n\n        .. versionchanged:: 0.18\n\n    flip_sign : boolean, (True by default)\n        The output of a singular value decomposition is only unique up to a\n        permutation of the signs of the singular vectors. If `flip_sign` is\n        set to `True`, the sign ambiguity is resolved by making the largest\n        loadings for each component in the left singular vectors positive.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`.\n\n    Notes\n    -----\n    This algorithm finds a (usually very good) approximate truncated\n    singular value decomposition using randomization to speed up the\n    computations. It is particularly fast on large matrices on which\n    you wish to extract only a small number of components. In order to\n    obtain further speed up, `n_iter` can be set <=2 (at the cost of\n    loss of precision).\n\n    References\n    ----------\n    * Finding structure with randomness: Stochastic algorithms for constructing\n      approximate matrix decompositions\n      Halko, et al., 2009 https://arxiv.org/abs/0909.4061\n\n    * A randomized algorithm for the decomposition of matrices\n      Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert\n\n    * An implementation of a randomized algorithm for principal component\n      analysis\n      A. Szlam et al. 2014\n    "},{inputs:[{is_optional:!1,param_type:["array"],name:"X",docstring:"Data matrix",options:null,default_value:null,expected_shape:"(n_samples, n_features)"},{is_optional:!1,param_type:["array"],name:"dictionary",docstring:"The dictionary matrix against which to solve the sparse coding of the data. Some of the algorithms assume normalized rows for meaningful output.",options:null,default_value:null,expected_shape:"(n_components, n_features)"},{is_optional:!1,param_type:["array"],name:"gram",docstring:"Precomputed Gram matrix, dictionary * dictionary'",options:null,default_value:null,expected_shape:"(n_components, n_components)"},{is_optional:!1,param_type:["array"],name:"cov",docstring:"Precomputed covariance, dictionary' * X",options:null,default_value:null,expected_shape:"(n_components, n_samples)"},{is_optional:!1,param_type:["LIST_VALID_OPTIONS"],name:"algorithm",docstring:"lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection dictionary * X'",options:["lasso_lars","lasso_cd","lars","omp","threshold"],default_value:null,expected_shape:null},{is_optional:!0,param_type:["int"],name:"n_nonzero_coefs",docstring:"Number of nonzero coefficients to target in each column of the solution. This is only used by `algorithm='lars'` and `algorithm='omp'` and is overridden by `alpha` in the `omp` case.",options:null,default_value:"",expected_shape:null},{is_optional:!0,param_type:["float"],name:"alpha",docstring:"If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the penalty applied to the L1 norm. If `algorithm='threshold'`, `alpha` is the absolute value of the threshold below which coefficients will be squashed to zero. If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides `n_nonzero_coefs`.",options:null,default_value:"",expected_shape:null},{is_optional:!0,param_type:["bool"],name:"copy_cov",docstring:"Whether to copy the precomputed covariance matrix; if False, it may be overwritten.",options:null,default_value:null,expected_shape:null},{is_optional:!1,param_type:["array"],name:"init",docstring:"Initialization value of the sparse codes. Only used if `algorithm='lasso_cd'`.",options:null,default_value:null,expected_shape:"(n_samples, n_components)"},{is_optional:!0,param_type:["int"],name:"max_iter",docstring:"Maximum number of iterations to perform if `algorithm='lasso_cd'`.",options:null,default_value:"",expected_shape:null},{is_optional:!0,param_type:["int",null],name:"n_jobs",docstring:"Number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",options:null,default_value:"None",expected_shape:null},{is_optional:!0,param_type:["bool"],name:"check_input",docstring:"If False, the input arrays X and dictionary will not be checked.",options:null,default_value:null,expected_shape:null},{is_optional:!0,param_type:["int"],name:"verbose",docstring:"Controls the verbosity; the higher, the more messages. Defaults to 0.",options:null,default_value:null,expected_shape:null},{is_optional:!0,param_type:["bool"],name:"positive",docstring:"Whether to enforce positivity when finding the encoding.  .. versionadded:: 0.20",options:null,default_value:null,expected_shape:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{returned:!0,param_type:["array"],name:"code",docstring:"The sparse codes"},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],name:"sparse_encode",docstring:"Sparse coding\n\n    Each row of the result is the solution to a sparse coding problem.\n    The goal is to find a sparse array `code` such that::\n\n        X ~= code * dictionary\n\n    Read more in the :ref:`User Guide <SparseCoder>`.\n\n    Parameters\n    ----------\n    X : array of shape (n_samples, n_features)\n        Data matrix\n\n    dictionary : array of shape (n_components, n_features)\n        The dictionary matrix against which to solve the sparse coding of\n        the data. Some of the algorithms assume normalized rows for meaningful\n        output.\n\n    gram : array, shape=(n_components, n_components)\n        Precomputed Gram matrix, dictionary * dictionary'\n\n    cov : array, shape=(n_components, n_samples)\n        Precomputed covariance, dictionary' * X\n\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}\n        lars: uses the least angle regression method (linear_model.lars_path)\n        lasso_lars: uses Lars to compute the Lasso solution\n        lasso_cd: uses the coordinate descent method to compute the\n        Lasso solution (linear_model.Lasso). lasso_lars will be faster if\n        the estimated components are sparse.\n        omp: uses orthogonal matching pursuit to estimate the sparse solution\n        threshold: squashes to zero all coefficients less than alpha from\n        the projection dictionary * X'\n\n    n_nonzero_coefs : int, 0.1 * n_features by default\n        Number of nonzero coefficients to target in each column of the\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n        and is overridden by `alpha` in the `omp` case.\n\n    alpha : float, 1. by default\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n        penalty applied to the L1 norm.\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\n        threshold below which coefficients will be squashed to zero.\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n        the reconstruction error targeted. In this case, it overrides\n        `n_nonzero_coefs`.\n\n    copy_cov : boolean, optional\n        Whether to copy the precomputed covariance matrix; if False, it may be\n        overwritten.\n\n    init : array of shape (n_samples, n_components)\n        Initialization value of the sparse codes. Only used if\n        `algorithm='lasso_cd'`.\n\n    max_iter : int, 1000 by default\n        Maximum number of iterations to perform if `algorithm='lasso_cd'`.\n\n    n_jobs : int or None, optional (default=None)\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    check_input : boolean, optional\n        If False, the input arrays X and dictionary will not be checked.\n\n    verbose : int, optional\n        Controls the verbosity; the higher, the more messages. Defaults to 0.\n\n    positive : boolean, optional\n        Whether to enforce positivity when finding the encoding.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    code : array of shape (n_samples, n_components)\n        The sparse codes\n\n    See also\n    --------\n    sklearn.linear_model.lars_path\n    sklearn.linear_model.orthogonal_mp\n    sklearn.linear_model.Lasso\n    SparseCoder\n    "}],docstring:"",outputs:[]}},function(e,t,n){"use strict";t.a={name:"sklearn.preprocessing",docstring:"",inputs:[],outputs:[],node_functions:[{name:"add_dummy_feature",docstring:"Augment dataset with an additional dummy feature.\n\n    This is useful for fitting an intercept term with implementations which\n    cannot otherwise fit it directly.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n        Data.\n\n    value : float\n        Value to use for the dummy feature.\n\n    Returns\n    -------\n\n    X : {array, sparse matrix}, shape [n_samples, n_features + 1]\n        Same data with dummy feature added as first column.\n\n    Examples\n    --------\n\n    >>> from sklearn.preprocessing import add_dummy_feature\n    >>> add_dummy_feature([[0, 1], [1, 0]])\n    array([[1., 0., 1.],\n           [1., 1., 0.]])\n    ",inputs:[{name:"X",docstring:"Data.",param_type:["LIST_VALID_OPTIONS","array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:["array-like","sparse matrix","shape [n_samples","n_features]"]},{name:"value",docstring:"Value to use for the dummy feature.",param_type:["float"],expected_shape:null,is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"X",docstring:"Same data with dummy feature added as first column.",param_type:["LIST_VALID_OPTIONS","array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"binarize",docstring:"Boolean thresholding of array-like or scipy.sparse matrix\n\n    Read more in the :ref:`User Guide <preprocessing_binarization>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n        The data to binarize, element by element.\n        scipy.sparse matrices should be in CSR or CSC format to avoid an\n        un-necessary copy.\n\n    threshold : float, optional (0.0 by default)\n        Feature values below or equal to this are replaced by 0, above it by 1.\n        Threshold may not be less than 0 for operations on sparse matrices.\n\n    copy : boolean, optional, default True\n        set to False to perform inplace binarization and avoid a copy\n        (if the input is already a numpy array or a scipy.sparse CSR / CSC\n        matrix and if axis is 1).\n\n    See also\n    --------\n    Binarizer: Performs binarization using the ``Transformer`` API\n        (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).\n    ",inputs:[{name:"X",docstring:"The data to binarize, element by element. scipy.sparse matrices should be in CSR or CSC format to avoid an un-necessary copy.",param_type:["LIST_VALID_OPTIONS","array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:["array-like","sparse matrix","shape [n_samples","n_features]"]},{name:"threshold",docstring:"Feature values below or equal to this are replaced by 0, above it by 1. Threshold may not be less than 0 for operations on sparse matrices.",param_type:["float"],expected_shape:null,is_optional:!0,default_value:"",options:null},{name:"copy",docstring:"set to False to perform inplace binarization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR / CSC matrix and if axis is 1).",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"label_binarize",docstring:"Binarize labels in a one-vs-all fashion\n\n    Several regression and binary classification algorithms are\n    available in scikit-learn. A simple way to extend these algorithms\n    to the multi-class classification case is to use the so-called\n    one-vs-all scheme.\n\n    This function makes it possible to compute this transformation for a\n    fixed set of class labels known ahead of time.\n\n    Parameters\n    ----------\n    y : array-like\n        Sequence of integer labels or multilabel data to encode.\n\n    classes : array-like of shape [n_classes]\n        Uniquely holds the label for each class.\n\n    neg_label : int (default: 0)\n        Value with which negative labels must be encoded.\n\n    pos_label : int (default: 1)\n        Value with which positive labels must be encoded.\n\n    sparse_output : boolean (default: False),\n        Set to true if output binary array is desired in CSR sparse format\n\n    Returns\n    -------\n    Y : numpy array or CSR matrix of shape [n_samples, n_classes]\n        Shape will be [n_samples, 1] for binary problems.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import label_binarize\n    >>> label_binarize([1, 6], classes=[1, 2, 4, 6])\n    array([[1, 0, 0, 0],\n           [0, 0, 0, 1]])\n\n    The class ordering is preserved:\n\n    >>> label_binarize([1, 6], classes=[1, 6, 4, 2])\n    array([[1, 0, 0, 0],\n           [0, 1, 0, 0]])\n\n    Binary targets transform to a column vector\n\n    >>> label_binarize(['yes', 'no', 'no', 'yes'], classes=['no', 'yes'])\n    array([[1],\n           [0],\n           [0],\n           [1]])\n\n    See also\n    --------\n    LabelBinarizer : class used to wrap the functionality of label_binarize and\n        allow for fitting to classes independently of the transform operation\n    ",inputs:[{name:"y",docstring:"Sequence of integer labels or multilabel data to encode.",param_type:["array"],expected_shape:null,is_optional:!1,default_value:null,options:null},{name:"classes",docstring:"Uniquely holds the label for each class.",param_type:["array"],expected_shape:"[n_classes]",is_optional:!1,default_value:null,options:null},{name:"neg_label",docstring:"Value with which negative labels must be encoded.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"0",options:null},{name:"pos_label",docstring:"Value with which positive labels must be encoded.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"1",options:null},{name:"sparse_output",docstring:"Set to true if output binary array is desired in CSR sparse format",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"Y",docstring:"Shape will be [n_samples, 1] for binary problems.",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"maxabs_scale",docstring:"Scale each feature to the [-1, 1] range without breaking the sparsity.\n\n    This estimator scales each feature individually such\n    that the maximal absolute value of each feature in the\n    training set will be 1.0.\n\n    This scaler can also be applied to sparse CSR or CSC matrices.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        The data.\n\n    axis : int (0 by default)\n        axis used to scale along. If 0, independently scale each feature,\n        otherwise (if 1) scale each sample.\n\n    copy : boolean, optional, default is True\n        Set to False to perform inplace scaling and avoid a copy (if the input\n        is already a numpy array).\n\n    See also\n    --------\n    MaxAbsScaler: Performs scaling to the [-1, 1] range using the``Transformer`` API\n        (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded to compute the statistics,\n    and maintained during the data transformation.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n    ",inputs:[{name:"X",docstring:"The data.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null},{name:"axis",docstring:"axis used to scale along. If 0, independently scale each feature, otherwise (if 1) scale each sample.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"",options:null},{name:"copy",docstring:"Set to False to perform inplace scaling and avoid a copy (if the input is already a numpy array).",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"minmax_scale",docstring:"Transforms features by scaling each feature to a given range.\n\n    This estimator scales and translates each feature individually such\n    that it is in the given range on the training set, i.e. between\n    zero and one.\n\n    The transformation is given by (when ``axis=0``)::\n\n        X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n        X_scaled = X_std * (max - min) + min\n\n    where min, max = feature_range.\n\n    The transformation is calculated as (when ``axis=0``)::\n\n       X_scaled = scale * X + min - X.min(axis=0) * scale\n       where scale = (max - min) / (X.max(axis=0) - X.min(axis=0))\n\n    This transformation is often used as an alternative to zero mean,\n    unit variance scaling.\n\n    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n\n    .. versionadded:: 0.17\n       *minmax_scale* function interface\n       to :class:`sklearn.preprocessing.MinMaxScaler`.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        The data.\n\n    feature_range : tuple (min, max), default=(0, 1)\n        Desired range of transformed data.\n\n    axis : int (0 by default)\n        axis used to scale along. If 0, independently scale each feature,\n        otherwise (if 1) scale each sample.\n\n    copy : boolean, optional, default is True\n        Set to False to perform inplace scaling and avoid a copy (if the input\n        is already a numpy array).\n\n    See also\n    --------\n    MinMaxScaler: Performs scaling to a given range using the``Transformer`` API\n        (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).\n\n    Notes\n    -----\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n    ",inputs:[{name:"X",docstring:"The data.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null},{name:"feature_range",docstring:"Desired range of transformed data.",param_type:["tuple"],expected_shape:null,is_optional:!0,default_value:null,options:null},{name:"axis",docstring:"axis used to scale along. If 0, independently scale each feature, otherwise (if 1) scale each sample.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"",options:null},{name:"copy",docstring:"Set to False to perform inplace scaling and avoid a copy (if the input is already a numpy array).",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"normalize",docstring:"Scale input vectors individually to unit norm (vector length).\n\n    Read more in the :ref:`User Guide <preprocessing_normalization>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n        The data to normalize, element by element.\n        scipy.sparse matrices should be in CSR format to avoid an\n        un-necessary copy.\n\n    norm : 'l1', 'l2', or 'max', optional ('l2' by default)\n        The norm to use to normalize each non zero sample (or each non-zero\n        feature if axis is 0).\n\n    axis : 0 or 1, optional (1 by default)\n        axis used to normalize the data along. If 1, independently normalize\n        each sample, otherwise (if 0) normalize each feature.\n\n    copy : boolean, optional, default True\n        set to False to perform inplace row normalization and avoid a\n        copy (if the input is already a numpy array or a scipy.sparse\n        CSR matrix and if axis is 1).\n\n    return_norm : boolean, default False\n        whether to return the computed norms\n\n    Returns\n    -------\n    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n        Normalized input X.\n\n    norms : array, shape [n_samples] if axis=1 else [n_features]\n        An array of norms along given axis for X.\n        When X is sparse, a NotImplementedError will be raised\n        for norm 'l1' or 'l2'.\n\n    See also\n    --------\n    Normalizer: Performs normalization using the ``Transformer`` API\n        (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).\n\n    Notes\n    -----\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\n    ",inputs:[{name:"X",docstring:"The data to normalize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.",param_type:["LIST_VALID_OPTIONS","array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:["array-like","sparse matrix","shape [n_samples","n_features]"]},{name:"norm",docstring:"The norm to use to normalize each non zero sample (or each non-zero feature if axis is 0).",param_type:[null],expected_shape:null,is_optional:!0,default_value:"",options:null},{name:"axis",docstring:"axis used to normalize the data along. If 1, independently normalize each sample, otherwise (if 0) normalize each feature.",param_type:[null],expected_shape:null,is_optional:!0,default_value:"",options:null},{name:"copy",docstring:"set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR matrix and if axis is 1).",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"return_norm",docstring:"whether to return the computed norms",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"False",options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"X",docstring:"Normalized input X.",param_type:["LIST_VALID_OPTIONS","array"],returned:!0},{name:"norms",docstring:"An array of norms along given axis for X. When X is sparse, a NotImplementedError will be raised for norm 'l1' or 'l2'.",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"power_transform",docstring:"\n    Power transforms are a family of parametric, monotonic transformations\n    that are applied to make data more Gaussian-like. This is useful for\n    modeling issues related to heteroscedasticity (non-constant variance),\n    or other situations where normality is desired.\n\n    Currently, power_transform supports the Box-Cox transform and the\n    Yeo-Johnson transform. The optimal parameter for stabilizing variance and\n    minimizing skewness is estimated through maximum likelihood.\n\n    Box-Cox requires input data to be strictly positive, while Yeo-Johnson\n    supports both positive or negative data.\n\n    By default, zero-mean, unit-variance normalization is applied to the\n    transformed data.\n\n    Read more in the :ref:`User Guide <preprocessing_transformer>`.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        The data to be transformed using a power transformation.\n\n    method : str\n        The power transform method. Available methods are:\n\n        - 'yeo-johnson' [1]_, works with positive and negative values\n        - 'box-cox' [2]_, only works with strictly positive values\n\n        The default method will be changed from 'box-cox' to 'yeo-johnson'\n        in version 0.23. To suppress the FutureWarning, explicitly set the\n        parameter.\n\n    standardize : boolean, default=True\n        Set to True to apply zero-mean, unit-variance normalization to the\n        transformed output.\n\n    copy : boolean, optional, default=True\n        Set to False to perform inplace computation during transformation.\n\n    Returns\n    -------\n    X_trans : array-like, shape (n_samples, n_features)\n        The transformed data.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.preprocessing import power_transform\n    >>> data = [[1, 2], [3, 2], [4, 5]]\n    >>> print(power_transform(data, method='box-cox'))  # doctest: +ELLIPSIS\n    [[-1.332... -0.707...]\n     [ 0.256... -0.707...]\n     [ 1.076...  1.414...]]\n\n    See also\n    --------\n    PowerTransformer : Equivalent transformation with the\n        ``Transformer`` API (e.g. as part of a preprocessing\n        :class:`sklearn.pipeline.Pipeline`).\n\n    quantile_transform : Maps data to a standard normal distribution with\n        the parameter `output_distribution='normal'`.\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded in ``fit``, and maintained\n    in ``transform``.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\n    References\n    ----------\n\n    .. [1] I.K. Yeo and R.A. Johnson, \"A new family of power transformations to\n           improve normality or symmetry.\" Biometrika, 87(4), pp.954-959,\n           (2000).\n\n    .. [2] G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal\n           of the Royal Statistical Society B, 26, 211-252 (1964).\n    ",inputs:[{name:"X",docstring:"The data to be transformed using a power transformation.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null},{name:"method",docstring:"The power transform method. Available methods are:  - 'yeo-johnson' [1]_, works with positive and negative values - 'box-cox' [2]_, only works with strictly positive values  The default method will be changed from 'box-cox' to 'yeo-johnson' in version 0.23. To suppress the FutureWarning, explicitly set the parameter.",param_type:["str"],expected_shape:null,is_optional:!1,default_value:null,options:null},{name:"standardize",docstring:"Set to True to apply zero-mean, unit-variance normalization to the transformed output.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"copy",docstring:"Set to False to perform inplace computation during transformation.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"X_trans",docstring:"The transformed data.",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"quantile_transform",docstring:"Transform features using quantiles information.\n\n    This method transforms the features to follow a uniform or a normal\n    distribution. Therefore, for a given feature, this transformation tends\n    to spread out the most frequent values. It also reduces the impact of\n    (marginal) outliers: this is therefore a robust preprocessing scheme.\n\n    The transformation is applied on each feature independently. First an\n    estimate of the cumulative distribution function of a feature is\n    used to map the original values to a uniform distribution. The obtained\n    values are then mapped to the desired output distribution using the\n    associated quantile function. Features values of new/unseen data that fall\n    below or above the fitted range will be mapped to the bounds of the output\n    distribution. Note that this transform is non-linear. It may distort linear\n    correlations between variables measured at the same scale but renders\n    variables measured at different scales more directly comparable.\n\n    Read more in the :ref:`User Guide <preprocessing_transformer>`.\n\n    Parameters\n    ----------\n    X : array-like, sparse matrix\n        The data to transform.\n\n    axis : int, (default=0)\n        Axis used to compute the means and standard deviations along. If 0,\n        transform each feature, otherwise (if 1) transform each sample.\n\n    n_quantiles : int, optional (default=1000 or n_samples)\n        Number of quantiles to be computed. It corresponds to the number\n        of landmarks used to discretize the cumulative distribution function.\n        If n_quantiles is larger than the number of samples, n_quantiles is set\n        to the number of samples as a larger number of quantiles does not give\n        a better approximation of the cumulative distribution function\n        estimator.\n\n    output_distribution : str, optional (default='uniform')\n        Marginal distribution for the transformed data. The choices are\n        'uniform' (default) or 'normal'.\n\n    ignore_implicit_zeros : bool, optional (default=False)\n        Only applies to sparse matrices. If True, the sparse entries of the\n        matrix are discarded to compute the quantile statistics. If False,\n        these entries are treated as zeros.\n\n    subsample : int, optional (default=1e5)\n        Maximum number of samples used to estimate the quantiles for\n        computational efficiency. Note that the subsampling procedure may\n        differ for value-identical sparse and dense matrices.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by np.random. Note that this is used by subsampling and smoothing\n        noise.\n\n    copy : boolean, optional, (default=\"warn\")\n        Set to False to perform inplace transformation and avoid a copy (if the\n        input is already a numpy array). If True, a copy of `X` is transformed,\n        leaving the original `X` unchanged\n\n        .. deprecated:: 0.21\n            The default value of parameter `copy` will be changed from False\n            to True in 0.23. The current default of False is being changed to\n            make it more consistent with the default `copy` values of other\n            functions in :mod:`sklearn.preprocessing.data`. Furthermore, the\n            current default of False may have unexpected side effects by\n            modifying the value of `X` inplace\n\n    Returns\n    -------\n    Xt : ndarray or sparse matrix, shape (n_samples, n_features)\n        The transformed data.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.preprocessing import quantile_transform\n    >>> rng = np.random.RandomState(0)\n    >>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)\n    >>> quantile_transform(X, n_quantiles=10, random_state=0, copy=True)\n    ... # doctest: +ELLIPSIS\n    array([...])\n\n    See also\n    --------\n    QuantileTransformer : Performs quantile-based scaling using the\n        ``Transformer`` API (e.g. as part of a preprocessing\n        :class:`sklearn.pipeline.Pipeline`).\n    power_transform : Maps data to a normal distribution using a\n        power transformation.\n    scale : Performs standardization that is faster, but less robust\n        to outliers.\n    robust_scale : Performs robust standardization that removes the influence\n        of outliers but does not put outliers and inliers on the same scale.\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded in fit, and maintained in\n    transform.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n    ",inputs:[{name:"X",docstring:"The data to transform.",param_type:["array"],expected_shape:null,is_optional:!1,default_value:null,options:null},{name:"axis",docstring:"Axis used to compute the means and standard deviations along. If 0, transform each feature, otherwise (if 1) transform each sample.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"0",options:null},{name:"n_quantiles",docstring:"Number of quantiles to be computed. It corresponds to the number of landmarks used to discretize the cumulative distribution function. If n_quantiles is larger than the number of samples, n_quantiles is set to the number of samples as a larger number of quantiles does not give a better approximation of the cumulative distribution function estimator.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"1000 or n_samples",options:null},{name:"output_distribution",docstring:"Marginal distribution for the transformed data. The choices are 'uniform' (default) or 'normal'.",param_type:["str"],expected_shape:null,is_optional:!0,default_value:"uniform",options:null},{name:"ignore_implicit_zeros",docstring:"Only applies to sparse matrices. If True, the sparse entries of the matrix are discarded to compute the quantile statistics. If False, these entries are treated as zeros.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"False",options:null},{name:"subsample",docstring:"Maximum number of samples used to estimate the quantiles for computational efficiency. Note that the subsampling procedure may differ for value-identical sparse and dense matrices.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"1e5",options:null},{name:"random_state",docstring:"If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Note that this is used by subsampling and smoothing noise.",param_type:["int",null],expected_shape:null,is_optional:!0,default_value:"None",options:null},{name:"copy",docstring:"Set to False to perform inplace transformation and avoid a copy (if the input is already a numpy array). If True, a copy of `X` is transformed, leaving the original `X` unchanged  .. deprecated:: 0.21     The default value of parameter `copy` will be changed from False     to True in 0.23. The current default of False is being changed to     make it more consistent with the default `copy` values of other     functions in :mod:`sklearn.preprocessing.data`. Furthermore, the     current default of False may have unexpected side effects by     modifying the value of `X` inplace",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:'"warn"',options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"Xt",docstring:"The transformed data.",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"robust_scale",docstring:"Standardize a dataset along any axis\n\n    Center to the median and component wise scale\n    according to the interquartile range.\n\n    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n\n    Parameters\n    ----------\n    X : array-like\n        The data to center and scale.\n\n    axis : int (0 by default)\n        axis used to compute the medians and IQR along. If 0,\n        independently scale each feature, otherwise (if 1) scale\n        each sample.\n\n    with_centering : boolean, True by default\n        If True, center the data before scaling.\n\n    with_scaling : boolean, True by default\n        If True, scale the data to unit variance (or equivalently,\n        unit standard deviation).\n\n    quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0\n        Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR\n        Quantile range used to calculate ``scale_``.\n\n        .. versionadded:: 0.18\n\n    copy : boolean, optional, default is True\n        set to False to perform inplace row normalization and avoid a\n        copy (if the input is already a numpy array or a scipy.sparse\n        CSR matrix and if axis is 1).\n\n    Notes\n    -----\n    This implementation will refuse to center scipy.sparse matrices\n    since it would make them non-sparse and would potentially crash the\n    program with memory exhaustion problems.\n\n    Instead the caller is expected to either set explicitly\n    `with_centering=False` (in that case, only variance scaling will be\n    performed on the features of the CSR matrix) or to call `X.toarray()`\n    if he/she expects the materialized dense array to fit in memory.\n\n    To avoid memory copy the caller should pass a CSR matrix.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\n    See also\n    --------\n    RobustScaler: Performs centering and scaling using the ``Transformer`` API\n        (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).\n    ",inputs:[{name:"X",docstring:"The data to center and scale.",param_type:["array"],expected_shape:null,is_optional:!1,default_value:null,options:null},{name:"axis",docstring:"axis used to compute the medians and IQR along. If 0, independently scale each feature, otherwise (if 1) scale each sample.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"",options:null},{name:"with_centering",docstring:"If True, center the data before scaling.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"",options:null},{name:"with_scaling",docstring:"If True, scale the data to unit variance (or equivalently, unit standard deviation).",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"",options:null},{name:"quantile_range",docstring:"Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR Quantile range used to calculate ``scale_``.  .. versionadded:: 0.18",param_type:["tuple"],expected_shape:null,is_optional:!1,default_value:null,options:null},{name:"copy",docstring:"set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR matrix and if axis is 1).",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"scale",docstring:"Standardize a dataset along any axis\n\n    Center to the mean and component wise scale to unit variance.\n\n    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}\n        The data to center and scale.\n\n    axis : int (0 by default)\n        axis used to compute the means and standard deviations along. If 0,\n        independently standardize each feature, otherwise (if 1) standardize\n        each sample.\n\n    with_mean : boolean, True by default\n        If True, center the data before scaling.\n\n    with_std : boolean, True by default\n        If True, scale the data to unit variance (or equivalently,\n        unit standard deviation).\n\n    copy : boolean, optional, default True\n        set to False to perform inplace row normalization and avoid a\n        copy (if the input is already a numpy array or a scipy.sparse\n        CSC matrix and if axis is 1).\n\n    Notes\n    -----\n    This implementation will refuse to center scipy.sparse matrices\n    since it would make them non-sparse and would potentially crash the\n    program with memory exhaustion problems.\n\n    Instead the caller is expected to either set explicitly\n    `with_mean=False` (in that case, only variance scaling will be\n    performed on the features of the CSC matrix) or to call `X.toarray()`\n    if he/she expects the materialized dense array to fit in memory.\n\n    To avoid memory copy the caller should pass a CSC matrix.\n\n    NaNs are treated as missing values: disregarded to compute the statistics,\n    and maintained during the data transformation.\n\n    We use a biased estimator for the standard deviation, equivalent to\n    `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n    affect model performance.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\n    See also\n    --------\n    StandardScaler: Performs scaling to unit variance using the``Transformer`` API\n        (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).\n\n    ",inputs:[{name:"X",docstring:"The data to center and scale.",param_type:["LIST_VALID_OPTIONS","array"],expected_shape:null,is_optional:!1,default_value:null,options:["array-like","sparse matrix"]},{name:"axis",docstring:"axis used to compute the means and standard deviations along. If 0, independently standardize each feature, otherwise (if 1) standardize each sample.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"",options:null},{name:"with_mean",docstring:"If True, center the data before scaling.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"",options:null},{name:"with_std",docstring:"If True, scale the data to unit variance (or equivalently, unit standard deviation).",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"",options:null},{name:"copy",docstring:"set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSC matrix and if axis is 1).",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]}],nodes:[{name:"MaxAbsScaler",docstring:"Scale each feature by its maximum absolute value.\n\n    This estimator scales and translates each feature individually such\n    that the maximal absolute value of each feature in the\n    training set will be 1.0. It does not shift/center the data, and\n    thus does not destroy any sparsity.",inputs:[{name:"copy",docstring:"Set to False to perform inplace scaling and avoid a copy (if the input is already a numpy array).",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null}],outputs:[{name:"scale_",docstring:"Per feature relative scaling of the data.",param_type:["array"],returned:!1},{name:"max_abs_",docstring:"Per feature maximum absolute value.",param_type:["array"],returned:!1},{name:"n_samples_seen_",docstring:"The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across ``partial_fit`` calls.",param_type:["int"],returned:!1}],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_get_param_names",docstring:"Get parameter names for the estimator",inputs:[],outputs:[]},{name:"_get_tags",docstring:"None",inputs:[],outputs:[]},{name:"_more_tags",docstring:"None",inputs:[],outputs:[]},{name:"_reset",docstring:"Reset internal data-dependent state of the scaler, if necessary.\n\n        __init__ parameters are not touched.\n        ",inputs:[],outputs:[]},{name:"fit",docstring:"Compute the maximum absolute value to be used for later scaling.",inputs:[{name:"X",docstring:"The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"fit_transform",docstring:"Fit to data, then transform it.\n\n        Fits transformer to X and y with optional parameters fit_params\n        and returns a transformed version of X.",inputs:[{name:"X",docstring:"Training set.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"y",docstring:"Target values.",param_type:["array"],expected_shape:"[n_samples]",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"X_new",docstring:"Transformed array.",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"inverse_transform",docstring:"Scale back the data to the original representation",inputs:[{name:"X",docstring:"The data that should be transformed back.",param_type:["array"],expected_shape:null,is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"partial_fit",docstring:"Online computation of max absolute value of X for later scaling.\n        All of X is processed as a single batch. This is intended for cases\n        when `fit` is not feasible due to very large number of `n_samples`\n        or because X is read from a continuous stream.",inputs:[{name:"X",docstring:"The data used to compute the mean and standard deviation used for later scaling along the features axis.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"y",docstring:"Ignored",param_type:[null],expected_shape:null,is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"transform",docstring:"Scale the data",inputs:[{name:"X",docstring:"The data that should be scaled.",param_type:["array"],expected_shape:null,is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]}],nodes:[]},{name:"MinMaxScaler",docstring:"Transforms features by scaling each feature to a given range.\n\n    This estimator scales and translates each feature individually such\n    that it is in the given range on the training set, e.g. between\n    zero and one.\n\n    The transformation is given by::\n\n        X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n        X_scaled = X_std * (max - min) + min\n\n    where min, max = feature_range.\n\n    The transformation is calculated as::\n\n        X_scaled = scale * X + min - X.min(axis=0) * scale\n        where scale = (max - min) / (X.max(axis=0) - X.min(axis=0))\n\n    This transformation is often used as an alternative to zero mean,\n    unit variance scaling.",inputs:[{name:"feature_range",docstring:"Desired range of transformed data.",param_type:["tuple"],expected_shape:null,is_optional:!0,default_value:"(0, 1)",options:null},{name:"copy",docstring:"Set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array).",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null}],outputs:[{name:"min_",docstring:"Per feature adjustment for minimum. Equivalent to ``min - X.min(axis=0) * self.scale_``",param_type:["array"],returned:!1},{name:"scale_",docstring:"Per feature relative scaling of the data. Equivalent to ``(max - min) / (X.max(axis=0) - X.min(axis=0))``",param_type:["array"],returned:!1},{name:"data_min_",docstring:"Per feature minimum seen in the data",param_type:["array"],returned:!1},{name:"data_max_",docstring:"Per feature maximum seen in the data",param_type:["array"],returned:!1},{name:"data_range_",docstring:"Per feature range ``(data_max_ - data_min_)`` seen in the data",param_type:["array"],returned:!1}],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_get_param_names",docstring:"Get parameter names for the estimator",inputs:[],outputs:[]},{name:"_get_tags",docstring:"None",inputs:[],outputs:[]},{name:"_more_tags",docstring:"None",inputs:[],outputs:[]},{name:"_reset",docstring:"Reset internal data-dependent state of the scaler, if necessary.\n\n        __init__ parameters are not touched.\n        ",inputs:[],outputs:[]},{name:"fit",docstring:"Compute the minimum and maximum to be used for later scaling.",inputs:[{name:"X",docstring:"The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"fit_transform",docstring:"Fit to data, then transform it.\n\n        Fits transformer to X and y with optional parameters fit_params\n        and returns a transformed version of X.",inputs:[{name:"X",docstring:"Training set.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"y",docstring:"Target values.",param_type:["array"],expected_shape:"[n_samples]",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"X_new",docstring:"Transformed array.",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"inverse_transform",docstring:"Undo the scaling of X according to feature_range.",inputs:[{name:"X",docstring:"Input data that will be transformed. It cannot be sparse.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"partial_fit",docstring:"Online computation of min and max on X for later scaling.\n        All of X is processed as a single batch. This is intended for cases\n        when `fit` is not feasible due to very large number of `n_samples`\n        or because X is read from a continuous stream.",inputs:[{name:"X",docstring:"The data used to compute the mean and standard deviation used for later scaling along the features axis.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"y",docstring:"Ignored",param_type:[null],expected_shape:null,is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"transform",docstring:"Scaling features of X according to feature_range.",inputs:[{name:"X",docstring:"Input data that will be transformed.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]}],nodes:[]},{name:"Normalizer",docstring:"Normalize samples individually to unit norm.\n\n    Each sample (i.e. each row of the data matrix) with at least one\n    non zero component is rescaled independently of other samples so\n    that its norm (l1 or l2) equals one.\n\n    This transformer is able to work both with dense numpy arrays and\n    scipy.sparse matrix (use CSR format if you want to avoid the burden of\n    a copy / conversion).\n\n    Scaling inputs to unit norms is a common operation for text\n    classification or clustering for instance. For instance the dot\n    product of two l2-normalized TF-IDF vectors is the cosine similarity\n    of the vectors and is the base similarity metric for the Vector\n    Space Model commonly used by the Information Retrieval community.",inputs:[{name:"norm",docstring:"The norm to use to normalize each non zero sample.",param_type:["LIST_VALID_OPTIONS"],expected_shape:null,is_optional:!0,default_value:"l2",options:["l1","l2","max"]},{name:"copy",docstring:"set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR matrix).",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null}],outputs:[],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_get_param_names",docstring:"Get parameter names for the estimator",inputs:[],outputs:[]},{name:"_get_tags",docstring:"None",inputs:[],outputs:[]},{name:"_more_tags",docstring:"None",inputs:[],outputs:[]},{name:"fit",docstring:"Do nothing and return the estimator unchanged\n\n        This method is just there to implement the usual API and hence\n        work in pipelines.",inputs:[{name:"X",docstring:"Data",param_type:["array"],expected_shape:null,is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"fit_transform",docstring:"Fit to data, then transform it.\n\n        Fits transformer to X and y with optional parameters fit_params\n        and returns a transformed version of X.",inputs:[{name:"X",docstring:"Training set.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"y",docstring:"Target values.",param_type:["array"],expected_shape:"[n_samples]",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"X_new",docstring:"Transformed array.",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"transform",docstring:"Scale each non zero row of X to unit norm",inputs:[{name:"X",docstring:"The data to normalize, row by row. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"copy",docstring:"Copy the input X or not.",param_type:["bool",null],expected_shape:null,is_optional:!0,default_value:"None",options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]}],nodes:[]},{name:"OneHotEncoder",docstring:"Encode categorical integer features as a one-hot numeric array.\n\n    The input to this transformer should be an array-like of integers or\n    strings, denoting the values taken on by categorical (discrete) features.\n    The features are encoded using a one-hot (aka 'one-of-K' or 'dummy')\n    encoding scheme. This creates a binary column for each category and\n    returns a sparse matrix or dense array.\n\n    By default, the encoder derives the categories based on the unique values\n    in each feature. Alternatively, you can also specify the `categories`\n    manually.\n    The OneHotEncoder previously assumed that the input features take on\n    values in the range [0, max(values)). This behaviour is deprecated.\n\n    This encoding is needed for feeding categorical data to many scikit-learn\n    estimators, notably linear models and SVMs with the standard kernels.\n\n    Note: a one-hot encoding of y labels should use a LabelBinarizer\n    instead.",inputs:[{name:"categories",docstring:"Categories (unique values) per feature:  - 'auto' : Determine categories automatically from the training data. - list : ``categories[i]`` holds the categories expected in the ith   column. The passed categories should not mix strings and numeric   values within a single feature, and should be sorted in case of   numeric values.  The used categories can be found in the ``categories_`` attribute.",param_type:["str","array","list"],expected_shape:null,is_optional:!0,default_value:"auto.",options:null},{name:"drop",docstring:"Specifies a methodology to use to drop one of the categories per feature. This is useful in situations where perfectly collinear features cause problems, such as when feeding the resulting data into a neural network or an unregularized regression.  - None : retain all features (the default). - 'first' : drop the first category in each feature. If only one   category is present, the feature will be dropped entirely. - array : ``drop[i]`` is the category in feature ``X[:, i]`` that   should be dropped.",param_type:["array","list","str"],expected_shape:"(n_features,)",is_optional:!0,default_value:"None",options:null},{name:"sparse",docstring:"Will return sparse matrix if set True else will return an array.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"dtype",docstring:"Desired dtype of output.",param_type:["float"],expected_shape:null,is_optional:!0,default_value:"np.float",options:null},{name:"handle_unknown",docstring:"Whether to raise an error or ignore if an unknown categorical feature is present during transform (default is to raise). When this parameter is set to 'ignore' and an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. In the inverse transform, an unknown category will be denoted as None.",param_type:["LIST_VALID_OPTIONS"],expected_shape:null,is_optional:!0,default_value:"error",options:["error","ignore"]},{name:"n_values",docstring:"Number of values per feature.  - 'auto' : determine value range from training data. - int : number of categorical values per feature.         Each feature value should be in ``range(n_values)`` - array : ``n_values[i]`` is the number of categorical values in           ``X[:, i]``. Each feature value should be           in ``range(n_values[i])``",param_type:["array","int","str"],expected_shape:null,is_optional:!0,default_value:"auto",options:null},{name:"categorical_features",docstring:"Specify what features are treated as categorical.  - 'all': All features are treated as categorical. - array of indices: Array of categorical feature indices. - mask: Array of length n_features and with dtype=bool.  Non-categorical features are always stacked to the right of the matrix.",param_type:["array","str"],expected_shape:null,is_optional:!0,default_value:"all",options:null}],outputs:[{name:"categories_",docstring:"The categories of each feature determined during fitting (in order of the features in X and corresponding with the output of ``transform``). This includes the category specified in ``drop`` (if any).",param_type:["array","list"],returned:!1},{name:"drop_idx_",docstring:"``drop_idx_[i]`` is the index in ``categories_[i]`` of the category to be dropped for each feature. None if all the transformed features will be retained.",param_type:["array"],returned:!1},{name:"active_features_",docstring:"Indices for active features, meaning values that actually occur in the training set. Only available when n_values is ``'auto'``.  .. deprecated:: 0.20     The ``active_features_`` attribute was deprecated in version     0.20 and will be removed in 0.22.",param_type:["array"],returned:!1},{name:"feature_indices_",docstring:"Indices to feature ranges. Feature ``i`` in the original data is mapped to features from ``feature_indices_[i]`` to ``feature_indices_[i+1]`` (and then potentially masked by ``active_features_`` afterwards)  .. deprecated:: 0.20     The ``feature_indices_`` attribute was deprecated in version     0.20 and will be removed in 0.22.",param_type:["array"],returned:!1},{name:"n_values_",docstring:"Maximum number of values per feature.  .. deprecated:: 0.20     The ``n_values_`` attribute was deprecated in version     0.20 and will be removed in 0.22.",param_type:["array"],returned:!1}],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_check_X",docstring:"\n        Perform custom check_array:\n        - convert list of strings to object dtype\n        - check for missing values for object dtype data (check_array does\n          not do that)\n        - return list of features (arrays): this list of features is\n          constructed feature by feature to preserve the data types\n          of pandas DataFrame columns, as otherwise information is lost\n          and cannot be used, eg for the `categories_` attribute.\n\n        ",inputs:[],outputs:[]},{name:"_compute_drop_idx",docstring:"None",inputs:[],outputs:[]},{name:"_fit",docstring:"None",inputs:[],outputs:[]},{name:"_get_feature",docstring:"None",inputs:[],outputs:[]},{name:"_get_param_names",docstring:"Get parameter names for the estimator",inputs:[],outputs:[]},{name:"_get_tags",docstring:"None",inputs:[],outputs:[]},{name:"_handle_deprecations",docstring:"None",inputs:[],outputs:[]},{name:"_legacy_fit_transform",docstring:"Assumes X contains only categorical features.",inputs:[],outputs:[]},{name:"_legacy_transform",docstring:"Assumes X contains only categorical features.",inputs:[],outputs:[]},{name:"_transform",docstring:"None",inputs:[],outputs:[]},{name:"_transform_new",docstring:"New implementation assuming categorical input",inputs:[],outputs:[]},{name:"_validate_keywords",docstring:"None",inputs:[],outputs:[]},{name:"fit",docstring:"Fit OneHotEncoder to X.",inputs:[{name:"X",docstring:"The data to determine the categories of each feature.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"",docstring:"",param_type:[null],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"fit_transform",docstring:"Fit OneHotEncoder to X, then transform X.\n\n        Equivalent to fit(X).transform(X) but more convenient.",inputs:[{name:"X",docstring:"The data to encode.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"X_out",docstring:"Transformed input.",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"get_feature_names",docstring:"Return feature names for output features.",inputs:[{name:"input_features",docstring:'String names for input features if available. By default, "x0", "x1", ... "xn_features" is used.',param_type:["list"],expected_shape:null,is_optional:!0,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"output_feature_names",docstring:"",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"inverse_transform",docstring:"Convert the back data to the original representation.\n\n        In case unknown categories are encountered (all zeros in the\n        one-hot encoding), ``None`` is used to represent this category.",inputs:[{name:"X",docstring:"The transformed data.",param_type:["array"],expected_shape:"[n_samples, n_encoded_features]",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"X_tr",docstring:"Inverse transformed array.",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"transform",docstring:"Transform X using one-hot encoding.",inputs:[{name:"X",docstring:"The data to encode.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"X_out",docstring:"Transformed input.",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]}],nodes:[]},{name:"StandardScaler",docstring:"Standardize features by removing the mean and scaling to unit variance\n\n    The standard score of a sample `x` is calculated as:\n\n        z = (x - u) / s\n\n    where `u` is the mean of the training samples or zero if `with_mean=False`,\n    and `s` is the standard deviation of the training samples or one if\n    `with_std=False`.\n\n    Centering and scaling happen independently on each feature by computing\n    the relevant statistics on the samples in the training set. Mean and\n    standard deviation are then stored to be used on later data using the\n    `transform` method.\n\n    Standardization of a dataset is a common requirement for many\n    machine learning estimators: they might behave badly if the\n    individual features do not more or less look like standard normally\n    distributed data (e.g. Gaussian with 0 mean and unit variance).\n\n    For instance many elements used in the objective function of\n    a learning algorithm (such as the RBF kernel of Support Vector\n    Machines or the L1 and L2 regularizers of linear models) assume that\n    all features are centered around 0 and have variance in the same\n    order. If a feature has a variance that is orders of magnitude larger\n    that others, it might dominate the objective function and make the\n    estimator unable to learn from other features correctly as expected.\n\n    This scaler can also be applied to sparse CSR or CSC matrices by passing\n    `with_mean=False` to avoid breaking the sparsity structure of the data.",inputs:[{name:"copy",docstring:"If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"with_mean",docstring:"If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"with_std",docstring:"If True, scale the data to unit variance (or equivalently, unit standard deviation).",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null}],outputs:[{name:"scale_",docstring:"Per feature relative scaling of the data. This is calculated using `np.sqrt(var_)`. Equal to ``None`` when ``with_std=False``.  .. versionadded:: 0.17    *scale_*",param_type:["array",null],returned:!1},{name:"mean_",docstring:"The mean value for each feature in the training set. Equal to ``None`` when ``with_mean=False``.",param_type:["array",null],returned:!1},{name:"var_",docstring:"The variance for each feature in the training set. Used to compute `scale_`. Equal to ``None`` when ``with_std=False``.",param_type:["array",null],returned:!1},{name:"n_samples_seen_",docstring:"The number of samples processed by the estimator for each feature. If there are not missing samples, the ``n_samples_seen`` will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across ``partial_fit`` calls.",param_type:["array","int"],returned:!1}],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_get_param_names",docstring:"Get parameter names for the estimator",inputs:[],outputs:[]},{name:"_get_tags",docstring:"None",inputs:[],outputs:[]},{name:"_more_tags",docstring:"None",inputs:[],outputs:[]},{name:"_reset",docstring:"Reset internal data-dependent state of the scaler, if necessary.\n\n        __init__ parameters are not touched.\n        ",inputs:[],outputs:[]},{name:"fit",docstring:"Compute the mean and std to be used for later scaling.",inputs:[{name:"X",docstring:"The data used to compute the mean and standard deviation used for later scaling along the features axis.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"y",docstring:"Ignored",param_type:[null],expected_shape:null,is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"fit_transform",docstring:"Fit to data, then transform it.\n\n        Fits transformer to X and y with optional parameters fit_params\n        and returns a transformed version of X.",inputs:[{name:"X",docstring:"Training set.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"y",docstring:"Target values.",param_type:["array"],expected_shape:"[n_samples]",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"X_new",docstring:"Transformed array.",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"inverse_transform",docstring:"Scale back the data to the original representation",inputs:[{name:"X",docstring:"The data used to scale along the features axis.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"copy",docstring:"Copy the input X or not.",param_type:["bool",null],expected_shape:null,is_optional:!0,default_value:"None",options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"X_tr",docstring:"Transformed array.",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"partial_fit",docstring:"Online computation of mean and std on X for later scaling.\n        All of X is processed as a single batch. This is intended for cases\n        when `fit` is not feasible due to very large number of `n_samples`\n        or because X is read from a continuous stream.",inputs:[{name:"X",docstring:"The data used to compute the mean and standard deviation used for later scaling along the features axis.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"y",docstring:"Ignored",param_type:[null],expected_shape:null,is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"transform",docstring:"Perform standardization by centering and scaling",inputs:[{name:"X",docstring:"The data used to scale along the features axis.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"copy",docstring:"Copy the input X or not.",param_type:["bool",null],expected_shape:null,is_optional:!0,default_value:"None",options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]}],nodes:[]}],library:"sklearn",module:"preprocessing"}},function(e,t,n){"use strict";t.a={name:"sklearn.svm",docstring:"",inputs:[],outputs:[],node_functions:[{name:"l1_min_c",docstring:"\n    Return the lowest bound for C such that for C in (l1_min_C, infinity)\n    the model is guaranteed not to be empty. This applies to l1 penalized\n    classifiers, such as LinearSVC with penalty='l1' and\n    linear_model.LogisticRegression with penalty='l1'.\n\n    This value is valid if class_weight parameter in fit() is not set.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape = [n_samples, n_features]\n        Training vector, where n_samples in the number of samples and\n        n_features is the number of features.\n\n    y : array, shape = [n_samples]\n        Target vector relative to X\n\n    loss : {'squared_hinge', 'log'}, default 'squared_hinge'\n        Specifies the loss function.\n        With 'squared_hinge' it is the squared hinge loss (a.k.a. L2 loss).\n        With 'log' it is the loss of logistic regression models.\n\n    fit_intercept : bool, default: True\n        Specifies if the intercept should be fitted by the model.\n        It must match the fit() method parameter.\n\n    intercept_scaling : float, default: 1\n        when fit_intercept is True, instance vector x becomes\n        [x, intercept_scaling],\n        i.e. a \"synthetic\" feature with constant value equals to\n        intercept_scaling is appended to the instance vector.\n        It must match the fit() method parameter.\n\n    Returns\n    -------\n    l1_min_c : float\n        minimum value for C\n    ",inputs:[{name:"X",docstring:"Training vector, where n_samples in the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"[n_samples, n_features]",is_optional:!1,default_value:null,options:null},{name:"y",docstring:"Target vector relative to X",param_type:["array"],expected_shape:"[n_samples]",is_optional:!1,default_value:null,options:null},{name:"loss",docstring:"Specifies the loss function. With 'squared_hinge' it is the squared hinge loss (a.k.a. L2 loss). With 'log' it is the loss of logistic regression models.",param_type:["LIST_VALID_OPTIONS"],expected_shape:null,is_optional:!0,default_value:"squared_hinge",options:["squared_hinge","log","default squared_hinge"]},{name:"fit_intercept",docstring:"Specifies if the intercept should be fitted by the model. It must match the fit() method parameter.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"intercept_scaling",docstring:'when fit_intercept is True, instance vector x becomes [x, intercept_scaling], i.e. a "synthetic" feature with constant value equals to intercept_scaling is appended to the instance vector. It must match the fit() method parameter.',param_type:["float"],expected_shape:null,is_optional:!0,default_value:"1",options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"l1_min_c",docstring:"minimum value for C",param_type:["float"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]}],nodes:[{name:"OneClassSVM",docstring:"Unsupervised Outlier Detection.\n\n    Estimate the support of a high-dimensional distribution",inputs:[{name:"kernel",docstring:"Specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable. If none is given, 'rbf' will be used. If a callable is given it is used to precompute the kernel matrix.",param_type:["LIST_VALID_OPTIONS"],expected_shape:null,is_optional:!0,default_value:"rbf",options:["linear","poly","rbf","sigmoid","precomputed"]},{name:"degree",docstring:"Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"3",options:null},{name:"gamma",docstring:"Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.  Current default is 'auto' which uses 1 / n_features, if ``gamma='scale'`` is passed then it uses 1 / (n_features * X.var()) as value of gamma.",param_type:["float"],expected_shape:null,is_optional:!0,default_value:"auto",options:null},{name:"coef0",docstring:"Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'.",param_type:["float"],expected_shape:null,is_optional:!0,default_value:"0.0",options:null},{name:"tol",docstring:"Tolerance for stopping criterion.",param_type:["float"],expected_shape:null,is_optional:!0,default_value:null,options:null},{name:"nu",docstring:"An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1]. By default 0.5 will be taken.",param_type:["float"],expected_shape:null,is_optional:!0,default_value:"0.5",options:null},{name:"shrinking",docstring:"Whether to use the shrinking heuristic.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:null,options:null},{name:"cache_size",docstring:"Specify the size of the kernel cache (in MB).",param_type:["float"],expected_shape:null,is_optional:!0,default_value:null,options:null},{name:"verbose",docstring:"Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"False",options:null},{name:"max_iter",docstring:"Hard limit on iterations within solver, or -1 for no limit.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"-1",options:null}],outputs:[{name:"support_",docstring:"Indices of support vectors.",param_type:["array"],returned:!1},{name:"support_vectors_",docstring:"Support vectors.",param_type:["array"],returned:!1},{name:"dual_coef_",docstring:"Coefficients of the support vectors in the decision function.",param_type:["array"],returned:!1},{name:"coef_",docstring:"Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel.  `coef_` is readonly property derived from `dual_coef_` and `support_vectors_`",param_type:["array"],returned:!1},{name:"intercept_",docstring:"Constant in the decision function.",param_type:["array"],returned:!1},{name:"offset_",docstring:"Offset used to define the decision function from the raw scores. We have the relation: decision_function = score_samples - `offset_`. The offset is the opposite of `intercept_` and is provided for consistency with other outlier detection algorithms.",param_type:["float"],returned:!1}],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_compute_kernel",docstring:"Return the data transformed by a callable kernel",inputs:[],outputs:[]},{name:"_decision_function",docstring:"Evaluates the decision function for the samples in X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        Returns\n        -------\n        X : array-like, shape (n_samples, n_class * (n_class-1) / 2)\n            Returns the decision function of the sample for each class\n            in the model.\n        ",inputs:[{name:"X",docstring:"",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null}],outputs:[{name:"X",docstring:"Returns the decision function of the sample for each class in the model.",param_type:["array"],returned:!0}]},{name:"_dense_decision_function",docstring:"None",inputs:[],outputs:[]},{name:"_dense_fit",docstring:"None",inputs:[],outputs:[]},{name:"_dense_predict",docstring:"None",inputs:[],outputs:[]},{name:"_get_coef",docstring:"None",inputs:[],outputs:[]},{name:"_get_param_names",docstring:"Get parameter names for the estimator",inputs:[],outputs:[]},{name:"_get_tags",docstring:"None",inputs:[],outputs:[]},{name:"_sparse_decision_function",docstring:"None",inputs:[],outputs:[]},{name:"_sparse_fit",docstring:"None",inputs:[],outputs:[]},{name:"_sparse_predict",docstring:"None",inputs:[],outputs:[]},{name:"_validate_for_predict",docstring:"None",inputs:[],outputs:[]},{name:"_validate_targets",docstring:"Validation of y and class_weight.\n\n        Default implementation for SVR and one-class; overridden in BaseSVC.\n        ",inputs:[],outputs:[]},{name:"_warn_from_fit_status",docstring:"None",inputs:[],outputs:[]},{name:"decision_function",docstring:"Signed distance to the separating hyperplane.\n\n        Signed distance is positive for an inlier and negative for an outlier.",inputs:[{name:"X",docstring:"Data.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"dec",docstring:"Returns the decision function of the samples.",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"fit",docstring:"\n        Detects the soft boundary of the set of samples X.",inputs:[{name:"X",docstring:"Set of samples, where n_samples is the number of samples and n_features is the number of features.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null},{name:"sample_weight",docstring:"Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points.",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null,options:null},{name:"y",docstring:"not used, present for API consistency by convention.",param_type:[null],expected_shape:null,is_optional:!0,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"fit_predict",docstring:"Performs fit on X and returns labels for X.\n\n        Returns -1 for outliers and 1 for inliers.",inputs:[{name:"X",docstring:"Input data.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null},{name:"y",docstring:"not used, present for API consistency by convention.",param_type:[null],expected_shape:null,is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"y",docstring:"1 for inliers, -1 for outliers.",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"predict",docstring:"\n        Perform classification on samples in X.\n\n        For a one-class model, +1 or -1 is returned.",inputs:[{name:"X",docstring:'For kernel="precomputed", the expected shape of X is [n_samples_test, n_samples_train]',param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"y_pred",docstring:"Class labels for samples in X.",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"score_samples",docstring:"Raw scoring function of the samples.",inputs:[{name:"X",docstring:"",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"score_samples",docstring:"Returns the (unshifted) scoring function of the samples.",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]}],nodes:[]},{name:"SVC",docstring:"C-Support Vector Classification.\n\n    The implementation is based on libsvm. The fit time scales at least\n    quadratically with the number of samples and may be impractical\n    beyond tens of thousands of samples.\n\n    The multiclass support is handled according to a one-vs-one scheme.\n\n    For details on the precise mathematical formulation of the provided\n    kernel functions and how `gamma`, `coef0` and `degree` affect each\n    other, see the corresponding section in the narrative documentation:\n    :ref:`svm_kernels`.",inputs:[{name:"C",docstring:"Penalty parameter C of the error term.",param_type:["float"],expected_shape:null,is_optional:!0,default_value:"1.0",options:null},{name:"kernel",docstring:"Specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable. If none is given, 'rbf' will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape ``(n_samples, n_samples)``.",param_type:["LIST_VALID_OPTIONS"],expected_shape:null,is_optional:!0,default_value:"rbf",options:["linear","poly","rbf","sigmoid","precomputed"]},{name:"degree",docstring:"Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"3",options:null},{name:"gamma",docstring:"Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.  Current default is 'auto' which uses 1 / n_features, if ``gamma='scale'`` is passed then it uses 1 / (n_features * X.var()) as value of gamma.",param_type:["float","str"],expected_shape:null,is_optional:!0,default_value:"auto",options:null},{name:"coef0",docstring:"Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'.",param_type:["float"],expected_shape:null,is_optional:!0,default_value:"0.0",options:null},{name:"shrinking",docstring:"Whether to use the shrinking heuristic.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"probability",docstring:"Whether to enable probability estimates. This must be enabled prior to calling `fit`, and will slow down that method.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"False",options:null},{name:"tol",docstring:"Tolerance for stopping criterion.",param_type:["float"],expected_shape:null,is_optional:!0,default_value:"1e-3",options:null},{name:"class_weight",docstring:'Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The "balanced" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``',param_type:["str","dict"],expected_shape:null,is_optional:!0,default_value:null,options:null},{name:"verbose",docstring:"Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"False",options:null},{name:"max_iter",docstring:"Hard limit on iterations within solver, or -1 for no limit.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"-1",options:null},{name:"decision_function_shape",docstring:"Whether to return a one-vs-rest ('ovr') decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one ('ovo') decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one ('ovo') is always used as multi-class strategy.",param_type:["LIST_VALID_OPTIONS"],expected_shape:null,is_optional:!0,default_value:"ovr",options:["ovr","ovo"]}],outputs:[{name:"support_",docstring:"Indices of support vectors.",param_type:["array"],returned:!1},{name:"support_vectors_",docstring:"Support vectors.",param_type:["array"],returned:!1},{name:"n_support_",docstring:"Number of support vectors for each class.",param_type:["array"],returned:!1},{name:"dual_coef_",docstring:"Coefficients of the support vector in the decision function. For multiclass, coefficient for all 1-vs-1 classifiers. The layout of the coefficients in the multiclass case is somewhat non-trivial.",param_type:["array"],returned:!1},{name:"coef_",docstring:"Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel.  `coef_` is a readonly property derived from `dual_coef_` and `support_vectors_`.",param_type:["array"],returned:!1},{name:"intercept_",docstring:"Constants in decision function.",param_type:["array"],returned:!1},{name:"fit_status_",docstring:"0 if correctly fitted, 1 otherwise (will raise warning)",param_type:["int"],returned:!1},{name:"probA_",docstring:"",param_type:["array"],returned:!1},{name:"probB_",docstring:"If probability=True, the parameters learned in Platt scaling to produce probability estimates from decision values. If probability=False, an empty array. Platt scaling uses the logistic function ``1 / (1 + exp(decision_value * probA_ + probB_))`` where ``probA_`` and ``probB_`` are learned from the dataset.",param_type:["array"],returned:!1}],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_check_proba",docstring:"None",inputs:[],outputs:[]},{name:"_compute_kernel",docstring:"Return the data transformed by a callable kernel",inputs:[],outputs:[]},{name:"_decision_function",docstring:"Evaluates the decision function for the samples in X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        Returns\n        -------\n        X : array-like, shape (n_samples, n_class * (n_class-1) / 2)\n            Returns the decision function of the sample for each class\n            in the model.\n        ",inputs:[{name:"X",docstring:"",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null}],outputs:[{name:"X",docstring:"Returns the decision function of the sample for each class in the model.",param_type:["array"],returned:!0}]},{name:"_dense_decision_function",docstring:"None",inputs:[],outputs:[]},{name:"_dense_fit",docstring:"None",inputs:[],outputs:[]},{name:"_dense_predict",docstring:"None",inputs:[],outputs:[]},{name:"_dense_predict_proba",docstring:"None",inputs:[],outputs:[]},{name:"_get_coef",docstring:"None",inputs:[],outputs:[]},{name:"_get_param_names",docstring:"Get parameter names for the estimator",inputs:[],outputs:[]},{name:"_get_tags",docstring:"None",inputs:[],outputs:[]},{name:"_predict_log_proba",docstring:"None",inputs:[],outputs:[]},{name:"_predict_proba",docstring:"None",inputs:[],outputs:[]},{name:"_sparse_decision_function",docstring:"None",inputs:[],outputs:[]},{name:"_sparse_fit",docstring:"None",inputs:[],outputs:[]},{name:"_sparse_predict",docstring:"None",inputs:[],outputs:[]},{name:"_sparse_predict_proba",docstring:"None",inputs:[],outputs:[]},{name:"_validate_for_predict",docstring:"None",inputs:[],outputs:[]},{name:"_validate_targets",docstring:"None",inputs:[],outputs:[]},{name:"_warn_from_fit_status",docstring:"None",inputs:[],outputs:[]},{name:"decision_function",docstring:"Evaluates the decision function for the samples in X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        Returns\n        -------\n        X : array-like, shape (n_samples, n_classes * (n_classes-1) / 2)\n            Returns the decision function of the sample for each class\n            in the model.\n            If decision_function_shape='ovr', the shape is (n_samples,\n            n_classes).\n\n        Notes\n        -----\n        If decision_function_shape='ovo', the function values are proportional\n        to the distance of the samples X to the separating hyperplane. If the\n        exact distances are required, divide the function values by the norm of\n        the weight vector (``coef_``). See also `this question\n        <https://stats.stackexchange.com/questions/14876/\n        interpreting-distance-from-hyperplane-in-svm>`_ for further details.\n        If decision_function_shape='ovr', the decision function is a monotonic\n        transformation of ovo decision function.\n        ",inputs:[{name:"X",docstring:"Data.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"X",docstring:"Returns the decision function of the sample for each class in the model. If decision_function_shape='ovr', the shape is (n_samples, n_classes), (n_samples, n_classes * (n_classes-1) / 2) otherwise",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"fit",docstring:"Fit the SVM model according to the given training data.",inputs:[{name:"X",docstring:'Training vectors, where n_samples is the number of samples and n_features is the number of features. For kernel="precomputed", the expected shape of X is (n_samples, n_samples).',param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null},{name:"y",docstring:"Target values (class labels in classification, real numbers in regression)",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null,options:null},{name:"sample_weight",docstring:"Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points.",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"predict",docstring:"Perform classification on samples in X.\n\n        For an one-class model, +1 or -1 is returned.",inputs:[{name:"X",docstring:'For kernel="precomputed", the expected shape of X is [n_samples_test, n_samples_train]',param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"y_pred",docstring:"Class labels for samples in X.",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"score",docstring:"Returns the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.",inputs:[{name:"X",docstring:"Test samples.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null},{name:"y",docstring:"True labels for X.",param_type:["array"],expected_shape:"(n_samples) or (n_samples, n_outputs)",is_optional:!1,default_value:null,options:null},{name:"sample_weight",docstring:"Sample weights.",param_type:["array"],expected_shape:"[n_samples], optional",is_optional:!0,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"score",docstring:"Mean accuracy of self.predict(X) wrt. y.",param_type:["float"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]}],nodes:[]},{name:"SVR",docstring:"Epsilon-Support Vector Regression.\n\n    The free parameters in the model are C and epsilon.\n\n    The implementation is based on libsvm. The fit time complexity\n    is more than quadratic with the number of samples which makes it hard\n    to scale to datasets with more than a couple of 10000 samples. ",inputs:[{name:"kernel",docstring:"Specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable. If none is given, 'rbf' will be used. If a callable is given it is used to precompute the kernel matrix.",param_type:["LIST_VALID_OPTIONS"],expected_shape:null,is_optional:!0,default_value:"rbf",options:["linear","poly","rbf","sigmoid","precomputed"]},{name:"degree",docstring:"Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"3",options:null},{name:"gamma",docstring:"Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.  Current default is 'auto' which uses 1 / n_features, if ``gamma='scale'`` is passed then it uses 1 / (n_features * X.var()) as value of gamma. ",param_type:["float"],expected_shape:null,is_optional:!0,default_value:"auto",options:null},{name:"coef0",docstring:"Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'.",param_type:["float"],expected_shape:null,is_optional:!0,default_value:"0.0",options:null},{name:"tol",docstring:"Tolerance for stopping criterion.",param_type:["float"],expected_shape:null,is_optional:!0,default_value:"1e-3",options:null},{name:"C",docstring:"Penalty parameter C of the error term.",param_type:["float"],expected_shape:null,is_optional:!0,default_value:"1.0",options:null},{name:"epsilon",docstring:"Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value.",param_type:["float"],expected_shape:null,is_optional:!0,default_value:"0.1",options:null},{name:"shrinking",docstring:"Whether to use the shrinking heuristic.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"cache_size",docstring:"Specify the size of the kernel cache (in MB).",param_type:["float"],expected_shape:null,is_optional:!0,default_value:null,options:null},{name:"verbose",docstring:"Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"False",options:null},{name:"max_iter",docstring:"Hard limit on iterations within solver, or -1 for no limit.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"-1",options:null}],outputs:[{name:"support_",docstring:"Indices of support vectors.",param_type:["array"],returned:!1},{name:"support_vectors_",docstring:"Support vectors.",param_type:["array"],returned:!1},{name:"dual_coef_",docstring:"Coefficients of the support vector in the decision function.",param_type:["array"],returned:!1},{name:"coef_",docstring:"Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel.  `coef_` is readonly property derived from `dual_coef_` and `support_vectors_`.",param_type:["array"],returned:!1},{name:"intercept_",docstring:"Constants in decision function.",param_type:["array"],returned:!1}],node_functions:[{name:"__init__",docstring:"None",inputs:[],outputs:[]},{name:"_compute_kernel",docstring:"Return the data transformed by a callable kernel",inputs:[],outputs:[]},{name:"_decision_function",docstring:"Evaluates the decision function for the samples in X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        Returns\n        -------\n        X : array-like, shape (n_samples, n_class * (n_class-1) / 2)\n            Returns the decision function of the sample for each class\n            in the model.\n        ",inputs:[{name:"X",docstring:"",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null}],outputs:[{name:"X",docstring:"Returns the decision function of the sample for each class in the model.",param_type:["array"],returned:!0}]},{name:"_dense_decision_function",docstring:"None",inputs:[],outputs:[]},{name:"_dense_fit",docstring:"None",inputs:[],outputs:[]},{name:"_dense_predict",docstring:"None",inputs:[],outputs:[]},{name:"_get_coef",docstring:"None",inputs:[],outputs:[]},{name:"_get_param_names",docstring:"Get parameter names for the estimator",inputs:[],outputs:[]},{name:"_get_tags",docstring:"None",inputs:[],outputs:[]},{name:"_sparse_decision_function",docstring:"None",inputs:[],outputs:[]},{name:"_sparse_fit",docstring:"None",inputs:[],outputs:[]},{name:"_sparse_predict",docstring:"None",inputs:[],outputs:[]},{name:"_validate_for_predict",docstring:"None",inputs:[],outputs:[]},{name:"_validate_targets",docstring:"Validation of y and class_weight.\n\n        Default implementation for SVR and one-class; overridden in BaseSVC.\n        ",inputs:[],outputs:[]},{name:"_warn_from_fit_status",docstring:"None",inputs:[],outputs:[]},{name:"fit",docstring:"Fit the SVM model according to the given training data.",inputs:[{name:"X",docstring:'Training vectors, where n_samples is the number of samples and n_features is the number of features. For kernel="precomputed", the expected shape of X is (n_samples, n_samples).',param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null},{name:"y",docstring:"Target values (class labels in classification, real numbers in regression)",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null,options:null},{name:"sample_weight",docstring:"Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points.",param_type:["array"],expected_shape:"(n_samples,)",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"predict",docstring:"Perform regression on samples in X.\n\n        For an one-class model, +1 (inlier) or -1 (outlier) is returned.",inputs:[{name:"X",docstring:'For kernel="precomputed", the expected shape of X is (n_samples_test, n_samples_train).',param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"y_pred",docstring:"",param_type:["array"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]},{name:"score",docstring:"Returns the coefficient of determination R^2 of the prediction.\n\n        The coefficient R^2 is defined as (1 - u/v), where u is the residual\n        sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n        sum of squares ((y_true - y_true.mean()) ** 2).sum().\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always\n        predicts the expected value of y, disregarding the input features,\n        would get a R^2 score of 0.0.",inputs:[{name:"X",docstring:"Test samples. For some estimators this may be a precomputed kernel matrix instead, shape = (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator.",param_type:["array"],expected_shape:"(n_samples, n_features)",is_optional:!1,default_value:null,options:null},{name:"y",docstring:"True values for X.",param_type:["array"],expected_shape:"(n_samples) or (n_samples, n_outputs)",is_optional:!1,default_value:null,options:null},{name:"sample_weight",docstring:"Sample weights.",param_type:["array"],expected_shape:"[n_samples], optional",is_optional:!0,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"score",docstring:"R^2 of self.predict(X) wrt. y.",param_type:["float"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]}],nodes:[]}],library:"sklearn",module:"svm"}},function(e,t,n){"use strict";t.a={name:"sklearn.linear_model",outputs:[],docstring:"",inputs:[],module:"linear_model",node_functions:[{name:"enet_path",outputs:[{name:"alphas",returned:!0,param_type:["array"],docstring:"The alphas along the path where models are computed."},{name:"coefs",returned:!0,param_type:["array"],docstring:"Coefficients along the path."},{name:"dual_gaps",returned:!0,param_type:["array"],docstring:"The dual gaps at the end of the optimization for each alpha."},{name:"n_iters",returned:!0,param_type:["array"],docstring:"The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha. (Is returned when ``return_n_iter`` is set to True)."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Compute elastic net path with coordinate descent\n\n    The elastic net optimization function varies for mono and multi-outputs.\n\n    For mono-output tasks it is::\n\n        1 / (2 * n_samples) * ||y - Xw||^2_2\n        + alpha * l1_ratio * ||w||_1\n        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\n    For multi-output tasks it is::\n\n        (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n        + alpha * l1_ratio * ||W||_21\n        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\n    Where::\n\n        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\n    i.e. the sum of norm of each row.\n\n    Read more in the :ref:`User Guide <elastic_net>`.\n\n    Parameters\n    ----------\n    X : {array-like}, shape (n_samples, n_features)\n        Training data. Pass directly as Fortran-contiguous data to avoid\n        unnecessary memory duplication. If ``y`` is mono-output then ``X``\n        can be sparse.\n\n    y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n        Target values\n\n    l1_ratio : float, optional\n        float between 0 and 1 passed to elastic net (scaling between\n        l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso\n\n    eps : float\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``\n\n    n_alphas : int, optional\n        Number of alphas along the regularization path\n\n    alphas : ndarray, optional\n        List of alphas where to compute the models.\n        If None alphas are set automatically\n\n    precompute : True | False | 'auto' | array-like\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    Xy : array-like, optional\n        Xy = np.dot(X.T, y) that can be precomputed. It is useful\n        only when the Gram matrix is precomputed.\n\n    copy_X : boolean, optional, default True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    coef_init : array, shape (n_features, ) | None\n        The initial values of the coefficients.\n\n    verbose : bool or integer\n        Amount of verbosity.\n\n    return_n_iter : bool\n        whether to return the number of iterations or not.\n\n    positive : bool, default False\n        If set to True, forces coefficients to be positive.\n        (Only allowed when ``y.ndim == 1``).\n\n    check_input : bool, default True\n        Skip input validation checks, including the Gram matrix when provided\n        assuming there are handled by the caller when check_input=False.\n\n    **params : kwargs\n        keyword arguments passed to the coordinate descent solver.\n\n    Returns\n    -------\n    alphas : array, shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : array, shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : array-like, shape (n_alphas,)\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n        (Is returned when ``return_n_iter`` is set to True).\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n\n    See also\n    --------\n    MultiTaskElasticNet\n    MultiTaskElasticNetCV\n    ElasticNet\n    ElasticNetCV\n    ",inputs:[{name:"X",default_value:null,param_type:["LIST_VALID_OPTIONS","array"],expected_shape:"(n_samples, n_features)",docstring:"Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If ``y`` is mono-output then ``X`` can be sparse.",is_optional:!1,options:["array-like}"," shape (n_samples"," n_features)"]},{name:"y",default_value:null,param_type:["array"],expected_shape:"(n_samples,) or (n_samples, n_outputs)",docstring:"Target values",is_optional:!1,options:null},{name:"l1_ratio",default_value:null,param_type:["float"],expected_shape:null,docstring:"float between 0 and 1 passed to elastic net (scaling between l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso",is_optional:!0,options:null},{name:"eps",default_value:null,param_type:["float"],expected_shape:null,docstring:"Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``",is_optional:!1,options:null},{name:"n_alphas",default_value:null,param_type:["int"],expected_shape:null,docstring:"Number of alphas along the regularization path",is_optional:!0,options:null},{name:"alphas",default_value:null,param_type:["array"],expected_shape:null,docstring:"List of alphas where to compute the models. If None alphas are set automatically",is_optional:!0,options:null},{name:"precompute",default_value:null,param_type:["LIST_VALID_OPTIONS","array"],expected_shape:null,docstring:"Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument.",is_optional:!1,options:["True "," False "," 'auto' "," array-like"]},{name:"Xy",default_value:null,param_type:["array"],expected_shape:null,docstring:"Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed.",is_optional:!0,options:null},{name:"copy_X",default_value:"True",param_type:["bool"],expected_shape:null,docstring:"If ``True``, X will be copied; else, it may be overwritten.",is_optional:!0,options:null},{name:"coef_init",default_value:null,param_type:["LIST_VALID_OPTIONS","array",null],expected_shape:"(n_features, ) | None",docstring:"The initial values of the coefficients.",is_optional:!1,options:["array, shape (n_features, ) "," None"]},{name:"verbose",default_value:null,param_type:["int","bool"],expected_shape:null,docstring:"Amount of verbosity.",is_optional:!1,options:null},{name:"return_n_iter",default_value:null,param_type:["bool"],expected_shape:null,docstring:"whether to return the number of iterations or not.",is_optional:!1,options:null},{name:"positive",default_value:"False",param_type:["bool"],expected_shape:null,docstring:"If set to True, forces coefficients to be positive. (Only allowed when ``y.ndim == 1``).",is_optional:!0,options:null},{name:"check_input",default_value:"True",param_type:["bool"],expected_shape:null,docstring:"Skip input validation checks, including the Gram matrix when provided assuming there are handled by the caller when check_input=False.",is_optional:!0,options:null},{name:"**params",default_value:null,param_type:[null],expected_shape:null,docstring:"keyword arguments passed to the coordinate descent solver.",is_optional:!1,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"lars_path",outputs:[{name:"alphas",returned:!0,param_type:["array"],docstring:"Maximum of covariances (in absolute value) at each iteration. ``n_alphas`` is either ``max_iter``, ``n_features`` or the number of nodes in the path with ``alpha >= alpha_min``, whichever is smaller."},{name:"active",returned:!0,param_type:["array"],docstring:"Indices of active variables at the end of the path."},{name:"coefs",returned:!0,param_type:["array"],docstring:"Coefficients along the path"},{name:"n_iter",returned:!0,param_type:["int"],docstring:"Number of iterations run. Returned only if return_n_iter is set to True."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Compute Least Angle Regression or Lasso path using LARS algorithm [1]\n\n    The optimization objective for the case method='lasso' is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    in the case of method='lars', the objective function is only known in\n    the form of an implicit equation (see discussion in [1])\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    -----------\n    X : array, shape: (n_samples, n_features)\n        Input data.\n\n    y : array, shape: (n_samples)\n        Input targets.\n\n    Xy : array-like, shape (n_samples,) or (n_samples, n_targets),             optional\n        Xy = np.dot(X.T, y) that can be precomputed. It is useful\n        only when the Gram matrix is precomputed.\n\n    Gram : None, 'auto', array, shape: (n_features, n_features), optional\n        Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram\n        matrix is precomputed from the given X, if there are more samples\n        than features.\n\n    max_iter : integer, optional (default=500)\n        Maximum number of iterations to perform, set to infinity for no limit.\n\n    alpha_min : float, optional (default=0)\n        Minimum correlation along the path. It corresponds to the\n        regularization parameter alpha parameter in the Lasso.\n\n    method : {'lar', 'lasso'}, optional (default='lar')\n        Specifies the returned model. Select ``'lar'`` for Least Angle\n        Regression, ``'lasso'`` for the Lasso.\n\n    copy_X : bool, optional (default=True)\n        If ``False``, ``X`` is overwritten.\n\n    eps : float, optional (default=``np.finfo(np.float).eps``)\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems.\n\n    copy_Gram : bool, optional (default=True)\n        If ``False``, ``Gram`` is overwritten.\n\n    verbose : int (default=0)\n        Controls output verbosity.\n\n    return_path : bool, optional (default=True)\n        If ``return_path==True`` returns the entire path, else returns only the\n        last point of the path.\n\n    return_n_iter : bool, optional (default=False)\n        Whether to return the number of iterations.\n\n    positive : boolean (default=False)\n        Restrict coefficients to be >= 0.\n        This option is only allowed with method 'lasso'. Note that the model\n        coefficients will not converge to the ordinary-least-squares solution\n        for small values of alpha. Only coefficients up to the smallest alpha\n        value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by\n        the stepwise Lars-Lasso algorithm are typically in congruence with the\n        solution of the coordinate descent lasso_path function.\n\n    Returns\n    --------\n    alphas : array, shape: [n_alphas + 1]\n        Maximum of covariances (in absolute value) at each iteration.\n        ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n        number of nodes in the path with ``alpha >= alpha_min``, whichever\n        is smaller.\n\n    active : array, shape [n_alphas]\n        Indices of active variables at the end of the path.\n\n    coefs : array, shape (n_features, n_alphas + 1)\n        Coefficients along the path\n\n    n_iter : int\n        Number of iterations run. Returned only if return_n_iter is set\n        to True.\n\n    See also\n    --------\n    lasso_path\n    LassoLars\n    Lars\n    LassoLarsCV\n    LarsCV\n    sklearn.decomposition.sparse_encode\n\n    References\n    ----------\n    .. [1] \"Least Angle Regression\", Effron et al.\n           http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n\n    .. [2] `Wikipedia entry on the Least-angle regression\n           <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n\n    .. [3] `Wikipedia entry on the Lasso\n           <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n\n    ",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:": (n_samples, n_features)",docstring:"Input data.",is_optional:!1,options:null},{name:"y",default_value:null,param_type:["array"],expected_shape:": (n_samples)",docstring:"Input targets.",is_optional:!1,options:null},{name:"Xy",default_value:null,param_type:["array"],expected_shape:"(n_samples,) or (n_samples, n_targets),             optional",docstring:"Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed.",is_optional:!0,options:null},{name:"Gram",default_value:null,param_type:["array",null],expected_shape:": (n_features, n_features), optional",docstring:"Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram matrix is precomputed from the given X, if there are more samples than features.",is_optional:!0,options:null},{name:"max_iter",default_value:"500",param_type:["int"],expected_shape:null,docstring:"Maximum number of iterations to perform, set to infinity for no limit.",is_optional:!0,options:null},{name:"alpha_min",default_value:"0",param_type:["float"],expected_shape:null,docstring:"Minimum correlation along the path. It corresponds to the regularization parameter alpha parameter in the Lasso.",is_optional:!0,options:null},{name:"method",default_value:"lar",param_type:["LIST_VALID_OPTIONS"],expected_shape:null,docstring:"Specifies the returned model. Select ``'lar'`` for Least Angle Regression, ``'lasso'`` for the Lasso.",is_optional:!0,options:["lar"," 'lasso'}"," optional (default='lar')"]},{name:"copy_X",default_value:"True",param_type:["bool"],expected_shape:null,docstring:"If ``False``, ``X`` is overwritten.",is_optional:!0,options:null},{name:"eps",default_value:"``np.finfonp.float.eps``",param_type:["float"],expected_shape:null,docstring:"The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems.",is_optional:!0,options:null},{name:"copy_Gram",default_value:"True",param_type:["bool"],expected_shape:null,docstring:"If ``False``, ``Gram`` is overwritten.",is_optional:!0,options:null},{name:"verbose",default_value:"0",param_type:["int"],expected_shape:null,docstring:"Controls output verbosity.",is_optional:!0,options:null},{name:"return_path",default_value:"True",param_type:["bool"],expected_shape:null,docstring:"If ``return_path==True`` returns the entire path, else returns only the last point of the path.",is_optional:!0,options:null},{name:"return_n_iter",default_value:"False",param_type:["bool"],expected_shape:null,docstring:"Whether to return the number of iterations.",is_optional:!0,options:null},{name:"positive",default_value:"False",param_type:["bool"],expected_shape:null,docstring:"Restrict coefficients to be >= 0. This option is only allowed with method 'lasso'. Note that the model coefficients will not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent lasso_path function.",is_optional:!0,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"lasso_path",outputs:[{name:"alphas",returned:!0,param_type:["array"],docstring:"The alphas along the path where models are computed."},{name:"coefs",returned:!0,param_type:["array"],docstring:"Coefficients along the path."},{name:"dual_gaps",returned:!0,param_type:["array"],docstring:"The dual gaps at the end of the optimization for each alpha."},{name:"n_iters",returned:!0,param_type:["array"],docstring:"The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Compute Lasso path with coordinate descent\n\n    The Lasso optimization function varies for mono and multi-outputs.\n\n    For mono-output tasks it is::\n\n        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    For multi-output tasks it is::\n\n        (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n\n    Where::\n\n        ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n\n    i.e. the sum of norm of each row.\n\n    Read more in the :ref:`User Guide <lasso>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n        Training data. Pass directly as Fortran-contiguous data to avoid\n        unnecessary memory duplication. If ``y`` is mono-output then ``X``\n        can be sparse.\n\n    y : ndarray, shape (n_samples,), or (n_samples, n_outputs)\n        Target values\n\n    eps : float, optional\n        Length of the path. ``eps=1e-3`` means that\n        ``alpha_min / alpha_max = 1e-3``\n\n    n_alphas : int, optional\n        Number of alphas along the regularization path\n\n    alphas : ndarray, optional\n        List of alphas where to compute the models.\n        If ``None`` alphas are set automatically\n\n    precompute : True | False | 'auto' | array-like\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    Xy : array-like, optional\n        Xy = np.dot(X.T, y) that can be precomputed. It is useful\n        only when the Gram matrix is precomputed.\n\n    copy_X : boolean, optional, default True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    coef_init : array, shape (n_features, ) | None\n        The initial values of the coefficients.\n\n    verbose : bool or integer\n        Amount of verbosity.\n\n    return_n_iter : bool\n        whether to return the number of iterations or not.\n\n    positive : bool, default False\n        If set to True, forces coefficients to be positive.\n        (Only allowed when ``y.ndim == 1``).\n\n    **params : kwargs\n        keyword arguments passed to the coordinate descent solver.\n\n    Returns\n    -------\n    alphas : array, shape (n_alphas,)\n        The alphas along the path where models are computed.\n\n    coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n        Coefficients along the path.\n\n    dual_gaps : array, shape (n_alphas,)\n        The dual gaps at the end of the optimization for each alpha.\n\n    n_iters : array-like, shape (n_alphas,)\n        The number of iterations taken by the coordinate descent optimizer to\n        reach the specified tolerance for each alpha.\n\n    Notes\n    -----\n    For an example, see\n    :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n    <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    Note that in certain cases, the Lars solver may be significantly\n    faster to implement this functionality. In particular, linear\n    interpolation can be used to retrieve model coefficients between the\n    values output by lars_path\n\n    Examples\n    ---------\n\n    Comparing lasso_path and lars_path with interpolation:\n\n    >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    >>> y = np.array([1, 2, 3.1])\n    >>> # Use lasso_path to compute a coefficient path\n    >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n    >>> print(coef_path)\n    [[0.         0.         0.46874778]\n     [0.2159048  0.4425765  0.23689075]]\n\n    >>> # Now use lars_path and 1D linear interpolation to compute the\n    >>> # same path\n    >>> from sklearn.linear_model import lars_path\n    >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n    >>> from scipy import interpolate\n    >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n    ...                                             coef_path_lars[:, ::-1])\n    >>> print(coef_path_continuous([5., 1., .5]))\n    [[0.         0.         0.46915237]\n     [0.2159048  0.4425765  0.23668876]]\n\n\n    See also\n    --------\n    lars_path\n    Lasso\n    LassoLars\n    LassoCV\n    LassoLarsCV\n    sklearn.decomposition.sparse_encode\n    ",inputs:[{name:"X",default_value:null,param_type:["LIST_VALID_OPTIONS","array"],expected_shape:"(n_samples, n_features)",docstring:"Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If ``y`` is mono-output then ``X`` can be sparse.",is_optional:!1,options:["array-like"," sparse matrix}"," shape (n_samples"," n_features)"]},{name:"y",default_value:null,param_type:["array"],expected_shape:"(n_samples,), or (n_samples, n_outputs)",docstring:"Target values",is_optional:!1,options:null},{name:"eps",default_value:null,param_type:["float"],expected_shape:null,docstring:"Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``",is_optional:!0,options:null},{name:"n_alphas",default_value:null,param_type:["int"],expected_shape:null,docstring:"Number of alphas along the regularization path",is_optional:!0,options:null},{name:"alphas",default_value:null,param_type:["array"],expected_shape:null,docstring:"List of alphas where to compute the models. If ``None`` alphas are set automatically",is_optional:!0,options:null},{name:"precompute",default_value:null,param_type:["LIST_VALID_OPTIONS","array"],expected_shape:null,docstring:"Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument.",is_optional:!1,options:["True "," False "," 'auto' "," array-like"]},{name:"Xy",default_value:null,param_type:["array"],expected_shape:null,docstring:"Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed.",is_optional:!0,options:null},{name:"copy_X",default_value:"True",param_type:["bool"],expected_shape:null,docstring:"If ``True``, X will be copied; else, it may be overwritten.",is_optional:!0,options:null},{name:"coef_init",default_value:null,param_type:["LIST_VALID_OPTIONS","array",null],expected_shape:"(n_features, ) | None",docstring:"The initial values of the coefficients.",is_optional:!1,options:["array, shape (n_features, ) "," None"]},{name:"verbose",default_value:null,param_type:["int","bool"],expected_shape:null,docstring:"Amount of verbosity.",is_optional:!1,options:null},{name:"return_n_iter",default_value:null,param_type:["bool"],expected_shape:null,docstring:"whether to return the number of iterations or not.",is_optional:!1,options:null},{name:"positive",default_value:"False",param_type:["bool"],expected_shape:null,docstring:"If set to True, forces coefficients to be positive. (Only allowed when ``y.ndim == 1``).",is_optional:!0,options:null},{name:"**params",default_value:null,param_type:[null],expected_shape:null,docstring:"keyword arguments passed to the coordinate descent solver.",is_optional:!1,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"lasso_stability_path",outputs:[{name:"alphas_grid",returned:!0,param_type:["array"],docstring:"The grid points between 0 and 1: alpha/alpha_max  scores_path : array, shape = [n_features, n_grid]     The scores for each feature along the path."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"DEPRECATED: The function lasso_stability_path is deprecated in 0.19 and will be removed in 0.21.\n\nStability path based on randomized Lasso estimates\n\n    Parameters\n    ----------\n    X : array-like, shape = [n_samples, n_features]\n        training data.\n\n    y : array-like, shape = [n_samples]\n        target values.\n\n    scaling : float, optional, default=0.5\n        The alpha parameter in the stability selection article used to\n        randomly scale the features. Should be between 0 and 1.\n\n    random_state : int, RandomState instance or None, optional, default=None\n        The generator used to randomize the design.  If int, random_state is\n        the seed used by the random number generator; If RandomState instance,\n        random_state is the random number generator; If None, the random number\n        generator is the RandomState instance used by `np.random`.\n\n    n_resampling : int, optional, default=200\n        Number of randomized models.\n\n    n_grid : int, optional, default=100\n        Number of grid points. The path is linearly reinterpolated\n        on a grid between 0 and 1 before computing the scores.\n\n    sample_fraction : float, optional, default=0.75\n        The fraction of samples to be used in each randomized design.\n        Should be between 0 and 1. If 1, all samples are used.\n\n    eps : float, optional\n        Smallest value of alpha / alpha_max considered\n\n    n_jobs : int or None, optional (default=None)\n        Number of CPUs to use during the resampling.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : boolean or integer, optional\n        Sets the verbosity amount\n\n    Returns\n    -------\n    alphas_grid : array, shape ~ [n_grid]\n        The grid points between 0 and 1: alpha/alpha_max\n\n    scores_path : array, shape = [n_features, n_grid]\n        The scores for each feature along the path.\n    ",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"[n_samples, n_features]",docstring:"training data.  y : array-like, shape = [n_samples]     target values.  scaling : float, optional, default=0.5     The alpha parameter in the stability selection article used to     randomly scale the features. Should be between 0 and 1.  random_state : int, RandomState instance or None, optional, default=None     The generator used to randomize the design.  If int, random_state is     the seed used by the random number generator; If RandomState instance,     random_state is the random number generator; If None, the random number     generator is the RandomState instance used by `np.random`.  n_resampling : int, optional, default=200     Number of randomized models.  n_grid : int, optional, default=100     Number of grid points. The path is linearly reinterpolated     on a grid between 0 and 1 before computing the scores.  sample_fraction : float, optional, default=0.75     The fraction of samples to be used in each randomized design.     Should be between 0 and 1. If 1, all samples are used.  eps : float, optional     Smallest value of alpha / alpha_max considered  n_jobs : int or None, optional (default=None)     Number of CPUs to use during the resampling.     ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.     ``-1`` means using all processors. See :term:`Glossary <n_jobs>`     for more details.  verbose : boolean or integer, optional     Sets the verbosity amount",is_optional:!1,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"logistic_regression_path",outputs:[{name:"coefs",returned:!0,param_type:["array"],docstring:"List of coefficients for the Logistic Regression model. If fit_intercept is set to True then the second dimension will be n_features + 1, where the last item represents the intercept. For ``multiclass='multinomial'``, the shape is (n_classes, n_cs, n_features) or (n_classes, n_cs, n_features + 1)."},{name:"Cs",returned:!0,param_type:["array"],docstring:"Grid of Cs used for cross-validation."},{name:"n_iter",returned:!0,param_type:["array"],docstring:"Actual number of iteration for each Cs."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Compute a Logistic Regression model for a list of regularization\n    parameters.\n\n    This is an implementation that uses the result of the previous model\n    to speed up computations along the set of solutions, making it faster\n    than sequentially calling LogisticRegression for the different parameters.\n    Note that there will be no speedup with liblinear solver, since it does\n    not handle warm-starting.\n\n    Read more in the :ref:`User Guide <logistic_regression>`.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape (n_samples, n_features)\n        Input data.\n\n    y : array-like, shape (n_samples,) or (n_samples, n_targets)\n        Input data, target values.\n\n    pos_class : int, None\n        The class with respect to which we perform a one-vs-all fit.\n        If None, then it is assumed that the given problem is binary.\n\n    Cs : int | array-like, shape (n_cs,)\n        List of values for the regularization parameter or integer specifying\n        the number of regularization parameters that should be used. In this\n        case, the parameters will be chosen in a logarithmic scale between\n        1e-4 and 1e4.\n\n    fit_intercept : bool\n        Whether to fit an intercept for the model. In this case the shape of\n        the returned array is (n_cs, n_features + 1).\n\n    max_iter : int\n        Maximum number of iterations for the solver.\n\n    tol : float\n        Stopping criterion. For the newton-cg and lbfgs solvers, the iteration\n        will stop when ``max{|g_i | i = 1, ..., n} <= tol``\n        where ``g_i`` is the i-th component of the gradient.\n\n    verbose : int\n        For the liblinear and lbfgs solvers set verbose to any positive\n        number for verbosity.\n\n    solver : {'lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'}\n        Numerical solver to use.\n\n    coef : array-like, shape (n_features,), default None\n        Initialization value for coefficients of logistic regression.\n        Useless for liblinear solver.\n\n    class_weight : dict or 'balanced', optional\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n    dual : bool\n        Dual or primal formulation. Dual formulation is only implemented for\n        l2 penalty with liblinear solver. Prefer dual=False when\n        n_samples > n_features.\n\n    penalty : str, 'l1' or 'l2'\n        Used to specify the norm used in the penalization. The 'newton-cg',\n        'sag' and 'lbfgs' solvers support only l2 penalties.\n\n    intercept_scaling : float, default 1.\n        Useful only when the solver 'liblinear' is used\n        and self.fit_intercept is set to True. In this case, x becomes\n        [x, self.intercept_scaling],\n        i.e. a \"synthetic\" feature with constant value equal to\n        intercept_scaling is appended to the instance vector.\n        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n\n        Note! the synthetic feature weight is subject to l1/l2 regularization\n        as all other features.\n        To lessen the effect of regularization on synthetic feature weight\n        (and therefore on the intercept) intercept_scaling has to be increased.\n\n    multi_class : str, {'ovr', 'multinomial', 'auto'}, default: 'ovr'\n        If the option chosen is 'ovr', then a binary problem is fit for each\n        label. For 'multinomial' the loss minimised is the multinomial loss fit\n        across the entire probability distribution, *even when the data is\n        binary*. 'multinomial' is unavailable when solver='liblinear'.\n        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n        and otherwise selects 'multinomial'.\n\n        .. versionadded:: 0.18\n           Stochastic Average Gradient descent solver for 'multinomial' case.\n        .. versionchanged:: 0.20\n            Default will change from 'ovr' to 'auto' in 0.22.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`. Used when ``solver`` == 'sag' or\n        'liblinear'.\n\n    check_input : bool, default True\n        If False, the input arrays X and y will not be checked.\n\n    max_squared_sum : float, default None\n        Maximum squared sum of X over samples. Used only in SAG solver.\n        If None, it will be computed, going through all the samples.\n        The value should be precomputed to speed up cross validation.\n\n    sample_weight : array-like, shape(n_samples,) optional\n        Array of weights that are assigned to individual samples.\n        If not provided, then each sample is given unit weight.\n\n    Returns\n    -------\n    coefs : ndarray, shape (n_cs, n_features) or (n_cs, n_features + 1)\n        List of coefficients for the Logistic Regression model. If\n        fit_intercept is set to True then the second dimension will be\n        n_features + 1, where the last item represents the intercept. For\n        ``multiclass='multinomial'``, the shape is (n_classes, n_cs,\n        n_features) or (n_classes, n_cs, n_features + 1).\n\n    Cs : ndarray\n        Grid of Cs used for cross-validation.\n\n    n_iter : array, shape (n_cs,)\n        Actual number of iteration for each Cs.\n\n    Notes\n    -----\n    You might get slightly different results with the solver liblinear than\n    with the others since this uses LIBLINEAR which penalizes the intercept.\n\n    .. versionchanged:: 0.19\n        The \"copy\" parameter was removed.\n    ",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"Input data.",is_optional:!1,options:null},{name:"y",default_value:null,param_type:["array"],expected_shape:"(n_samples,) or (n_samples, n_targets)",docstring:"Input data, target values.",is_optional:!1,options:null},{name:"pos_class",default_value:null,param_type:["int",null],expected_shape:null,docstring:"The class with respect to which we perform a one-vs-all fit. If None, then it is assumed that the given problem is binary.",is_optional:!1,options:null},{name:"Cs",default_value:null,param_type:["LIST_VALID_OPTIONS","array","int"],expected_shape:"(n_cs,)",docstring:"List of values for the regularization parameter or integer specifying the number of regularization parameters that should be used. In this case, the parameters will be chosen in a logarithmic scale between 1e-4 and 1e4.",is_optional:!1,options:["int "," array-like, shape (n_cs,)"]},{name:"fit_intercept",default_value:null,param_type:["bool"],expected_shape:null,docstring:"Whether to fit an intercept for the model. In this case the shape of the returned array is (n_cs, n_features + 1).",is_optional:!1,options:null},{name:"max_iter",default_value:null,param_type:["int"],expected_shape:null,docstring:"Maximum number of iterations for the solver.",is_optional:!1,options:null},{name:"tol",default_value:null,param_type:["float"],expected_shape:null,docstring:"Stopping criterion. For the newton-cg and lbfgs solvers, the iteration will stop when ``max{|g_i | i = 1, ..., n} <= tol`` where ``g_i`` is the i-th component of the gradient.",is_optional:!1,options:null},{name:"verbose",default_value:null,param_type:["int"],expected_shape:null,docstring:"For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.",is_optional:!1,options:null},{name:"solver",default_value:null,param_type:["LIST_VALID_OPTIONS"],expected_shape:null,docstring:"Numerical solver to use.",is_optional:!1,options:["lbfgs"," 'newton-cg"," 'liblinear"," 'sag"," 'saga"]},{name:"coef",default_value:"None",param_type:["array",null],expected_shape:"(n_features,), default None",docstring:"Initialization value for coefficients of logistic regression. Useless for liblinear solver.",is_optional:!0,options:null},{name:"class_weight",default_value:null,param_type:["dict"],expected_shape:null,docstring:'Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.  The "balanced" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.  Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.',is_optional:!0,options:null},{name:"dual",default_value:null,param_type:["bool"],expected_shape:null,docstring:"Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features.",is_optional:!1,options:null},{name:"penalty",default_value:null,param_type:["str"],expected_shape:null,docstring:"Used to specify the norm used in the penalization. The 'newton-cg', 'sag' and 'lbfgs' solvers support only l2 penalties.",is_optional:!1,options:null},{name:"intercept_scaling",default_value:"1.",param_type:["float"],expected_shape:null,docstring:"Useful only when the solver 'liblinear' is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a \"synthetic\" feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes ``intercept_scaling * synthetic_feature_weight``.  Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased.",is_optional:!0,options:null},{name:"multi_class",default_value:"ovr",param_type:["LIST_VALID_OPTIONS","str"],expected_shape:null,docstring:"If the option chosen is 'ovr', then a binary problem is fit for each label. For 'multinomial' the loss minimised is the multinomial loss fit across the entire probability distribution, *even when the data is binary*. 'multinomial' is unavailable when solver='liblinear'. 'auto' selects 'ovr' if the data is binary, or if solver='liblinear', and otherwise selects 'multinomial'.  .. versionadded:: 0.18    Stochastic Average Gradient descent solver for 'multinomial' case. .. versionchanged:: 0.20     Default will change from 'ovr' to 'auto' in 0.22.",is_optional:!0,options:["str"," {'ovr"," 'multinomial"," 'auto'}"," default: 'ovr"]},{name:"random_state",default_value:"None",param_type:["int",null],expected_shape:null,docstring:"The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. Used when ``solver`` == 'sag' or 'liblinear'.",is_optional:!0,options:null},{name:"check_input",default_value:"True",param_type:["bool"],expected_shape:null,docstring:"If False, the input arrays X and y will not be checked.",is_optional:!0,options:null},{name:"max_squared_sum",default_value:"None",param_type:["float",null],expected_shape:null,docstring:"Maximum squared sum of X over samples. Used only in SAG solver. If None, it will be computed, going through all the samples. The value should be precomputed to speed up cross validation.",is_optional:!0,options:null},{name:"sample_weight",default_value:null,param_type:["array"],expected_shape:"(n_samples,) optional",docstring:"Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.",is_optional:!0,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"orthogonal_mp",outputs:[{name:"coef",returned:!0,param_type:["array"],docstring:"Coefficients of the OMP solution. If `return_path=True`, this contains the whole coefficient path. In this case its shape is (n_features, n_features) or (n_features, n_targets, n_features) and iterating over the last axis yields coefficients in increasing order of active features."},{name:"n_iters",returned:!0,param_type:["array","int"],docstring:"Number of active features across every target. Returned only if `return_n_iter` is set to True."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Orthogonal Matching Pursuit (OMP)\n\n    Solves n_targets Orthogonal Matching Pursuit problems.\n    An instance of the problem has the form:\n\n    When parametrized by the number of non-zero coefficients using\n    `n_nonzero_coefs`:\n    argmin ||y - X\\gamma||^2 subject to ||\\gamma||_0 <= n_{nonzero coefs}\n\n    When parametrized by error using the parameter `tol`:\n    argmin ||\\gamma||_0 subject to ||y - X\\gamma||^2 <= tol\n\n    Read more in the :ref:`User Guide <omp>`.\n\n    Parameters\n    ----------\n    X : array, shape (n_samples, n_features)\n        Input data. Columns are assumed to have unit norm.\n\n    y : array, shape (n_samples,) or (n_samples, n_targets)\n        Input targets\n\n    n_nonzero_coefs : int\n        Desired number of non-zero entries in the solution. If None (by\n        default) this value is set to 10% of n_features.\n\n    tol : float\n        Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n\n    precompute : {True, False, 'auto'},\n        Whether to perform precomputations. Improves performance when n_targets\n        or n_samples is very large.\n\n    copy_X : bool, optional\n        Whether the design matrix X must be copied by the algorithm. A false\n        value is only helpful if X is already Fortran-ordered, otherwise a\n        copy is made anyway.\n\n    return_path : bool, optional. Default: False\n        Whether to return every value of the nonzero coefficients along the\n        forward path. Useful for cross-validation.\n\n    return_n_iter : bool, optional default False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    coef : array, shape (n_features,) or (n_features, n_targets)\n        Coefficients of the OMP solution. If `return_path=True`, this contains\n        the whole coefficient path. In this case its shape is\n        (n_features, n_features) or (n_features, n_targets, n_features) and\n        iterating over the last axis yields coefficients in increasing order\n        of active features.\n\n    n_iters : array-like or int\n        Number of active features across every target. Returned only if\n        `return_n_iter` is set to True.\n\n    See also\n    --------\n    OrthogonalMatchingPursuit\n    orthogonal_mp_gram\n    lars_path\n    decomposition.sparse_encode\n\n    Notes\n    -----\n    Orthogonal matching pursuit was introduced in S. Mallat, Z. Zhang,\n    Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n    (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n\n    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n    Matching Pursuit Technical Report - CS Technion, April 2008.\n    http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n\n    ",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"Input data. Columns are assumed to have unit norm.",is_optional:!1,options:null},{name:"y",default_value:null,param_type:["array"],expected_shape:"(n_samples,) or (n_samples, n_targets)",docstring:"Input targets",is_optional:!1,options:null},{name:"n_nonzero_coefs",default_value:null,param_type:["int"],expected_shape:null,docstring:"Desired number of non-zero entries in the solution. If None (by default) this value is set to 10% of n_features.",is_optional:!1,options:null},{name:"tol",default_value:null,param_type:["float"],expected_shape:null,docstring:"Maximum norm of the residual. If not None, overrides n_nonzero_coefs.",is_optional:!1,options:null},{name:"precompute",default_value:null,param_type:["LIST_VALID_OPTIONS"],expected_shape:null,docstring:"Whether to perform precomputations. Improves performance when n_targets or n_samples is very large.",is_optional:!1,options:["True"," False"," 'auto'}",""]},{name:"copy_X",default_value:null,param_type:["bool"],expected_shape:null,docstring:"Whether the design matrix X must be copied by the algorithm. A false value is only helpful if X is already Fortran-ordered, otherwise a copy is made anyway.",is_optional:!0,options:null},{name:"return_path",default_value:null,param_type:["bool"],expected_shape:null,docstring:"Whether to return every value of the nonzero coefficients along the forward path. Useful for cross-validation.",is_optional:!0,options:null},{name:"return_n_iter",default_value:"False",param_type:["bool"],expected_shape:null,docstring:"Whether or not to return the number of iterations.",is_optional:!0,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"orthogonal_mp_gram",outputs:[{name:"coef",returned:!0,param_type:["array"],docstring:"Coefficients of the OMP solution. If `return_path=True`, this contains the whole coefficient path. In this case its shape is (n_features, n_features) or (n_features, n_targets, n_features) and iterating over the last axis yields coefficients in increasing order of active features."},{name:"n_iters",returned:!0,param_type:["array","int"],docstring:"Number of active features across every target. Returned only if `return_n_iter` is set to True."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Gram Orthogonal Matching Pursuit (OMP)\n\n    Solves n_targets Orthogonal Matching Pursuit problems using only\n    the Gram matrix X.T * X and the product X.T * y.\n\n    Read more in the :ref:`User Guide <omp>`.\n\n    Parameters\n    ----------\n    Gram : array, shape (n_features, n_features)\n        Gram matrix of the input data: X.T * X\n\n    Xy : array, shape (n_features,) or (n_features, n_targets)\n        Input targets multiplied by X: X.T * y\n\n    n_nonzero_coefs : int\n        Desired number of non-zero entries in the solution. If None (by\n        default) this value is set to 10% of n_features.\n\n    tol : float\n        Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n\n    norms_squared : array-like, shape (n_targets,)\n        Squared L2 norms of the lines of y. Required if tol is not None.\n\n    copy_Gram : bool, optional\n        Whether the gram matrix must be copied by the algorithm. A false\n        value is only helpful if it is already Fortran-ordered, otherwise a\n        copy is made anyway.\n\n    copy_Xy : bool, optional\n        Whether the covariance vector Xy must be copied by the algorithm.\n        If False, it may be overwritten.\n\n    return_path : bool, optional. Default: False\n        Whether to return every value of the nonzero coefficients along the\n        forward path. Useful for cross-validation.\n\n    return_n_iter : bool, optional default False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    coef : array, shape (n_features,) or (n_features, n_targets)\n        Coefficients of the OMP solution. If `return_path=True`, this contains\n        the whole coefficient path. In this case its shape is\n        (n_features, n_features) or (n_features, n_targets, n_features) and\n        iterating over the last axis yields coefficients in increasing order\n        of active features.\n\n    n_iters : array-like or int\n        Number of active features across every target. Returned only if\n        `return_n_iter` is set to True.\n\n    See also\n    --------\n    OrthogonalMatchingPursuit\n    orthogonal_mp\n    lars_path\n    decomposition.sparse_encode\n\n    Notes\n    -----\n    Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n    Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n    (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n\n    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n    Matching Pursuit Technical Report - CS Technion, April 2008.\n    http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n\n    ",inputs:[{name:"Gram",default_value:null,param_type:["array"],expected_shape:"(n_features, n_features)",docstring:"Gram matrix of the input data: X.T * X",is_optional:!1,options:null},{name:"Xy",default_value:null,param_type:["array"],expected_shape:"(n_features,) or (n_features, n_targets)",docstring:"Input targets multiplied by X: X.T * y",is_optional:!1,options:null},{name:"n_nonzero_coefs",default_value:null,param_type:["int"],expected_shape:null,docstring:"Desired number of non-zero entries in the solution. If None (by default) this value is set to 10% of n_features.",is_optional:!1,options:null},{name:"tol",default_value:null,param_type:["float"],expected_shape:null,docstring:"Maximum norm of the residual. If not None, overrides n_nonzero_coefs.",is_optional:!1,options:null},{name:"norms_squared",default_value:null,param_type:["array"],expected_shape:"(n_targets,)",docstring:"Squared L2 norms of the lines of y. Required if tol is not None.",is_optional:!1,options:null},{name:"copy_Gram",default_value:null,param_type:["bool"],expected_shape:null,docstring:"Whether the gram matrix must be copied by the algorithm. A false value is only helpful if it is already Fortran-ordered, otherwise a copy is made anyway.",is_optional:!0,options:null},{name:"copy_Xy",default_value:null,param_type:["bool"],expected_shape:null,docstring:"Whether the covariance vector Xy must be copied by the algorithm. If False, it may be overwritten.",is_optional:!0,options:null},{name:"return_path",default_value:null,param_type:["bool"],expected_shape:null,docstring:"Whether to return every value of the nonzero coefficients along the forward path. Useful for cross-validation.",is_optional:!0,options:null},{name:"return_n_iter",default_value:"False",param_type:["bool"],expected_shape:null,docstring:"Whether or not to return the number of iterations.",is_optional:!0,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"ridge_regression",outputs:[{name:"coef",returned:!0,param_type:["array"],docstring:"Weight vector(s)."},{name:"n_iter",returned:!0,param_type:["int"],docstring:"The actual number of iteration performed by the solver. Only returned if `return_n_iter` is True."},{name:"intercept",returned:!0,param_type:["array","float"],docstring:"The intercept of the model. Only returned if `return_intercept` is True and if X is a scipy sparse array."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Solve the ridge equation by the method of normal equations.\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix, LinearOperator},\n        shape = [n_samples, n_features]\n        Training data\n\n    y : array-like, shape = [n_samples] or [n_samples, n_targets]\n        Target values\n\n    alpha : {float, array-like},\n        shape = [n_targets] if array-like\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``C^-1`` in other linear models such as\n        LogisticRegression or LinearSVC. If an array is passed, penalties are\n        assumed to be specific to the targets. Hence they must correspond in\n        number.\n\n    sample_weight : float or numpy array of shape [n_samples]\n        Individual weights for each sample. If sample_weight is not None and\n        solver='auto', the solver will be set to 'cholesky'.\n\n        .. versionadded:: 0.17\n\n    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'}\n        Solver to use in the computational routines:\n\n        - 'auto' chooses the solver automatically based on the type of data.\n\n        - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n          coefficients. More stable for singular matrices than\n          'cholesky'.\n\n        - 'cholesky' uses the standard scipy.linalg.solve function to\n          obtain a closed-form solution via a Cholesky decomposition of\n          dot(X.T, X)\n\n        - 'sparse_cg' uses the conjugate gradient solver as found in\n          scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n          more appropriate than 'cholesky' for large-scale data\n          (possibility to set `tol` and `max_iter`).\n\n        - 'lsqr' uses the dedicated regularized least-squares routine\n          scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n          procedure.\n\n        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n          its improved, unbiased version named SAGA. Both methods also use an\n          iterative procedure, and are often faster than other solvers when\n          both n_samples and n_features are large. Note that 'sag' and\n          'saga' fast convergence is only guaranteed on features with\n          approximately the same scale. You can preprocess the data with a\n          scaler from sklearn.preprocessing.\n\n\n        All last five solvers support both dense and sparse data. However, only\n        'sag' and 'saga' supports sparse input when`fit_intercept` is True.\n\n        .. versionadded:: 0.17\n           Stochastic Average Gradient descent solver.\n        .. versionadded:: 0.19\n           SAGA solver.\n\n    max_iter : int, optional\n        Maximum number of iterations for conjugate gradient solver.\n        For the 'sparse_cg' and 'lsqr' solvers, the default value is determined\n        by scipy.sparse.linalg. For 'sag' and saga solver, the default value is\n        1000.\n\n    tol : float\n        Precision of the solution.\n\n    verbose : int\n        Verbosity level. Setting verbose > 0 will display additional\n        information depending on the solver used.\n\n    random_state : int, RandomState instance or None, optional, default None\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`. Used when ``solver`` == 'sag'.\n\n    return_n_iter : boolean, default False\n        If True, the method also returns `n_iter`, the actual number of\n        iteration performed by the solver.\n\n        .. versionadded:: 0.17\n\n    return_intercept : boolean, default False\n        If True and if X is sparse, the method also returns the intercept,\n        and the solver is automatically changed to 'sag'. This is only a\n        temporary fix for fitting the intercept with sparse data. For dense\n        data, use sklearn.linear_model._preprocess_data before your regression.\n\n        .. versionadded:: 0.17\n\n    Returns\n    -------\n    coef : array, shape = [n_features] or [n_targets, n_features]\n        Weight vector(s).\n\n    n_iter : int, optional\n        The actual number of iteration performed by the solver.\n        Only returned if `return_n_iter` is True.\n\n    intercept : float or array, shape = [n_targets]\n        The intercept of the model. Only returned if `return_intercept`\n        is True and if X is a scipy sparse array.\n\n    Notes\n    -----\n    This function won't compute the intercept.\n    ",inputs:[{name:"X",default_value:null,param_type:["LIST_VALID_OPTIONS","array"],expected_shape:null,docstring:"shape = [n_samples, n_features] Training data",is_optional:!1,options:["array-like"," sparse matrix"," LinearOperator}",""]},{name:"y",default_value:null,param_type:["array"],expected_shape:"[n_samples] or [n_samples, n_targets]",docstring:"Target values",is_optional:!1,options:null},{name:"alpha",default_value:null,param_type:["LIST_VALID_OPTIONS","array","float"],expected_shape:null,docstring:"shape = [n_targets] if array-like Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``C^-1`` in other linear models such as LogisticRegression or LinearSVC. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number.",is_optional:!1,options:["float"," array-like}",""]},{name:"sample_weight",default_value:null,param_type:["array","float"],expected_shape:"[n_samples]",docstring:"Individual weights for each sample. If sample_weight is not None and solver='auto', the solver will be set to 'cholesky'.  .. versionadded:: 0.17",is_optional:!1,options:null},{name:"solver",default_value:null,param_type:["LIST_VALID_OPTIONS"],expected_shape:null,docstring:"Solver to use in the computational routines:  - 'auto' chooses the solver automatically based on the type of data.  - 'svd' uses a Singular Value Decomposition of X to compute the Ridge   coefficients. More stable for singular matrices than   'cholesky'.  - 'cholesky' uses the standard scipy.linalg.solve function to   obtain a closed-form solution via a Cholesky decomposition of   dot(X.T, X)  - 'sparse_cg' uses the conjugate gradient solver as found in   scipy.sparse.linalg.cg. As an iterative algorithm, this solver is   more appropriate than 'cholesky' for large-scale data   (possibility to set `tol` and `max_iter`).  - 'lsqr' uses the dedicated regularized least-squares routine   scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative   procedure.  - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses   its improved, unbiased version named SAGA. Both methods also use an   iterative procedure, and are often faster than other solvers when   both n_samples and n_features are large. Note that 'sag' and   'saga' fast convergence is only guaranteed on features with   approximately the same scale. You can preprocess the data with a   scaler from sklearn.preprocessing.  All last five solvers support both dense and sparse data. However, only 'sag' and 'saga' supports sparse input when`fit_intercept` is True.  .. versionadded:: 0.17    Stochastic Average Gradient descent solver. .. versionadded:: 0.19    SAGA solver.",is_optional:!1,options:["auto"," 'svd"," 'cholesky"," 'lsqr"," 'sparse_cg"," 'sag"," 'saga"]},{name:"max_iter",default_value:null,param_type:["int"],expected_shape:null,docstring:"Maximum number of iterations for conjugate gradient solver. For the 'sparse_cg' and 'lsqr' solvers, the default value is determined by scipy.sparse.linalg. For 'sag' and saga solver, the default value is 1000.",is_optional:!0,options:null},{name:"tol",default_value:null,param_type:["float"],expected_shape:null,docstring:"Precision of the solution.",is_optional:!1,options:null},{name:"verbose",default_value:null,param_type:["int"],expected_shape:null,docstring:"Verbosity level. Setting verbose > 0 will display additional information depending on the solver used.",is_optional:!1,options:null},{name:"random_state",default_value:"None",param_type:["int",null],expected_shape:null,docstring:"The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. Used when ``solver`` == 'sag'.",is_optional:!0,options:null},{name:"return_n_iter",default_value:"False",param_type:["bool"],expected_shape:null,docstring:"If True, the method also returns `n_iter`, the actual number of iteration performed by the solver.  .. versionadded:: 0.17",is_optional:!0,options:null},{name:"return_intercept",default_value:"False",param_type:["bool"],expected_shape:null,docstring:"If True and if X is sparse, the method also returns the intercept, and the solver is automatically changed to 'sag'. This is only a temporary fix for fitting the intercept with sparse data. For dense data, use sklearn.linear_model._preprocess_data before your regression.  .. versionadded:: 0.17",is_optional:!0,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]}],nodes:[{name:"ElasticNet",outputs:[{name:"coef_",returned:!1,param_type:["array"],docstring:"parameter vector (w in the cost function formula)"},{name:"sparse_coef_",returned:!1,param_type:["scipy.sparse.matrix"],docstring:"``sparse_coef_`` is a readonly property derived from ``coef_``"},{name:"intercept_",returned:!1,param_type:["array","float"],docstring:"independent term in decision function."},{name:"n_iter_",returned:!1,param_type:["array"],docstring:"number of iterations run by the coordinate descent solver to reach the specified tolerance."}],docstring:"Linear regression with combined L1 and L2 priors as regularizer.\n\n    Minimizes the objective function::\n\n            1 / (2 * n_samples) * ||y - Xw||^2_2\n            + alpha * l1_ratio * ||w||_1\n            + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\n    If you are interested in controlling the L1 and L2 penalty\n    separately, keep in mind that this is equivalent to::\n\n            a * L1 + b * L2\n\n    where::\n\n            alpha = a + b and l1_ratio = a / (a + b)\n\n    The parameter l1_ratio corresponds to alpha in the glmnet R package while\n    alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n    = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\n    unless you supply your own sequence of alpha.",inputs:[{name:"alpha",default_value:"1.0",param_type:["float"],expected_shape:null,docstring:"Constant that multiplies the penalty terms. Defaults to 1.0. See the notes for the exact mathematical meaning of this parameter.``alpha = 0`` is equivalent to an ordinary least square, solved by the :class:`LinearRegression` object. For numerical reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised. Given this, you should use the :class:`LinearRegression` object.",is_optional:!0,options:null},{name:"l1_ratio",default_value:null,param_type:["float"],expected_shape:null,docstring:"The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2.",is_optional:!1,options:null},{name:"fit_intercept",default_value:null,param_type:["bool"],expected_shape:null,docstring:"Whether the intercept should be estimated or not. If ``False``, the data is assumed to be already centered.",is_optional:!1,options:null},{name:"normalize",default_value:"False",param_type:["bool"],expected_shape:null,docstring:"This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``.",is_optional:!0,options:null},{name:"precompute",default_value:null,param_type:["bool","array"],expected_shape:null,docstring:"Whether to use a precomputed Gram matrix to speed up calculations. The Gram matrix can also be passed as argument. For sparse input this option is always ``True`` to preserve sparsity.",is_optional:!1,options:null},{name:"max_iter",default_value:null,param_type:["int"],expected_shape:null,docstring:"The maximum number of iterations",is_optional:!0,options:null},{name:"copy_X",default_value:"True",param_type:["bool"],expected_shape:null,docstring:"If ``True``, X will be copied; else, it may be overwritten.",is_optional:!0,options:null},{name:"tol",default_value:null,param_type:["float"],expected_shape:null,docstring:"The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``.",is_optional:!0,options:null},{name:"warm_start",default_value:!1,param_type:["bool"],expected_shape:null,docstring:"When set to ``True``, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.",is_optional:!0,options:null},{name:"positive",default_value:!1,param_type:["bool"],expected_shape:null,docstring:"When set to ``True``, forces the coefficients to be positive.",is_optional:!0,options:null},{name:"selection",default_value:"cyclic",param_type:["str"],expected_shape:null,docstring:"If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4.",is_optional:!0,options:["random","cyclic"]}],node_functions:[{name:"__init__",outputs:[],docstring:"None",inputs:[]},{name:"_decision_function",outputs:[{name:"T",returned:!0,param_type:["array"],docstring:"The predicted decision function"}],docstring:"Decision function of the linear model\n\n        Parameters\n        ----------\n        X : numpy array or scipy.sparse matrix of shape (n_samples, n_features)\n\n        Returns\n        -------\n        T : array, shape (n_samples,)\n            The predicted decision function\n        ",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"",is_optional:!1,options:null}]},{name:"_get_param_names",outputs:[],docstring:"Get parameter names for the estimator",inputs:[]},{name:"_preprocess_data",outputs:[],docstring:"\n    Centers data to have mean zero along axis 0. If fit_intercept=False or if\n    the X is a sparse matrix, no centering is done, but normalization can still\n    be applied. The function returns the statistics necessary to reconstruct\n    the input data, which are X_offset, y_offset, X_scale, such that the output\n\n        X = (X - X_offset) / X_scale\n\n    X_scale is the L2 norm of X - X_offset. If sample_weight is not None,\n    then the weighted mean of X and y is zero, and not the mean itself. If\n    return_mean=True, the mean, eventually weighted, is returned, independently\n    of whether X was centered (option used for optimization with sparse data in\n    coordinate_descend).\n\n    This is here because nearly all linear models will want their data to be\n    centered. This function also systematically makes y consistent with X.dtype\n    ",inputs:[]},{name:"_set_intercept",outputs:[],docstring:"Set the intercept_\n        ",inputs:[]},{name:"fit",outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Fit model with coordinate descent.",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:null,docstring:"Data",is_optional:!1,options:null},{name:"y",default_value:null,param_type:["array"],expected_shape:"(n_samples,) or (n_samples, n_targets)",docstring:"Target. Will be cast to X's dtype if necessary",is_optional:!1,options:null},{name:"check_input",default_value:"True",param_type:["bool"],expected_shape:null,docstring:"Allow to bypass several input checking. Don't use this parameter unless you know what you do.",is_optional:!0,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"predict",outputs:[{name:"C",returned:!0,param_type:["array"],docstring:"Returns predicted values."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Predict using the linear model",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"Samples.",is_optional:!1,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"score",outputs:[{name:"score",returned:!0,param_type:["float"],docstring:"R^2 of self.predict(X) wrt. y."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Returns the coefficient of determination R^2 of the prediction.\n\n        The coefficient R^2 is defined as (1 - u/v), where u is the residual\n        sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n        sum of squares ((y_true - y_true.mean()) ** 2).sum().\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always\n        predicts the expected value of y, disregarding the input features,\n        would get a R^2 score of 0.0.",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"Test samples. For some estimators this may be a precomputed kernel matrix instead, shape = (n_samples, n_samples_fitted], where n_samples_fitted is the number of samples used in the fitting for the estimator.",is_optional:!1,options:null},{name:"y",default_value:null,param_type:["array"],expected_shape:"(n_samples) or (n_samples, n_outputs)",docstring:"True values for X.",is_optional:!1,options:null},{name:"sample_weight",default_value:null,param_type:["array"],expected_shape:"[n_samples], optional",docstring:"Sample weights.",is_optional:!0,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]}],nodes:[]},{name:"Lasso",outputs:[{name:"coef_",returned:!1,param_type:["array"],docstring:"parameter vector (w in the cost function formula)"},{name:"sparse_coef_",returned:!1,param_type:["scipy.sparse.matrix"],docstring:"``sparse_coef_`` is a readonly property derived from ``coef_``"},{name:"intercept_",returned:!1,param_type:["array","float"],docstring:"independent term in decision function."},{name:"n_iter_",returned:!1,param_type:["array","int"],docstring:"number of iterations run by the coordinate descent solver to reach the specified tolerance."}],docstring:"Linear Model trained with L1 prior as regularizer (aka the Lasso)\n\n    The optimization objective for Lasso is::\n\n        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Technically the Lasso model is optimizing the same objective function as\n    the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).",inputs:[{name:"alpha",default_value:"1.0",param_type:["float"],expected_shape:null,docstring:"Constant that multiplies the L1 term. Defaults to 1.0. ``alpha = 0`` is equivalent to an ordinary least square, solved by the :class:`LinearRegression` object. For numerical reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised. Given this, you should use the :class:`LinearRegression` object.",is_optional:!0,options:null},{name:"fit_intercept",default_value:"True",param_type:["bool"],expected_shape:null,docstring:"Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (e.g. data is expected to be already centered).",is_optional:!0,options:null},{name:"normalize",default_value:"False",param_type:["bool"],expected_shape:null,docstring:"This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``.",is_optional:!0,options:null},{name:"precompute",default_value:"False",param_type:["bool","array"],expected_shape:null,docstring:"Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument. For sparse input this option is always ``True`` to preserve sparsity.",is_optional:!0,options:null},{name:"copy_X",default_value:"True",param_type:["bool"],expected_shape:null,docstring:"If ``True``, X will be copied; else, it may be overwritten.",is_optional:!0,options:null},{name:"max_iter",default_value:null,param_type:["int"],expected_shape:null,docstring:"The maximum number of iterations",is_optional:!0,options:null},{name:"tol",default_value:null,param_type:["float"],expected_shape:null,docstring:"The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``.",is_optional:!0,options:null},{name:"warm_start",default_value:null,param_type:["bool"],expected_shape:null,docstring:"When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.",is_optional:!0,options:null},{name:"positive",default_value:null,param_type:["bool"],expected_shape:null,docstring:"When set to ``True``, forces the coefficients to be positive.",is_optional:!0,options:null},{name:"selection",default_value:"cyclic",param_type:["LIST_VALID_OPTIONS"],expected_shape:null,docstring:"If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4.",is_optional:!0,options:["random","cyclic"]}],node_functions:[{name:"__init__",outputs:[],docstring:"None",inputs:[]},{name:"_decision_function",outputs:[{name:"T",returned:!0,param_type:["array"],docstring:"The predicted decision function"}],docstring:"Decision function of the linear model\n\n        Parameters\n        ----------\n        X : numpy array or scipy.sparse matrix of shape (n_samples, n_features)\n\n        Returns\n        -------\n        T : array, shape (n_samples,)\n            The predicted decision function\n        ",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"",is_optional:!1,options:null}]},{name:"_get_param_names",outputs:[],docstring:"Get parameter names for the estimator",inputs:[]},{name:"_preprocess_data",outputs:[],docstring:"\n    Centers data to have mean zero along axis 0. If fit_intercept=False or if\n    the X is a sparse matrix, no centering is done, but normalization can still\n    be applied. The function returns the statistics necessary to reconstruct\n    the input data, which are X_offset, y_offset, X_scale, such that the output\n\n        X = (X - X_offset) / X_scale\n\n    X_scale is the L2 norm of X - X_offset. If sample_weight is not None,\n    then the weighted mean of X and y is zero, and not the mean itself. If\n    return_mean=True, the mean, eventually weighted, is returned, independently\n    of whether X was centered (option used for optimization with sparse data in\n    coordinate_descend).\n\n    This is here because nearly all linear models will want their data to be\n    centered. This function also systematically makes y consistent with X.dtype\n    ",inputs:[]},{name:"_set_intercept",outputs:[],docstring:"Set the intercept_\n        ",inputs:[]},{name:"fit",outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Fit model with coordinate descent.",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"Data",is_optional:!1,options:null},{name:"y",default_value:null,param_type:["array"],expected_shape:"(n_samples,) or (n_samples, n_targets)",docstring:"Target. Will be cast to X's dtype if necessary",is_optional:!1,options:null},{name:"check_input",default_value:"True",param_type:["bool"],expected_shape:null,docstring:"Allow to bypass several input checking. Don't use this parameter unless you know what you do.",is_optional:!0,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"predict",outputs:[{name:"C",returned:!0,param_type:["array"],docstring:"Returns predicted values."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Predict using the linear model\n\n        Parameters\n        ----------\n        X : array_like or sparse matrix, shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        C : array, shape (n_samples,)\n            Returns predicted values.\n        ",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"Samples.",is_optional:!1,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"score",outputs:[{name:"score",returned:!0,param_type:["float"],docstring:"R^2 of self.predict(X) wrt. y."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Returns the coefficient of determination R^2 of the prediction.\n\n        The coefficient R^2 is defined as (1 - u/v), where u is the residual\n        sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n        sum of squares ((y_true - y_true.mean()) ** 2).sum().\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always\n        predicts the expected value of y, disregarding the input features,\n        would get a R^2 score of 0.0.",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"Test samples.",is_optional:!1,options:null},{name:"y",default_value:null,param_type:["array"],expected_shape:"(n_samples) or (n_samples, n_outputs)",docstring:"True values for X.",is_optional:!1,options:null},{name:"sample_weight",default_value:null,param_type:["array"],expected_shape:"[n_samples], optional",docstring:"Sample weights.",is_optional:!0,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]}],nodes:[]},{name:"LinearRegression",outputs:[{name:"coef_",returned:!1,param_type:["array"],docstring:"Estimated coefficients for the linear regression problem. If multiple targets are passed during the fit (y 2D), this is a 2D array of shape (n_targets, n_features), while if only one target is passed, this is a 1D array of length n_features."},{name:"intercept_",returned:!1,param_type:["array"],docstring:"Independent term in the linear model."}],docstring:"Ordinary least squares Linear Regression.",inputs:[{name:"fit_intercept",default_value:"True",param_type:["bool"],expected_shape:null,docstring:"whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (e.g. data is expected to be already centered).",is_optional:!0,options:null},{name:"normalize",default_value:"False",param_type:["bool"],expected_shape:null,docstring:"This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``.",is_optional:!0,options:null},{name:"copy_X",default_value:"True",param_type:["bool"],expected_shape:null,docstring:"If True, X will be copied; else, it may be overwritten.",is_optional:!0,options:null},{name:"n_jobs",default_value:"None",param_type:["int",null],expected_shape:null,docstring:"The number of jobs to use for the computation. This will only provide speedup for n_targets > 1 and sufficient large problems. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",is_optional:!0,options:null}],node_functions:[{name:"__init__",outputs:[],docstring:"None",inputs:[]},{name:"_decision_function",outputs:[],docstring:"None",inputs:[]},{name:"_get_param_names",outputs:[],docstring:"Get parameter names for the estimator",inputs:[]},{name:"_preprocess_data",outputs:[],docstring:"\n    Centers data to have mean zero along axis 0. If fit_intercept=False or if\n    the X is a sparse matrix, no centering is done, but normalization can still\n    be applied. The function returns the statistics necessary to reconstruct\n    the input data, which are X_offset, y_offset, X_scale, such that the output\n\n        X = (X - X_offset) / X_scale\n\n    X_scale is the L2 norm of X - X_offset. If sample_weight is not None,\n    then the weighted mean of X and y is zero, and not the mean itself. If\n    return_mean=True, the mean, eventually weighted, is returned, independently\n    of whether X was centered (option used for optimization with sparse data in\n    coordinate_descend).\n\n    This is here because nearly all linear models will want their data to be\n    centered. This function also systematically makes y consistent with X.dtype\n    ",inputs:[]},{name:"_set_intercept",outputs:[],docstring:"Set the intercept_\n        ",inputs:[]},{name:"fit",outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Fit linear model",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"Training data",is_optional:!1,options:null},{name:"y",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_targets)",docstring:"Target values. Will be cast to X's dtype if necessary",is_optional:!1,options:null},{name:"sample_weight",default_value:null,param_type:["array"],expected_shape:"[n_samples]",docstring:"Individual weights for each sample  .. versionadded:: 0.17    parameter *sample_weight* support to LinearRegression.",is_optional:!1,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"predict",outputs:[{name:"C",returned:!0,param_type:["array"],docstring:"Returns predicted values."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Predict using the linear model",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"Samples.",is_optional:!1,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"score",outputs:[{name:"score",returned:!0,param_type:["float"],docstring:"R^2 of self.predict(X) wrt. y."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Returns the coefficient of determination R^2 of the prediction.\n\n        The coefficient R^2 is defined as (1 - u/v), where u is the residual\n        sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n        sum of squares ((y_true - y_true.mean()) ** 2).sum().\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always\n        predicts the expected value of y, disregarding the input features,\n        would get a R^2 score of 0.0.",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"Test samples. For some estimators this may be a precomputed kernel matrix instead, shape = (n_samples, n_samples_fitted], where n_samples_fitted is the number of samples used in the fitting for the estimator.",is_optional:!1,options:null},{name:"y",default_value:null,param_type:["array"],expected_shape:"(n_samples) or (n_samples, n_outputs)",docstring:"True values for X.",is_optional:!1,options:null},{name:"sample_weight",default_value:null,param_type:["array"],expected_shape:"[n_samples], optional",docstring:"Sample weights.",is_optional:!0,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]}],nodes:[]},{name:"LogisticRegression",outputs:[{name:"classes_",returned:!1,param_type:["array"],docstring:"A list of class labels known to the classifier."},{name:"coef_",returned:!1,param_type:["array"],docstring:"Coefficient of the features in the decision function.  `coef_` is of shape (1, n_features) when the given problem is binary. In particular, when `multi_class='multinomial'`, `coef_` corresponds to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False)."},{name:"intercept_",returned:!1,param_type:["array"],docstring:"Intercept (a.k.a. bias) added to the decision function.  If `fit_intercept` is set to False, the intercept is set to zero. `intercept_` is of shape (1,) when the given problem is binary. In particular, when `multi_class='multinomial'`, `intercept_` corresponds to outcome 1 (True) and `-intercept_` corresponds to outcome 0 (False)."},{name:"n_iter_",returned:!1,param_type:["array"],docstring:"Actual number of iterations for all classes. If binary or multinomial, it returns only 1 element. For liblinear solver, only the maximum number of iteration across all classes is given.  .. versionchanged:: 0.20      In SciPy <= 1.0.0 the number of lbfgs iterations may exceed     ``max_iter``. ``n_iter_`` will now report at most ``max_iter``."}],docstring:"Logistic Regression (aka logit, MaxEnt) classifier.\n\n    In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n    scheme if the 'multi_class' option is set to 'ovr', and uses the cross-\n    entropy loss if the 'multi_class' option is set to 'multinomial'.\n    (Currently the 'multinomial' option is supported only by the 'lbfgs',\n    'sag' and 'newton-cg' solvers.)\n\n    This class implements regularized logistic regression using the\n    'liblinear' library, 'newton-cg', 'sag' and 'lbfgs' solvers. It can handle\n    both dense and sparse input. Use C-ordered arrays or CSR matrices\n    containing 64-bit floats for optimal performance; any other input format\n    will be converted (and copied).\n\n    The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n    with primal formulation. The 'liblinear' solver supports both L1 and L2\n    regularization, with a dual formulation only for the L2 penalty.",inputs:[{name:"penalty",default_value:"l2",param_type:["LIST_VALID_OPTIONS"],expected_shape:null,docstring:"Used to specify the norm used in the penalization. The 'newton-cg', 'sag' and 'lbfgs' solvers support only l2 penalties.  .. versionadded:: 0.19    l1 penalty with SAGA solver (allowing 'multinomial' + L1)",is_optional:!0,options:["l1","l2","elasticnet","none"]},{name:"dual",default_value:"False",param_type:["bool"],expected_shape:null,docstring:"Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features.",is_optional:!0,options:null},{name:"tol",default_value:"1e-4",param_type:["float"],expected_shape:null,docstring:"Tolerance for stopping criteria.",is_optional:!0,options:null},{name:"C",default_value:"1.0",param_type:["float"],expected_shape:null,docstring:"Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.",is_optional:!0,options:null},{name:"fit_intercept",default_value:"True",param_type:["bool"],expected_shape:null,docstring:"Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function.",is_optional:!0,options:null},{name:"intercept_scaling",default_value:"1.",param_type:["float"],expected_shape:null,docstring:"Useful only when the solver 'liblinear' is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a \"synthetic\" feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes ``intercept_scaling * synthetic_feature_weight``.  Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased.",is_optional:!0,options:null},{name:"class_weight",default_value:"None",param_type:["dict","str",null],expected_shape:null,docstring:'Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.  The "balanced" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.  Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.',is_optional:!0,options:null},{name:"solver",default_value:"liblinear.",param_type:["LIST_VALID_OPTIONS"],expected_shape:null,docstring:"Algorithm to use in the optimization problem.  - For small datasets, 'liblinear' is a good choice, whereas 'sag' and   'saga' are faster for large ones. - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'   handle multinomial loss; 'liblinear' is limited to one-versus-rest   schemes. - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas   'liblinear' and 'saga' handle L1 penalty.  Note that 'sag' and 'saga' fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.",is_optional:!0,options:["newton-cg","lbfgs","liblinear","sag","saga'}"]},{name:"max_iter",default_value:"100",param_type:["int"],expected_shape:null,docstring:"Useful only for the newton-cg, sag and lbfgs solvers. Maximum number of iterations taken for the solvers to converge.",is_optional:!0,options:null},{name:"multi_class",default_value:"ovr",param_type:["LIST_VALID_OPTIONS"],expected_shape:null,docstring:"If the option chosen is 'ovr', then a binary problem is fit for each label. For 'multinomial' the loss minimised is the multinomial loss fit across the entire probability distribution, *even when the data is binary*. 'multinomial' is unavailable when solver='liblinear'. 'auto' selects 'ovr' if the data is binary, or if solver='liblinear', and otherwise selects 'multinomial'.",is_optional:!0,options:["ovr","multinomial","auto"]},{name:"verbose",default_value:"0",param_type:["int"],expected_shape:null,docstring:"For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.",is_optional:!0,options:null},{name:"warm_start",default_value:"False",param_type:["bool"],expected_shape:null,docstring:"When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver.",is_optional:!0,options:null},{name:"n_jobs",default_value:"None",param_type:["int",null],expected_shape:null,docstring:"Number of CPU cores used when parallelizing over classes if multi_class='ovr'\". This parameter is ignored when the ``solver`` is set to 'liblinear' regardless of whether 'multi_class' is specified or not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",is_optional:!0,options:null}],node_functions:[{name:"__init__",outputs:[],docstring:"None",inputs:[]},{name:"_get_param_names",outputs:[],docstring:"Get parameter names for the estimator",inputs:[]},{name:"_predict_proba_lr",outputs:[],docstring:"Probability estimation for OvR logistic regression.\n\n        Positive class probabilities are computed as\n        1. / (1. + np.exp(-self.decision_function(X)));\n        multiclass is handled by normalizing that over all classes.\n        ",inputs:[]},{name:"decision_function",outputs:[{name:"array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)",returned:!0,param_type:[null],docstring:"Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Predict confidence scores for samples.\n\n        The confidence score for a sample is the signed distance of that\n        sample to the hyperplane.\n\n        Parameters\n        ----------\n        X : array_like or sparse matrix, shape (n_samples, n_features)\n            Samples.\n\n        Returns\n        -------\n        array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n            Confidence scores per (sample, class) combination. In the binary\n            case, confidence score for self.classes_[1] where >0 means this\n            class would be predicted.\n        ",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"Samples.",is_optional:!1,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"fit",outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Fit the model according to the given training data.",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"Training vector, where n_samples is the number of samples and n_features is the number of features.",is_optional:!1,options:null},{name:"y",default_value:null,param_type:["array"],expected_shape:"(n_samples,)",docstring:"Target vector relative to X.",is_optional:!1,options:null},{name:"sample_weight",default_value:null,param_type:["array"],expected_shape:"(n_samples,) optional",docstring:"Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.",is_optional:!0,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"predict",outputs:[{name:"C",returned:!0,param_type:["array"],docstring:"Predicted class label per sample."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Predict class labels for samples in X.",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"Samples.",is_optional:!1,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"predict_log_proba",outputs:[{name:"T",returned:!0,param_type:["array"],docstring:"Returns the log-probability of the sample for each class in the model, where classes are ordered as they are in ``classes_``."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Log of probability estimates.\n\n        The returned estimates for all classes are ordered by the\n        label of classes.",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"[n_samples, n_features]",docstring:"",is_optional:!1,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"predict_proba",outputs:[{name:"T",returned:!0,param_type:["array"],docstring:"Returns the probability of the sample for each class in the model, where classes are ordered as they are in ``classes_``."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:'Probability estimates.\n\n        The returned estimates for all classes are ordered by the\n        label of classes.\n\n        For a multi_class problem, if multi_class is set to be "multinomial"\n        the softmax function is used to find the predicted probability of\n        each class.\n        Else use a one-vs-rest approach, i.e calculate the probability\n        of each class assuming it to be positive using the logistic function.\n        and normalize these values across all the classes.',inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"[n_samples, n_features]",docstring:"",is_optional:!1,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"score",outputs:[{name:"score",returned:!0,param_type:["float"],docstring:"Mean accuracy of self.predict(X) wrt. y."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Returns the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"Test samples.",is_optional:!1,options:null},{name:"y",default_value:null,param_type:["array"],expected_shape:"(n_samples) or (n_samples, n_outputs)",docstring:"True labels for X.",is_optional:!1,options:null},{name:"sample_weight",default_value:null,param_type:["array"],expected_shape:"[n_samples]",docstring:"Sample weights.",is_optional:!0,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]}],nodes:[]},{name:"Ridge",outputs:[{name:"coef_",returned:!1,param_type:["array"],docstring:"Weight vector(s)."},{name:"intercept_",returned:!1,param_type:["LIST_VALID_OPTIONS","array","float"],docstring:"Independent term in decision function. Set to 0.0 if ``fit_intercept = False``."},{name:"n_iter_",returned:!1,param_type:["array",null],docstring:"Actual number of iterations for each target. Available only for sag and lsqr solvers. Other solvers will return None.  .. versionadded:: 0.17"}],docstring:"Linear least squares with l2 regularization.\n\n    Minimizes the objective function::\n\n    ||y - Xw||^2_2 + alpha * ||w||^2_2\n\n    This model solves a regression model where the loss function is\n    the linear least squares function and regularization is given by\n    the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n    This estimator has built-in support for multi-variate regression\n    (i.e., when y is a 2d-array of shape [n_samples, n_targets]).",inputs:[{name:"alpha",default_value:null,param_type:["array","float"],expected_shape:"(n_targets)",docstring:"Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``C^-1`` in other linear models such as LogisticRegression or LinearSVC. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number.",is_optional:!1,options:null},{name:"fit_intercept",default_value:null,param_type:["bool"],expected_shape:null,docstring:"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).",is_optional:!1,options:null},{name:"normalize",default_value:"False",param_type:["bool"],expected_shape:null,docstring:"This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``.",is_optional:!0,options:null},{name:"copy_X",default_value:"True",param_type:["bool"],expected_shape:null,docstring:"If True, X will be copied; else, it may be overwritten.",is_optional:!0,options:null},{name:"max_iter",default_value:null,param_type:["int"],expected_shape:null,docstring:"Maximum number of iterations for conjugate gradient solver. For 'sparse_cg' and 'lsqr' solvers, the default value is determined by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.",is_optional:!0,options:null},{name:"tol",default_value:null,param_type:["float"],expected_shape:null,docstring:"Precision of the solution.",is_optional:!1,options:null},{name:"solver",default_value:null,param_type:["LIST_VALID_OPTIONS"],expected_shape:null,docstring:"Solver to use in the computational routines:  - 'auto' chooses the solver automatically based on the type of data.  - 'svd' uses a Singular Value Decomposition of X to compute the Ridge   coefficients. More stable for singular matrices than   'cholesky'.  - 'cholesky' uses the standard scipy.linalg.solve function to   obtain a closed-form solution.  - 'sparse_cg' uses a conjugate gradient solver. As an iterative algorithm, this solver is   more appropriate than 'cholesky' for large-scale data   (possibility to set `tol` and `max_iter`).  - 'lsqr' uses a dedicated regularized least-squares routine. It is the fastest and uses an iterative   procedure.  - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses   its improved, unbiased version named SAGA. Both methods also use an   iterative procedure, and are often faster than other solvers when   both n_samples and n_features are large. Note that 'sag' and   'saga' fast convergence is only guaranteed on features with   approximately the same scale. You can preprocess the data with a   scaler from sklearn.preprocessing.",is_optional:!1,options:["auto","svd","cholesky","lsqr","sparse_cg","sag","saga"]}],node_functions:[{name:"__init__",outputs:[],docstring:"None",inputs:[]},{name:"_decision_function",outputs:[],docstring:"None",inputs:[]},{name:"_get_param_names",outputs:[],docstring:"Get parameter names for the estimator",inputs:[]},{name:"_preprocess_data",outputs:[],docstring:"\n    Centers data to have mean zero along axis 0. If fit_intercept=False or if\n    the X is a sparse matrix, no centering is done, but normalization can still\n    be applied. The function returns the statistics necessary to reconstruct\n    the input data, which are X_offset, y_offset, X_scale, such that the output\n\n        X = (X - X_offset) / X_scale\n\n    X_scale is the L2 norm of X - X_offset. If sample_weight is not None,\n    then the weighted mean of X and y is zero, and not the mean itself. If\n    return_mean=True, the mean, eventually weighted, is returned, independently\n    of whether X was centered (option used for optimization with sparse data in\n    coordinate_descend).\n\n    This is here because nearly all linear models will want their data to be\n    centered. This function also systematically makes y consistent with X.dtype\n    ",inputs:[]},{name:"_set_intercept",outputs:[],docstring:"Set the intercept_\n        ",inputs:[]},{name:"fit",outputs:[{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Fit Ridge regression model",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"[n_samples, n_features]",docstring:"Training data",is_optional:!1,options:null},{name:"y",default_value:null,param_type:["array"],expected_shape:"[n_samples] or [n_samples, n_targets]",docstring:"Target values",is_optional:!1,options:null},{name:"sample_weight",default_value:null,param_type:["array","float"],expected_shape:"[n_samples]",docstring:"Individual weights for each sample",is_optional:!1,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"predict",outputs:[{name:"C",returned:!0,param_type:["array"],docstring:"Returns predicted values."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Predict using the linear model",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"Samples.",is_optional:!1,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]},{name:"score",outputs:[{name:"score",returned:!0,param_type:["float"],docstring:"R^2 of self.predict(X) wrt. y."},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}],docstring:"Returns the coefficient of determination R^2 of the prediction.\n\n        The coefficient R^2 is defined as (1 - u/v), where u is the residual\n        sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n        sum of squares ((y_true - y_true.mean()) ** 2).sum().\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always\n        predicts the expected value of y, disregarding the input features,\n        would get a R^2 score of 0.0.",inputs:[{name:"X",default_value:null,param_type:["array"],expected_shape:"(n_samples, n_features)",docstring:"Test samples.",is_optional:!1,options:null},{name:"y",default_value:null,param_type:["array"],expected_shape:"(n_samples) or (n_samples, n_outputs)",docstring:"True values for X.",is_optional:!1,options:null},{name:"sample_weight",default_value:null,param_type:["array"],expected_shape:"[n_samples], optional",docstring:"Sample weights.",is_optional:!0,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}]}],nodes:[]}],library:"sklearn"}},function(e,t,n){"use strict";t.a={name:"pandas",docstring:"",inputs:[],outputs:[],node_functions:[{name:"read_csv",docstring:"\nRead a comma-separated values (csv) file into DataFrame.",inputs:[{name:"filepath_or_buffer",docstring:"Choose a CSV file to upload.",param_type:["object","str"],expected_shape:null,is_optional:!1,default_value:null,options:null},{name:"sep",docstring:"Delimiter to use. Use \\s+ for whitespace",param_type:["str"],expected_shape:null,is_optional:!0,default_value:",",options:null},{name:"header",docstring:"Row number(s) to use as the column names, and the start of the data.  Default behavior is to infer the column names: if no names are passed the behavior is identical to ``header=0`` and column names are inferred from the first line of the file, if column names are passed explicitly then the behavior is identical to ``header=None``. Explicitly pass ``header=0`` to be able to replace existing names. The header can be a list of integers that specify row locations for a multi-index on the columns e.g. [0,1,3]. Intervening rows that are not specified will be skipped (e.g. 2 in this example is skipped). Note that this parameter ignores commented lines and empty lines if ``skip_blank_lines=True``, so ``header=0`` denotes the first line of data rather than the first line of the file.",param_type:["int","list"],expected_shape:null,is_optional:!0,default_value:"infer",options:null},{name:"names",docstring:"List of column names to use. If file contains no header row, then you should explicitly pass ``header=None``. Duplicates in this list are not allowed.",param_type:["array"],expected_shape:null,is_optional:!0,default_value:null,options:null},{name:"index_col",docstring:"Column(s) to use as the row labels of the ``DataFrame``, either given as string name or column index. If a sequence of int / str is given, a MultiIndex is used.  Note: ``index_col=False`` can be used to force pandas to *not* use the first column as the index, e.g. when you have a malformed file with delimiters at the end of each line.",param_type:["int","str",null],expected_shape:null,is_optional:!0,default_value:"``None``",options:null},{name:"usecols",docstring:"Return a subset of the columns. If list-like, all elements must either be positional (i.e. integer indices into the document columns) or strings that correspond to column names provided either by the user in `names` or inferred from the document header row(s). For example, a valid list-like `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``. Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.",param_type:["list","callable"],expected_shape:null,is_optional:!0,default_value:null,options:null},{name:"mangle_dupe_cols",docstring:"Duplicate columns will be specified as 'X', 'X.1', ...'X.N', rather than 'X'...'X'. Passing in False will cause data to be overwritten if there are duplicate names in the columns.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"skiprows",docstring:"Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file. By default no rows are skipped",param_type:["int","list","callable"],expected_shape:null,is_optional:!0,default_value:null,options:null},{name:"skipfooter",docstring:"Number of lines at bottom of file to skip (Unsupported with engine='c').",param_type:["int"],expected_shape:null,is_optional:!0,default_value:"0",options:null},{name:"nrows",docstring:"Number of rows of file to read. Useful for reading pieces of large files.",param_type:["int"],expected_shape:null,is_optional:!0,default_value:null,options:null},{name:"na_values",docstring:"Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values.  By default the following values are interpreted as NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NA', 'NULL', 'NaN', 'n/a', 'nan', 'null'.",param_type:["str","dict","list"],expected_shape:null,is_optional:!0,default_value:null,options:null},{name:"keep_default_na",docstring:"Whether or not to include the default NaN values when parsing the data. Depending on whether `na_values` is passed in, the behavior is as follows:  * If `keep_default_na` is True, and `na_values` are specified, `na_values`   is appended to the default NaN values used for parsing. * If `keep_default_na` is True, and `na_values` are not specified, only   the default NaN values are used for parsing. * If `keep_default_na` is False, and `na_values` are specified, only   the NaN values specified `na_values` are used for parsing. * If `keep_default_na` is False, and `na_values` are not specified, no   strings will be parsed as NaN.  Note that if `na_filter` is passed in as False, the `keep_default_na` and `na_values` parameters will be ignored.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"na_filter",docstring:"Detect missing value markers (empty strings and the value of na_values). In data without any NAs, passing na_filter=False can improve the performance of reading a large file.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"skip_blank_lines",docstring:"If True, skip over blank lines rather than interpreting as NaN values.",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:"True",options:null},{name:"compression",docstring:"For on-the-fly decompression of on-disk data. If 'infer' and `filepath_or_buffer` is path-like, then detect compression from the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no decompression). If using 'zip', the ZIP file must contain only one data file to be read in. Set to None for no decompression.  .. versionadded:: 0.18.1 support for 'zip' and 'xz' compression.",param_type:["LIST_VALID_OPTIONS",null],expected_shape:null,is_optional:!0,default_value:"infer",options:["infer","gzip","bz2","zip","xz","None"]},{name:"thousands",docstring:"Thousands separator.",param_type:["str"],expected_shape:null,is_optional:!0,default_value:null,options:null},{name:"decimal",docstring:"Character to recognize as decimal point (e.g. use ',' for European data).",param_type:["str"],expected_shape:null,is_optional:!0,default_value:".",options:null},{name:"comment",docstring:"Indicates remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character. Like empty lines (as long as ``skip_blank_lines=True``), fully commented lines are ignored by the parameter `header` but not by `skiprows`. For example, if ``comment='#'``, parsing ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being treated as the header.",param_type:["str"],expected_shape:null,is_optional:!0,default_value:null,options:null},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],expected_shape:null,is_optional:!0,default_value:!1,options:null}],outputs:[{name:"data",docstring:"A comma-separated values (csv) file is returned as two-dimensional data structure with labeled axes.",param_type:["dataframe"],returned:!0},{name:"obj",docstring:"True/False flag to indicate reusing the same instance instead of creating a new one",param_type:["bool"],returned:!0}]}],nodes:[],library:"pandas",module:null}},,,,,,,,,,,,,,,,,,,,function(e,t,n){"use strict";function a(e){return function(t){var n=t.target||t.cyTarget,a=n.data().id,s={group:"nodes",data:e.cy.getElementById(a).data(),position:e.cy.getElementById(a).position()};e.$store.commit("setSelectedNodeId",a),e.$store.commit("setSelectedNodeElem",s),e.$store.commit("showEditNode")}}function s(e){return function(t){var n=t.target||t.cyTarget,a=n.data().id,s={group:"edges",data:e.cy.getElementById(a).data()};e.$store.commit("setSelectedEdge",s),e.$store.commit("showEditEdge")}}function i(e){return function(t){var n=t.target||t.cyTarget,a=n.data().id;e.cy.remove(e.cy.$id(a))}}function o(e){return function(t){var n=t.target||t.cyTarget,a=n.data().id;e.cy.remove(e.cy.$id(a))}}function r(e){return function(t){e.$store.commit("showAddNode");var n=t.position||t.cyPosition;e.$store.commit("setCyClickPos",{x:n.x,y:n.y})}}function l(e){return function(t,n,a){if(console.log(a.data()),a){console.log("source node data",t.data().params),console.log("target node data",n.data().params);var s=t.data().params.op,i=n.data().params.inp;console.log("sfunc",s),console.log("tfnc",i);var o={group:"edges",data:a.data()};o.data.inputs=p.a.cloneDeep(s),o.data.outputs=p.a.cloneDeep(i),e.$store.commit("setSelectedEdge",o),e.$store.commit("showEditEdge")}}}var u=n(70),p=n.n(u);t.a={getEditNodeOnClickFunction:a,getEditEdgeOnClickFunction:s,getRemoveNodeOnClickFunction:i,getRemoveEdgeOnClickFunction:o,getAddNodeOnClickFunction:r,getEdgeDropFunction:l}},function(e,t,n){"use strict";var a={addNodeVisible:!1,editNodeVisible:!1,editEdgeVisible:!1,loadGraphVisible:!1,saveGraphVisible:!1,helpVisible:!1,updateGraphVisible:!1,wizardVisible:!1},s={addNodeVisible:function(e){return e.addNodeVisible},editNodeVisible:function(e){return e.editNodeVisible},editEdgeVisible:function(e){return e.editEdgeVisible},loadGraphVisible:function(e){return e.loadGraphVisible},saveGraphVisible:function(e){return e.saveGraphVisible},updateGraphVisible:function(e){return e.updateGraphVisible},wizardVisible:function(e){return e.wizardVisible},helpVisible:function(e){return e.helpVisible}},i={},o={showAddNode:function(e){e.addNodeVisible=!0},showEditNode:function(e){e.editNodeVisible=!0},showEditEdge:function(e){e.editEdgeVisible=!0},showSaveGraph:function(e){e.saveGraphVisible=!0},showHelp:function(e){e.helpVisible=!0},hideHelp:function(e){e.helpVisible=!1},showLoadGraph:function(e){e.loadGraphVisible=!0},showUpdateGraph:function(e){e.updateGraphVisible=!0},showWizard:function(e){e.wizardVisible=!0},hideAddNode:function(e){e.addNodeVisible=!1},hideEditNode:function(e){e.editNodeVisible=!1},hideEditEdge:function(e){e.editEdgeVisible=!1},hideSaveGraph:function(e){e.saveGraphVisible=!1},hideLoadGraph:function(e){e.loadGraphVisible=!1},hideUpdateGraph:function(e){e.updateGraphVisible=!1},hideWizard:function(e){e.wizardVisible=!1}};t.a={state:a,getters:s,actions:i,mutations:o}},function(e,t,n){"use strict";var a,s=n(72),i=n.n(s),o=n(73),r=n(44),l=n(74),u={authenticating:!1,error:!1,token:null},p={isAuthenticated:function(e){return!!e.token},loginFailed:function(e){return e.error}},d={login:function(e,t){var n=e.commit,a=t.username,s=t.password;return n(l.e),o.a.login(a,s).then(function(e){var t=e.data;return n(l.v,t.key)}).then(function(){return n(l.g)}).catch(function(){return n(l.f)})},logout:function(e){var t=e.commit;return o.a.logout().then(function(){return t(l.h)}).finally(function(){return t(l.u)})},initialize:function(e){var t=e.commit,n=localStorage.getItem("TOKEN_STORAGE_KEY");n?t(l.v,n):t(l.u)}},c=(a={},i()(a,l.e,function(e){e.authenticating=!0,e.error=!1}),i()(a,l.f,function(e){e.authenticating=!1,e.error=!0}),i()(a,l.g,function(e){e.authenticating=!1,e.error=!1}),i()(a,l.h,function(e){e.authenticating=!1,e.error=!1}),i()(a,l.v,function(e,t){localStorage.setItem("TOKEN_STORAGE_KEY",t),r.a.defaults.headers.Authorization="Token "+t,e.token=t}),i()(a,l.u,function(e){localStorage.removeItem("TOKEN_STORAGE_KEY"),delete r.a.defaults.headers.Authorization,e.token=null}),a);t.a={namespaced:!0,state:u,getters:p,actions:d,mutations:c}},,,,function(e,t,n){"use strict";var a,s=n(72),i=n.n(s),o=n(73),r=n(74);t.a={namespaced:!0,state:{emailCompleted:!1,emailError:!1,emailLoading:!1,resetCompleted:!1,resetError:!1,resetLoading:!1},actions:{resetPassword:function(e,t){var n=e.commit,a=t.uid,s=t.token,i=t.password1,l=t.password2;return n(r.m),o.a.resetAccountPassword(a,s,i,l).then(function(){return n(r.p)}).catch(function(){return n(r.o)})},sendPasswordResetEmail:function(e,t){var n=e.commit,a=t.email;return n(r.i),o.a.sendAccountPasswordResetEmail(a).then(function(){return n(r.l)}).catch(function(){return n(r.k)})},clearResetStatus:function(e){(0,e.commit)(r.n)},clearEmailStatus:function(e){(0,e.commit)(r.j)}},mutations:(a={},i()(a,r.m,function(e){e.resetLoading=!0}),i()(a,r.n,function(e){e.resetCompleted=!1,e.resetError=!1,e.resetLoading=!1}),i()(a,r.o,function(e){e.resetError=!0,e.resetLoading=!1}),i()(a,r.p,function(e){e.resetCompleted=!0,e.resetError=!1,e.resetLoading=!1}),i()(a,r.i,function(e){e.emailLoading=!0}),i()(a,r.j,function(e){e.emailCompleted=!1,e.emailError=!1,e.emailLoading=!1}),i()(a,r.k,function(e){e.emailError=!0,e.emailLoading=!1}),i()(a,r.l,function(e){e.emailCompleted=!0,e.emailError=!1,e.emailLoading=!1}),a)}},function(e,t,n){"use strict";var a,s=n(72),i=n.n(s),o=n(73),r=n(74);t.a={namespaced:!0,state:{activationCompleted:!1,activationError:!1,activationLoading:!1,registrationCompleted:!1,registrationError:!1,registrationLoading:!1},actions:{createAccount:function(e,t){var n=e.commit,a=t.username,s=t.password1,i=t.password2,l=t.email;return n(r.q),o.a.createAccount(a,s,i,l).then(function(){return n(r.t)}).catch(function(){return n(r.s)})},activateAccount:function(e,t){var n=e.commit,a=t.key;return n(r.a),o.a.verifyAccountEmail(a).then(function(){return n(r.d)}).catch(function(){return n(r.c)})},clearRegistrationStatus:function(e){(0,e.commit)(r.r)},clearActivationStatus:function(e){(0,e.commit)(r.b)}},mutations:(a={},i()(a,r.a,function(e){e.activationLoading=!0}),i()(a,r.b,function(e){e.activationCompleted=!1,e.activationError=!1,e.activationLoading=!1}),i()(a,r.c,function(e){e.activationError=!0,e.activationLoading=!1}),i()(a,r.d,function(e){e.activationCompleted=!0,e.activationError=!1,e.activationLoading=!1}),i()(a,r.q,function(e){e.registrationLoading=!0}),i()(a,r.r,function(e){e.registrationCompleted=!1,e.registrationError=!1,e.registrationLoading=!1}),i()(a,r.s,function(e){e.registrationError=!0,e.registrationLoading=!1}),i()(a,r.t,function(e){e.registrationCompleted=!0,e.registrationError=!1,e.registrationLoading=!1}),a)}},function(e,t,n){"use strict";var a=n(361),s={usergraphs:null,rootgraphs:null,error:!1,currentGraphId:"1",currentGraphType:"root",currentGraphTitle:"init_template",graphruns:null},i={usergraphs:function(e){return e.usergraphs},rootgraphs:function(e){return e.rootgraphs},error:function(e){return e.error},currentGraphId:function(e){return e.currentGraphId},currentGraphType:function(e){return e.currentGraphType},currentGraphTitle:function(e){return e.currentGraphTitle},graphruns:function(e){return e.graphruns}},o={updateRootGraphs:function(e){var t=e.commit;return a.a.getRootGraphs().then(function(e){var n=e.data;return t("setRootGraphs",n)}).then(function(){return t("apiSuccess")}).catch(function(){return t("apiError")})},updateUserGraphs:function(e){var t=e.commit;return a.a.getUserGraphs().then(function(e){var n=e.data;return t("setUserGraphs",n)}).then(function(){return t("apiSuccess")}).catch(function(){return t("apiError")})},updateCurrentUserGraph:function(e,t){var n=e.commit;return a.a.updateCurrentUserGraph(t.id,{title:s.currentGraphTitle,content:t.content}).then(function(){return n("apiSuccess")}).catch(function(){return n("apiError")})},saveCurrentUserGraph:function(e,t){var n=e.commit;return a.a.saveCurrentUserGraph({title:t.title,content:t.content}).then(function(){return n("apiSuccess")}).catch(function(){return n("apiError")})},setCurrentGraphId:function(e,t){(0,e.commit)("setCurrentGraphId",t)},setCurrentGraphType:function(e,t){(0,e.commit)("setCurrentGraphType",t)},setCurrentGraphTitle:function(e,t){(0,e.commit)("setCurrentGraphTitle",t)},runCurrentGraph:function(e){var t=e.commit;return a.a.runGraph({graph_id:s.currentGraphId}).then(function(){return t("apiSuccess")}).catch(function(){return t("apiError")})},setGraphRuns:function(e){var t=e.commit;return a.a.getGraphRuns().then(function(e){var n=e.data;return t("setGraphRuns",n)}).then(function(){return t("apiSuccess")}).catch(function(){return t("apiError")})}},r={setRootGraphs:function(e,t){e.rootgraphs=t},setUserGraphs:function(e,t){e.usergraphs=t},apiError:function(e){e.error=!0},apiSuccess:function(e){e.error=!1},setCurrentGraphId:function(e,t){e.currentGraphId=t},setCurrentGraphType:function(e,t){e.currentGraphType=t},setCurrentGraphTitle:function(e,t){e.currentGraphTitle=t},setGraphRuns:function(e,t){var n=[];t.forEach(function(e){var t={id:e.content,graph:e.graph.title,start_time:e.created,status:e.state,actions:"<a id='"+e.content+"' href='api/downloadGraphRun?run="+e.content+"' download>download</a>"};t.status=0===e.state?"Created":1===e.state?"Requested":2===e.state?"Queued":3===e.state?"Started":4===e.state?"Error":5===e.state?"Success":6===e.state?"Error":"NA",n.push(t)}),e.graphruns=n}};t.a={namespaced:!0,state:s,getters:i,actions:o,mutations:r}},function(e,t,n){"use strict";var a=n(44);t.a={getUserGraphs:function(){return a.a.get("/api/usergraphs/")},getRootGraphs:function(){return a.a.get("/api/rootgraphs/")},updateCurrentUserGraph:function(e,t){return a.a.put("/api/graphs/"+e+"/",t)},saveCurrentUserGraph:function(e){return a.a.post("/api/graphs/",e)},runGraph:function(e){return a.a.post("/api/rungraph/",e)},getGraphRuns:function(){return a.a.get("/api/graphruns/")}}},function(e,t,n){"use strict";var a=n(135),s=n(404),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";t.a={items:[{name:"Home",url:"/landing",icon:"icon-drawer"},{name:"Dashboard",url:"/dashboard",icon:"icon-speedometer"},{name:"Results",url:"/results",icon:"icon-direction"},{name:"Analysis",url:"/analysis",icon:"icon-globe"},{name:"Visualizations",url:"http://localhost:8000/",icon:"icon-pie-chart"}]}},function(e,t,n){"use strict";var a=n(365),s=n(367),i=(n(369),n(371)),o=n(373),r=n(377);n(402);n.d(t,"a",function(){return a.a}),n.d(t,"b",function(){return s.a}),n.d(t,"c",function(){return i.a}),n.d(t,"d",function(){return o.a}),n.d(t,"e",function(){return r.a})},function(e,t,n){"use strict";var a=n(136),s=n(366),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement;return(e._self._c||t)("aside",{staticClass:"aside-menu"})},s=[]},function(e,t,n){"use strict";var a=n(137),s=n(368),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("ol",{staticClass:"breadcrumb"},e._l(e.list,function(t,a){return n("li",{staticClass:"breadcrumb-item"},[e.isLast(a)?n("span",{staticClass:"active"},[e._v(e._s(e.showName(t)))]):n("router-link",{attrs:{to:t}},[e._v(e._s(e.showName(t)))])],1)}))},s=[]},function(e,t,n){"use strict";var a=n(138),s=n(370),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement;return(e._self._c||t)("div",{class:e.classList},[e._t("default")],2)},s=[]},function(e,t,n){"use strict";var a=n(139),s=n(372),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement;return(e._self._c||t)("footer",{staticClass:"app-footer"})},s=[]},function(e,t,n){"use strict";var a=n(140),s=n(376),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";var a=n(141),s=n(375),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("b-nav-item-dropdown",{attrs:{right:"","no-caret":""}},[n("template",{slot:"button-content"},[n("i",{staticClass:"fa fa-user-circle fa-lg"})]),e._v(" "),n("b-dropdown-item",[n("i",{staticClass:"fa fa-lock"}),e._v(" "),n("router-link",{attrs:{to:"/logout"}},[e._v("logout")])],1)],2)},s=[]},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("header",{staticClass:"app-header navbar"},[n("button",{staticClass:"navbar-toggler mobile-sidebar-toggler d-lg-none",attrs:{type:"button"},on:{click:e.mobileSidebarToggle}},[n("span",{staticClass:"navbar-toggler-icon"})]),e._v(" "),n("b-link",{staticClass:"navbar-brand",attrs:{to:"#"}}),e._v(" "),n("button",{staticClass:"navbar-toggler sidebar-toggler d-md-down-none mr-auto",attrs:{type:"button"},on:{click:e.sidebarToggle}},[n("span",{staticClass:"navbar-toggler-icon"})]),e._v(" "),n("b-navbar-nav",{staticClass:"ml-auto"},[n("HeaderDropdown")],1)],1)},s=[]},function(e,t,n){"use strict";function a(e){n(378)}var s=n(142),i=n(401),o=n(2),r=a,l=Object(o.a)(s.a,i.a,i.b,!1,r,null,null);t.a=l.exports},function(e,t){},function(e,t,n){"use strict";var a=n(143),s=n(380),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement;return(e._self._c||t)("div")},s=[]},function(e,t,n){"use strict";var a=n(144),s=n(382),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement;return(e._self._c||t)("div")},s=[]},function(e,t,n){"use strict";var a=n(145),s=n(384),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement;return(e._self._c||t)("div")},s=[]},function(e,t,n){"use strict";var a=n(146),s=n(386),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement;return(e._self._c||t)("button",{staticClass:"sidebar-minimizer",attrs:{type:"button"},on:{click:function(t){e.sidebarMinimize(),e.brandMinimize()}}})},s=[]},function(e,t,n){"use strict";var a=n(147),s=n(392),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},,,,,function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement;return(e._self._c||t)("li",{class:e.classList})},s=[]},function(e,t,n){"use strict";var a=n(148),s=n(394),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("router-link",{staticClass:"nav-item nav-dropdown",attrs:{tag:"li",to:e.url,disabled:""}},[n("div",{staticClass:"nav-link nav-dropdown-toggle",on:{click:e.handleClick}},[n("i",{class:e.icon}),e._v(" "+e._s(e.name))]),e._v(" "),n("ul",{staticClass:"nav-dropdown-items"},[e._t("default")],2)])},s=[]},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return e.isExternalLink?n("div",[n("a",{class:e.classList,attrs:{href:e.url}},[n("i",{class:e.icon}),e._v(" "+e._s(e.name)+"\n    "),e.badge&&e.badge.text?n("b-badge",{attrs:{variant:e.badge.variant}},[e._v(e._s(e.badge.text))]):e._e()],1)]):n("div",[n("router-link",{class:e.classList,attrs:{to:e.url}},[n("i",{class:e.icon}),e._v(" "+e._s(e.name)+"\n    "),e.badge&&e.badge.text?n("b-badge",{attrs:{variant:e.badge.variant}},[e._v(e._s(e.badge.text))]):e._e()],1)],1)},s=[]},function(e,t,n){"use strict";var a=n(151),s=n(397),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("li",{class:e.classList},[e.wrapper&&e.wrapper.element?[n(e.wrapper.element,e._b({tag:"component"},"component",e.wrapper.attributes,!1),[e._v("\n      "+e._s(e.name)+"\n    ")])]:[e._v("\n    "+e._s(e.name)+"\n  ")]],2)},s=[]},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement;return(e._self._c||t)("li",{class:e.classList,on:{click:e.hideMobile}},[e._t("default")],2)},s=[]},function(e,t,n){"use strict";var a=n(154),s=n(400),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("SidebarNavItem",{attrs:{classes:e.classList.navItem}},[n("a",{class:e.classList.navLink,attrs:{href:e.url}},[n("i",{class:e.classList.icon}),e._v(" "+e._s(e.name))])])},s=[]},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"sidebar"},[n("SidebarHeader"),e._v(" "),n("SidebarForm"),e._v(" "),n("nav",{staticClass:"sidebar-nav"},[n("div",{attrs:{slot:"header"},slot:"header"}),e._v(" "),n("ul",{staticClass:"nav"},[e._l(e.navItems,function(t,a){return[t.title?[n("SidebarNavTitle",{attrs:{name:t.name,classes:t.class,wrapper:t.wrapper}})]:t.divider?[n("SidebarNavDivider",{attrs:{classes:t.class}})]:t.label?[n("SidebarNavLabel",{attrs:{name:t.name,url:t.url,icon:t.icon,label:t.label,classes:t.class}})]:[t.children?[n("SidebarNavDropdown",{attrs:{name:t.name,url:t.url,icon:t.icon}},[e._l(t.children,function(a,s){return[a.children?[n("SidebarNavDropdown",{attrs:{name:a.name,url:a.url,icon:a.icon}},e._l(a.children,function(e,a){return n("li",{staticClass:"nav-item"},[n("SidebarNavLink",{attrs:{name:e.name,url:e.url,icon:e.icon,badge:e.badge,variant:t.variant}})],1)}))]:[n("SidebarNavItem",{attrs:{classes:t.class}},[n("SidebarNavLink",{attrs:{name:a.name,url:a.url,icon:a.icon,badge:a.badge,variant:t.variant}})],1)]]})],2)]:[n("SidebarNavItem",{attrs:{classes:t.class}},[n("SidebarNavLink",{attrs:{name:t.name,url:t.url,icon:t.icon,badge:t.badge,variant:t.variant}})],1)]]]})],2),e._v(" "),e._t("default")],2),e._v(" "),n("SidebarFooter"),e._v(" "),n("SidebarMinimizer")],1)},s=[]},function(e,t,n){"use strict";var a=n(155),s=n(403),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("label",{class:e.classList},[n("input",{staticClass:"switch-input",attrs:{type:"checkbox"},domProps:{value:e.value,checked:e.isChecked},on:{change:e.handleChange}}),e._v(" "),e.isOn?[n("span",{staticClass:"switch-label",attrs:{"data-on":e.on,"data-off":e.off}})]:[n("span",{staticClass:"switch-label"})],e._v(" "),n("span",{staticClass:"switch-handle"})],2)},s=[]},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"app"},[n("AppHeader"),e._v(" "),n("div",{staticClass:"app-body"},[n("Sidebar",{attrs:{navItems:e.nav}}),e._v(" "),n("main",{staticClass:"main"},[n("breadcrumb",{attrs:{list:e.list}}),e._v(" "),n("div",{staticClass:"container-fluid"},[n("router-view")],1)],1),e._v(" "),n("AppAside")],1),e._v(" "),n("AppFooter")],1)},s=[]},function(e,t,n){"use strict";var a=n(156),s=n.n(a),i=n(406),o=n(2),r=Object(o.a)(s.a,i.a,i.b,!1,null,null,null);t.default=r.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"animated fadeIn"},[n("b-row",[n("b-col",{attrs:{lg:"12"}},[n("h1",[e._v("Welcome to ML GuI")]),e._v(" "),n("h3",[e._v("Building ML models from user defined computational graphs !")]),e._v(" "),n("p",[e._v("ML GuI provides an online web application based utility for desigining Machine Learning Models for generic users without nedding to have indepth knowledge of coding required.")]),e._v(" "),n("p",[e._v("Built as a wrapper around "),n("a",{attrs:{href:"#"}},[e._v("ChemML")]),e._v(" and a separate visualisation module - the app just needs a user to define the graphs. Rest is taken care dynamically. ")]),e._v(" "),n("p",[e._v("Steps to define a graph : \n        "),n("ol",[n("li",[e._v("\n            Go to Dashboard\n          ")]),e._v(" "),n("li",[e._v("\n            Load a pre saved template or if starting from scratch design your own - Right Click on canvas to add node and follow prompt to add node properties\n          ")]),e._v(" "),n("li",[e._v("\n            To save as templates for further use, the current graph must be saved by root user. It is then available to all users for loading as template.\n          ")]),e._v(" "),n("li",[e._v("\n            After Defining a graph and setting up all node properties click on red button to run graph. Multiple Graphs can be run asynchronously in the background. \n             \n          ")])])])])],1),e._v(" "),n("b-row",[n("b-col",{attrs:{lg:"10"}},[n("b-img",{attrs:{height:"280",src:"static/img/banner.png",rounded:"",alt:"Banner"}})],1)],1)],1)},s=[]},function(e,t,n){"use strict";var a=n(157),s=n(495),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},,,,,,function(e,t,n){"use strict";function a(e){n(414)}var s=n(159),i=n(462),o=n(2),r=a,l=Object(o.a)(s.a,i.a,i.b,!1,r,null,null);t.a=l.exports},function(e,t){},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement;return(e._self._c||t)("div",{attrs:{id:"cy"}})},s=[]},function(e,t,n){"use strict";var a=n(166),s=n(494),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";function a(e){n(465)}var s=n(167),i=n(467),o=n(2),r=a,l=Object(o.a)(s.a,i.a,i.b,!1,r,"data-v-776df528",null);t.a=l.exports},function(e,t){},function(e,t,n){"use strict";function a(){return Math.floor(65536*(1+Math.random())).toString(16).substring(1)}function s(){return this.s4()+this.s4()+"-"+this.s4()+"-"+this.s4()+"-"+this.s4()+"-"+this.s4()+this.s4()+this.s4()}t.a={s4:a,guid:s}},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("b-modal",{attrs:{title:"Add Node","hide-header-close":""},on:{ok:function(t){e.handleOk()},cancel:function(t){e.hide()}},model:{value:e.myModal,callback:function(t){e.myModal=t},expression:"myModal"}},[n("div",[e._l(e.libHierarchy,function(t,a){return[n("b-form-group",{attrs:{label:a,"label-for":"radios","label-cols":3,horizontal:!0}},[n("b-form-radio-group",{attrs:{id:"radios",name:"radiosInline"}},e._l(t,function(s,i){return n("div",{staticClass:"custom-control custom-radio custom-control-inline"},[e._v("\n            "+e._s(t.name)+"\n            "),n("input",{staticClass:"custom-control-input",attrs:{type:"radio",id:i,name:"radiosInline",key1:a,key2:i},domProps:{value:JSON.stringify(s)}}),e._v(" "),n("label",{staticClass:"custom-control-label",attrs:{for:"radiosInline"}},[e._v(e._s(i))])])}))],1)]})],2)])},s=[]},function(e,t,n){"use strict";function a(e){n(469)}var s=n(168),i=n(470),o=n(2),r=a,l=Object(o.a)(s.a,i.a,i.b,!1,r,"data-v-813ec168",null);t.a=l.exports},function(e,t){},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("b-modal",{attrs:{title:"Edit Node"},on:{ok:e.handleOk,cancel:e.handleCancel},model:{value:e.myModal,callback:function(t){e.myModal=t},expression:"myModal"}},[e.selectedNode.hasOwnProperty("elem")?n("div",[n("strong",[e._v("Node Type")]),e._v(" "),e._l(e.selectedNode.elem.data.info,function(t,a){return[n("b-form-group",{attrs:{label:a,"label-for":"radios","label-cols":3,horizontal:!0}},[n("b-form-radio-group",{attrs:{id:"radios",name:"funcRadios"}},e._l(t.name,function(s){return n("div",{staticClass:"custom-control custom-radio custom-control-inline"},[n("input",{staticClass:"custom-control-input",attrs:{type:"radio",id:s,name:"funcRadios"},domProps:{value:s},on:{click:function(n){e.handleFuncChange(a,s,t.functions[t.name.indexOf(s)])}}}),e._v(" "),n("label",{staticClass:"custom-control-label",attrs:{for:"radiosInline"}},[e._v(e._s(s))])])}))],1)]}),e._v(" "),void 0!==e.meths?n("strong",[e._v("Select Class Method")]):e._e(),e._v(" "),e._l(e.meths,function(t){return[n("b-form-radio-group",{attrs:{id:"radiosmeths",name:"methRadios"}},[n("input",{attrs:{type:"radio",id:t,name:"methRadios"},domProps:{value:t},on:{click:function(n){e.handleMethChange(t)}}}),e._v(" "),n("label",{staticClass:"custom-control-label",attrs:{for:"radiosInline"}},[e._v(e._s(t))])])]}),e._v(" "),n("br"),e._v(" "),e.fparams.length>0?n("div",[n("button",{staticClass:"btn btn-success",on:{click:function(t){e.isHidden=!e.isHidden}}},[e._v("Click to Set Base Parameter Values")]),e._v(" "),n("b-button",{staticClass:"btn btn-outline-info",on:{click:function(t){e.isHidden2=!e.isHidden2}}},[n("i",{staticClass:"fa fa-question"})])],1):e._e(),e._v(" "),e._l(e.fparams,function(t){return e.isHidden?e._e():[n("b-form-group",[n("dl",{staticClass:"row"},[n("dt",{staticClass:"col-sm-4 "},[t.is_optional?n("div",[n("label",{attrs:{for:t.name}},[e._v(e._s(t.name))])]):n("div",{staticClass:"form-group required"},[n("label",{staticClass:"control-label",attrs:{for:t.name}},[e._v(e._s(t.name))])])]),e._v(" "),n("dd",{staticClass:"col-sm-7"},[e.isHidden2?e._e():n("div",{staticClass:"alert alert-light"},[e._v("\n                  "+e._s(t.desc)+"\n                ")])]),e._v(" "),n("dd",{staticClass:"col-sm-12"},[n("b-form-input",{attrs:{type:"text",id:t.name,placeholder:"Enter Value"},model:{value:t.value,callback:function(n){e.$set(t,"value",n)},expression:"param.value"}})],1)])])]})],2):e._e()])},s=[]},function(e,t,n){"use strict";var a=n(169),s=n(472),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("b-modal",{attrs:{title:"Edit Edge","hide-header-close":""},on:{ok:function(t){e.handleOk()},cancel:function(t){e.hide()}},model:{value:e.myModal,callback:function(t){e.myModal=t},expression:"myModal"}},[e.inputs.length>0?n("strong",[e._v("Source Node Outputs")]):e._e(),e._v(" "),e._l(e.inputs,function(t){return[n("b-form-group",[n("label",{attrs:{for:t.name}},[e._v(e._s(t.name))]),e._v(" "),n("b-form-checkbox",{attrs:{type:"",id:t.name,placeholder:"Enter Value",value:"true","unchecked-value":"false"},model:{value:t.value,callback:function(n){e.$set(t,"value",n)},expression:"input.value"}})],1)]}),e._v(" "),n("br"),e._v(" "),e.outputs.length>0?n("strong",[e._v("Target Node Inputs")]):e._e(),e._v(" "),n("br"),e._v(" "),e.outputs.length>0?n("small",[e._v(" Select unique values for each from dropdown output of previous node")]):e._e(),e._v(" "),e._l(e.outputs,function(t){return[n("b-form-group",[n("label",{attrs:{for:t.name}},[e._v(e._s(t.name))]),e._v(" "),n("b-form-select",{attrs:{id:t.name},model:{value:t.value,callback:function(n){e.$set(t,"value",n)},expression:"output.value"}},e._l(e.inputs,function(t){return n("option",{domProps:{value:t.name}},[e._v("\n          "+e._s(t.name)+"\n        ")])}))],1)]})],2)},s=[]},function(e,t,n){"use strict";function a(e){n(474)}var s=n(170),i=n(475),o=n(2),r=a,l=Object(o.a)(s.a,i.a,i.b,!1,r,"data-v-c37c3fa2",null);t.a=l.exports},function(e,t){},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("b-modal",{attrs:{title:"Load Graph",size:"lg","hide-header-close":""},on:{ok:function(t){e.handleOk()},cancel:function(t){e.hide()}},model:{value:e.myModal,callback:function(t){e.myModal=t},expression:"myModal"}},[n("div",{staticClass:"row"},[n("div",{staticClass:"col-sm-2",staticStyle:{"border-right":"1px solid #a4b7c1"}},[n("b-list-group",[n("b-list-group-item",{staticClass:"sections btn-secondary btn-sm active",attrs:{id:"root",href:"#",view:"rootgraphs"},on:{click:function(t){e.openView("rootgraphs")}}},[e._v("Templates")]),e._v(" "),n("b-list-group-item",{staticClass:"sections btn-secondary btn-sm",attrs:{id:"user",href:"#",view:"usergraphs"},on:{click:function(t){e.openView("usergraphs")}}},[e._v("Saved")])],1)],1),e._v(" "),n("div",{staticClass:"col-sm-10 graphviews",attrs:{id:"rootgraphs"}},[n("div",{staticClass:"col-sm-6"},[n("select",{staticClass:"form-control",attrs:{name:"user",size:"8"}},[e._l(e.rootgraphs,function(t){return[n("option",{domProps:{value:t.graph_id}},[e._v(e._s(t.title))])]})],2)])]),e._v(" "),n("div",{staticClass:"col-sm-10 graphviews hiddenView",attrs:{id:"usergraphs"}},[n("div",{staticClass:"col-sm-6"},[n("select",{staticClass:"form-control",attrs:{name:"user",size:"8"}},[e._l(e.usergraphs,function(t){return[n("option",{domProps:{value:t.graph_id}},[e._v(e._s(t.title))])]})],2)])])])])},s=[]},function(e,t,n){"use strict";function a(e){n(477)}var s=n(171),i=n(479),o=n(2),r=a,l=Object(o.a)(s.a,i.a,i.b,!1,r,"data-v-7ab72377",null);t.a=l.exports},function(e,t){},,function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("b-modal",{attrs:{title:"Save Graph","hide-header-close":""},on:{ok:function(t){e.handleOk()},cancel:function(t){e.hide()}},model:{value:e.myModal,callback:function(t){e.myModal=t},expression:"myModal"}},[n("div",[n("label",{attrs:{for:"name"}},[e._v("Name")]),e._v(" "),n("b-form-input",{attrs:{type:"text",id:"name"},model:{value:e.newGraphTitle,callback:function(t){e.newGraphTitle=t},expression:"newGraphTitle"}})],1)])},s=[]},function(e,t,n){"use strict";function a(e){n(481)}var s=n(172),i=n(482),o=n(2),r=a,l=Object(o.a)(s.a,i.a,i.b,!1,r,"data-v-6af6e162",null);t.a=l.exports},function(e,t){},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("b-modal",{attrs:{title:"Update Graph","ok-title":"Yes","cancel-title":"No","no-close-on-esc":"","no-close-on-backdrop":"","hide-header-close":""},on:{ok:function(t){e.handleOk()},cancel:function(t){e.handleCancel()}},model:{value:e.myModal,callback:function(t){e.myModal=t},expression:"myModal"}},[n("div",[n("h5",[e._v("Do you want update/overwrite the current graph?")])])])},s=[]},function(e,t,n){"use strict";function a(e){n(484)}var s=n(173),i=n(490),o=n(2),r=a,l=Object(o.a)(s.a,i.a,i.b,!1,r,"data-v-12646eb7",null);t.a=l.exports},function(e,t){},,function(e,t){},function(e,t,n){"use strict";function a(e){n(488)}var s=n(174),i=n(489),o=n(2),r=a,l=Object(o.a)(s.a,i.a,i.b,!1,r,null,null);t.a=l.exports},function(e,t){},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("label",{staticClass:"text-reader"},[n("input",{attrs:{type:"file"},on:{change:e.loadTextFromFile}}),e._v(" "),e._v("\n  filereader\n")])},s=[]},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("b-modal",{attrs:{title:"Wizard",size:"lg","hide-header":"","hide-footer":"","ok-title":"Yes","cancel-title":"No","hide-header-close":""},on:{ok:e.handleOk,cancel:e.handleCancel},model:{value:e.myModal,callback:function(t){e.myModal=t},expression:"myModal"}},[n("div",[n("form-wizard",{ref:"formWizard",attrs:{"step-size":"sm",title:"Startup Wizard",color:"#184da3"},on:{"on-complete":e.handleOk,"on-change":e.setActiveIndex}},[e._l(e.wizHierarchy,function(t,a){return["Mix"!=a?n("tab-content",{attrs:{icon:e.icon[a],title:a}},["Finish"!=a?n("div",[n("div",{staticStyle:{border:"solid rgb(164, 183, 193)","border-width":"1px 0px",padding:"0px 5px 10px 5px"}},[e._l(1,function(s){return n("div",{staticClass:"row",staticStyle:{"margin-top":"10px"}},[n("div",{staticClass:"col-sm-6"},[n("select",{directives:[{name:"model",rawName:"v-model",value:e.selectedFunction[a],expression:"selectedFunction[key1]"}],staticClass:"form-control",attrs:{name:"user",size:"5"},on:{change:function(t){var n=Array.prototype.filter.call(t.target.options,function(e){return e.selected}).map(function(e){return"_value"in e?e._value:e.value});e.$set(e.selectedFunction,a,t.target.multiple?n:n[0])}}},[e._l(t,function(t,a){return[n("option",{domProps:{value:a}},[e._v(e._s(e.libUINames[a]))])]}),e._v(" "),n("option",{attrs:{value:"NA"}},[e._v(" -- ")])],2)]),e._v(" "),n("div",{staticClass:"col-sm-6"},[n("div",{staticClass:"col-sm-12",staticStyle:{border:"1px solid rgb(164, 183, 193)",padding:"15px 15px",height:"100%"}},[e._v("\n                  "+e._s(e.selectedFunction[a])+"\n                  ")])])])}),e._v(" "),n("div",{staticClass:"row"},[n("div",{staticClass:"col-12",staticStyle:{"margin-top":"5px"}},[n("b-button",{staticClass:"float-right",staticStyle:{"margin-left":"3px"},attrs:{type:"button",variant:"secondary"}},[n("i",{staticClass:"fa fa-plus"})])],1)])],2)]):n("div",{staticStyle:{border:"solid rgb(164, 183, 193)","border-width":"1px 0px",padding:"15px 0px"}},[e._v("\n          This Wizard helps you get started on the workflow\n          "),n("ul",[n("li",[e._v("\n              To select the functions used in each step and to configure the parameters for\n              the same right click on the node and select the 'Edit' option from the context menu.\n            ")]),e._v(" "),n("li",[e._v("\n              To configure the data is passed from one step to another, right click on the edge and\n              select the 'Edit' option from the context menu\n            ")]),e._v(" "),n("li",[e._v("\n              To create a new edge, hover over the source node, click and drag the edge-handle\n              (red circle) to the destination node. Configure the parameters in the pop-up window.\n            ")]),e._v(" "),n("li",[e._v("\n              To remove a node or a edge, right click on the same and select the 'Remove' option\n              from the context menu.\n            ")])])])]):e._e()]})],2),e._v(" "),n("span")],1),e._v(" "),n("textarea",{directives:[{name:"model",rawName:"v-model",value:e.text,expression:"text"}],attrs:{rows:"10"},domProps:{value:e.text},on:{input:function(t){t.target.composing||(e.text=t.target.value)}}}),e._v(" "),n("br"),e._v(" "),n("file-reader",{on:{load:function(t){e.text=t}}})],1)},s=[]},function(e,t,n){"use strict";function a(e){n(492)}var s=n(175),i=n(493),o=n(2),r=a,l=Object(o.a)(s.a,i.a,i.b,!1,r,"data-v-5e885dd4",null);t.a=l.exports},function(e,t){},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("b-modal",{attrs:{title:"Help",size:"lg","hide-header-close":""},on:{ok:function(t){e.handleOk()},cancel:function(t){e.hide()}},model:{value:e.myModal,callback:function(t){e.myModal=t},expression:"myModal"}},[n("div",{staticClass:"row"},[n("div",{staticClass:"col-lg-12"},[n("h2",[e._v("How To Do Stuff Goes here")])])])])},s=[]},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"wrapper"},[n("add-node"),e._v(" "),n("edit-node"),e._v(" "),n("edit-edge"),e._v(" "),n("load-graph"),e._v(" "),n("save-graph"),e._v(" "),n("update-graph"),e._v(" "),n("help"),e._v(" "),n("wizard")],1)},s=[]},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"animated fadeIn"},[n("b-card",[n("b-row",[n("b-col",{attrs:{sm:"5"}},[n("h4",{staticClass:"card-title mb-0",attrs:{id:"graph"}},[e._v("Model Graph")])]),e._v(" "),n("b-col",{staticClass:"d-md-block",attrs:{sm:"7"}},[n("b-button",{staticClass:"float-right",staticStyle:{"margin-left":"3px"},attrs:{type:"button",variant:"secondary",title:"Help"},on:{click:function(t){e.handleHelp()}}},[n("i",{staticClass:"fa fa-question"})]),e._v(" "),n("b-button",{staticClass:"float-right",staticStyle:{"margin-left":"3px"},attrs:{type:"button",variant:"danger",title:"Run Graph"},on:{click:function(t){e.handleRun()}}},[n("i",{staticClass:"fa fa-play"})]),e._v(" "),n("b-button",{staticClass:"float-right",staticStyle:{"margin-left":"3px"},attrs:{type:"button",variant:"primary",title:"Save Graph"},on:{click:function(t){e.handleSave()}}},[n("i",{staticClass:"fa fa-save"})]),e._v(" "),n("b-dropdown",{staticClass:"float-right",staticStyle:{"margin-left":"3px"},attrs:{right:"",split:"",variant:"primary",title:"Set Layout"},on:{click:function(t){e.handleResetLayout("dagre")}}},[n("template",{slot:"button-content"},[n("i",{staticClass:"fa fa-sitemap",attrs:{title:"DAG Layout"}})]),e._v(" "),n("b-dropdown-item",{on:{click:function(t){e.handleResetLayout("dagre")}}},[e._v("DAG Layout")]),e._v(" "),n("b-dropdown-item",{on:{click:function(t){e.handleResetLayout("circle")}}},[e._v("Circle Layout")]),e._v(" "),n("b-dropdown-item",{on:{click:function(t){e.handleResetLayout("breadthfirst")}}},[e._v("Breadth First Layout")]),e._v(" "),n("b-dropdown-item",{on:{click:function(t){e.handleResetLayout("grid")}}},[e._v("Grid Layout")]),e._v(" "),n("b-dropdown-item",{on:{click:function(t){e.handleResetLayout("concentric")}}},[e._v("Concentric Layout")]),e._v(" "),n("b-dropdown-item",{on:{click:function(t){e.handleResetLayout("cose")}}},[e._v("CoSE Layout")]),e._v(" "),n("b-dropdown-item",{on:{click:function(t){e.handleResetLayout("random")}}},[e._v("Random Layout")])],2),e._v(" "),n("b-button",{staticClass:"float-right",staticStyle:{"margin-left":"3px"},attrs:{type:"button",variant:"success",title:"Load Template"},on:{click:function(t){e.handleLoad()}}},[n("i",{staticClass:"fa fa-folder-open"})])],1)],1),e._v(" "),n("cytoscape-graph",{staticClass:"chart-wrapper",staticStyle:{height:"600px","margin-top":"40px"},attrs:{height:"600"}}),e._v(" "),n("div",{attrs:{slot:"footer"},slot:"footer"})],1),e._v(" "),n("modals")],1)},s=[]},function(e,t,n){"use strict";var a=n(176),s=n(499),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";var a=n(177),s=n(498),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("b-card",{attrs:{header:e.caption}},[n("b-table",{attrs:{hover:e.hover,striped:e.striped,bordered:e.bordered,small:e.small,fixed:e.fixed,responsive:"sm",items:e.graphruns,fields:e.graphruns_fields,"current-page":e.currentPage,"per-page":e.perPage},scopedSlots:e._u([{key:"status",fn:function(t){return[n("b-badge",{attrs:{variant:e.getBadge(t.item.status)}},[e._v(e._s(t.item.status))])]}}])}),e._v(" "),n("nav",[n("b-pagination",{attrs:{"total-rows":e.getRowCount(e.items),"per-page":e.perPage,"prev-text":"Prev","next-text":"Next","hide-goto-end-buttons":""},model:{value:e.currentPage,callback:function(t){e.currentPage=t},expression:"currentPage"}})],1)],1)},s=[]},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"animated fadeIn"},[n("b-row",[n("b-col",{attrs:{lg:"12"}},[n("c-table",{attrs:{striped:"",caption:"<i class='fa fa-align-justify'></i> Executions"}})],1)],1)],1)},s=[]},function(e,t,n){"use strict";var a=n(501),s=n(2),i=Object(s.a)(null,a.a,a.b,!1,null,null,null);t.a=i.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"animated fadeIn"},[n("b-row",[n("b-col",{attrs:{lg:"12"}},[n("iframe",{attrs:{src:"http://localhost:7000/d3map/"}})])],1)],1)},s=[]},function(e,t,n){"use strict";var a=n(178),s=n(503),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"app flex-row align-items-center"},[n("div",{staticClass:"container"},[n("b-row",{staticClass:"justify-content-center"},[n("b-col",{attrs:{md:"6"}},[n("div",{staticClass:"clearfix"},[n("h1",{staticClass:"float-left display-3 mr-4"},[e._v("404")]),e._v(" "),n("h4",{staticClass:"pt-3"},[e._v("Oops! You're lost.")]),e._v(" "),n("p",{staticClass:"text-muted"},[e._v("The page you are looking for was not found.")])]),e._v(" "),n("b-input-group",[n("b-input-group-prepend",[n("b-input-group-text",[n("i",{staticClass:"fa fa-search"})])],1),e._v(" "),n("input",{staticClass:"form-control",attrs:{id:"prependedInput",size:"16",type:"text",placeholder:"What are you looking for?"}}),e._v(" "),n("b-input-group-append",[n("b-button",{attrs:{variant:"info"}},[e._v("Search")])],1)],1)],1)],1)],1)])},s=[]},function(e,t,n){"use strict";var a=n(179),s=n(505),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"app flex-row align-items-center"},[n("div",{staticClass:"container"},[n("b-row",{staticClass:"justify-content-center"},[n("b-col",{attrs:{md:"6"}},[n("div",{staticClass:"clearfix"},[n("h1",{staticClass:"float-left display-3 mr-4"},[e._v("500")]),e._v(" "),n("h4",{staticClass:"pt-3"},[e._v("Houston, we have a problem!")]),e._v(" "),n("p",{staticClass:"text-muted"},[e._v("The page you are looking for is temporarily unavailable.")])]),e._v(" "),n("b-input-group",[n("b-input-group-prepend",[n("b-input-group-text",[n("i",{staticClass:"fa fa-search"})])],1),e._v(" "),n("input",{staticClass:"form-control",attrs:{id:"prependedInput",size:"16",type:"text",placeholder:"What are you looking for?"}}),e._v(" "),n("b-input-group-append",[n("b-button",{attrs:{variant:"info"}},[e._v("Search")])],1)],1)],1)],1)],1)])},s=[]},function(e,t,n){"use strict";var a=n(180),s=n(507),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"app flex-row align-items-center"},[n("div",{staticClass:"container"},[n("b-row",{staticClass:"justify-content-center"},[n("b-col",{attrs:{md:"8"}},[n("b-card-group",[n("b-card",{staticClass:"p-4",attrs:{"no-body":""}},[n("b-card-body",[n("h1",[e._v("Login")]),e._v(" "),n("p",{staticClass:"text-muted"},[e._v("Sign In to your account")]),e._v(" "),n("b-input-group",{staticClass:"mb-3"},[n("b-input-group-prepend",[n("b-input-group-text",[n("i",{staticClass:"icon-user"})])],1),e._v(" "),n("input",{directives:[{name:"model",rawName:"v-model",value:e.user,expression:"user"}],staticClass:"form-control",attrs:{type:"text",placeholder:"Username"},domProps:{value:e.user},on:{input:function(t){t.target.composing||(e.user=t.target.value)}}})],1),e._v(" "),n("b-input-group",{staticClass:"mb-4"},[n("b-input-group-prepend",[n("b-input-group-text",[n("i",{staticClass:"icon-lock"})])],1),e._v(" "),n("input",{directives:[{name:"model",rawName:"v-model",value:e.pass,expression:"pass"}],staticClass:"form-control",attrs:{type:"password",placeholder:"Password"},domProps:{value:e.pass},on:{input:function(t){t.target.composing||(e.pass=t.target.value)}}})],1),e._v(" "),n("b-row",[n("b-col",{attrs:{cols:"6"}},[n("b-button",{staticClass:"px-4",attrs:{variant:"primary"},on:{click:function(t){e.login(e.user,e.pass)}}},[e._v("Login")])],1),e._v(" "),e.loginFailed?n("b-col",{staticClass:"text-right",attrs:{cols:"6"}},[n("span",{staticStyle:{color:"red"}},[e._v("Login Failed")])]):e._e()],1)],1)],1),e._v(" "),n("b-card",{staticClass:"text-white bg-primary py-5 d-md-down-none",staticStyle:{width:"44%"},attrs:{"no-body":""}},[n("b-card-body",{staticClass:"text-center"},[n("div",[n("h2",[e._v("Sign up")]),e._v(" "),n("p",[e._v("Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.")]),e._v(" "),n("b-button",{staticClass:"active mt-3",attrs:{variant:"primary"}},[e._v("Register Now!")])],1)])],1)],1)],1)],1)],1)])},s=[]},function(e,t,n){"use strict";var a=n(181),s=n(509),i=n(2),o=Object(i.a)(a.a,s.a,s.b,!1,null,null,null);t.a=o.exports},function(e,t,n){"use strict";n.d(t,"a",function(){return a}),n.d(t,"b",function(){return s});var a=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"app flex-row align-items-center"},[n("div",{staticClass:"container"},[n("b-row",{staticClass:"justify-content-center"},[n("b-col",{attrs:{md:"6",sm:"8"}},[n("b-card",{staticClass:"mx-4",attrs:{"no-body":""}},[n("b-card-body",{staticClass:"p-4"},[n("h1",[e._v("Register")]),e._v(" "),n("p",{staticClass:"text-muted"},[e._v("Create your account")]),e._v(" "),n("b-input-group",{staticClass:"mb-3"},[n("b-input-group-prepend",[n("b-input-group-text",[n("i",{staticClass:"icon-user"})])],1),e._v(" "),n("input",{staticClass:"form-control",attrs:{type:"text",placeholder:"Username"}})],1),e._v(" "),n("b-input-group",{staticClass:"mb-3"},[n("b-input-group-prepend",[n("b-input-group-text",[e._v("@")])],1),e._v(" "),n("input",{staticClass:"form-control",attrs:{type:"text",placeholder:"Email"}})],1),e._v(" "),n("b-input-group",{staticClass:"mb-3"},[n("b-input-group-prepend",[n("b-input-group-text",[n("i",{staticClass:"icon-lock"})])],1),e._v(" "),n("input",{staticClass:"form-control",attrs:{type:"password",placeholder:"Password"}})],1),e._v(" "),n("b-input-group",{staticClass:"mb-4"},[n("b-input-group-prepend",[n("b-input-group-text",[n("i",{staticClass:"icon-lock"})])],1),e._v(" "),n("input",{staticClass:"form-control",attrs:{type:"password",placeholder:"Repeat password"}})],1),e._v(" "),n("b-button",{attrs:{variant:"success",block:""}},[e._v("Create Account")])],1),e._v(" "),n("b-card-footer",{staticClass:"p-4"},[n("b-row",[n("b-col",{attrs:{cols:"6"}},[n("b-button",{staticClass:"btn btn-facebook",attrs:{block:""}},[n("span",[e._v("facebook")])])],1),e._v(" "),n("b-col",{attrs:{cols:"6"}},[n("b-button",{staticClass:"btn btn-twitter",attrs:{block:"",type:"button"}},[n("span",[e._v("twitter")])])],1)],1)],1)],1)],1)],1)],1)])},s=[]}]),[182]);
//# sourceMappingURL=app.29dcf439fcb0215ad92e.js.map